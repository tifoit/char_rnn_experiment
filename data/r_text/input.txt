Next: Preface   [Contents][Index] This is an introduction to R (“GNU S”), a language and environment for
statistical computing and graphics.  R is similar to the
award-winning1 S
system, which was developed at Bell Laboratories by John Chambers et al.
It provides a wide variety of statistical and graphical techniques
(linear and nonlinear modelling, statistical tests, time series
analysis, classification, clustering, ...).
 This manual provides information on data types, programming elements,
statistical modelling and graphics.
 This manual is for R, version 3.3.1 (2016-06-21).
 Copyright © 1990 W. N. Venables
Copyright © 1992 W. N. Venables & D. M. Smith
Copyright © 1997 R. Gentleman & R. Ihaka
Copyright © 1997, 1998 M. Maechler
Copyright © 1999–2016 R Core Team
 Permission is granted to make and distribute verbatim copies of this
manual provided the copyright notice and this permission notice are
preserved on all copies.
 Permission is granted to copy and distribute modified versions of this
manual under the conditions for verbatim copying, provided that the
entire resulting derived work is distributed under the terms of a
permission notice identical to this one.
 Permission is granted to copy and distribute translations of this manual
into another language, under the above conditions for modified versions,
except that this permission notice may be stated in a translation
approved by the R Core Team.
 
Next: Introduction and preliminaries, Previous: Top, Up: Top   [Contents][Index] This introduction to R is derived from an original set of notes
describing the S and S-PLUS environments written in 1990–2 by
Bill Venables and David M. Smith when at the University of Adelaide.  We
have made a number of small changes to reflect differences between the
R and S programs, and expanded some of the material.
 We would like to extend warm thanks to Bill Venables (and David Smith)
for granting permission to distribute this modified version of the notes
in this way, and for being a supporter of R from way back.
 Comments and corrections are always welcome.  Please address email
correspondence to R-core@R-project.org.
 Most R novices will start with the introductory session in Appendix
A.  This should give some familiarity with the style of R sessions
and more importantly some instant feedback on what actually happens.
 Many users will come to R mainly for its graphical facilities.
See Graphics, which can be read at almost any time and need not wait
until all the preceding sections have been digested.
 
Next: Simple manipulations numbers and vectors, Previous: Preface, Up: Top   [Contents][Index] 
Next: Related software and documentation, Previous: Introduction and preliminaries, Up: Introduction and preliminaries   [Contents][Index] R is an integrated suite of software facilities for data
manipulation, calculation and graphical display.  Among other things it
has
 The term “environment” is intended to characterize it as a fully
planned and coherent system, rather than an incremental accretion of
very specific and inflexible tools, as is frequently the case with other
data analysis software.
 R is very much a vehicle for newly developing methods of interactive
data analysis.  It has developed rapidly, and has been extended by a
large collection of packages.  However, most programs written in
R are essentially ephemeral, written for a single piece of data
analysis.
 
Next: R and statistics, Previous: The R environment, Up: Introduction and preliminaries   [Contents][Index] R can be regarded as an implementation of the S language which
was developed at Bell Laboratories by Rick Becker, John Chambers and
Allan Wilks, and also forms the basis of the S-PLUS systems.
 The evolution of the S language is characterized by four books by
John Chambers and coauthors.  For R, the basic reference is The
New S Language: A Programming Environment for Data Analysis and
Graphics by Richard A. Becker, John M. Chambers and Allan R.
Wilks.  The new features of the 1991 release of S
are covered in Statistical Models in S edited by John M.
Chambers and Trevor J. Hastie.  The formal methods and classes of the
methods package are based on those described in Programming
with Data by John M. Chambers.  See References, for precise
references.
 There are now a number of books which describe how to use R for data
analysis and statistics, and documentation for S/S-PLUS can
typically be used with R, keeping the differences between the S
implementations in mind.  See What documentation exists for R? in The R statistical system FAQ.
 
Next: R and the window system, Previous: Related software and documentation, Up: Introduction and preliminaries   [Contents][Index] Our introduction to the R environment did not mention
statistics, yet many people use R as a statistics system.  We
prefer to think of it of an environment within which many classical and
modern statistical techniques have been implemented.  A few of these are
built into the base R environment, but many are supplied as
packages.  There are about 25 packages supplied with R (called
“standard” and “recommended” packages) and many more are available
through the CRAN family of Internet sites (via
https://CRAN.R-project.org) and elsewhere.  More details on
packages are given later (see Packages).
 Most classical statistics and much of the latest methodology is
available for use with R, but users may need to be prepared to do a
little work to find it.
 There is an important difference in philosophy between S (and hence
R) and the other main statistical systems.  In S a statistical
analysis is normally done as a series of steps, with intermediate
results being stored in objects.  Thus whereas SAS and SPSS will give
copious output from a regression or discriminant analysis, R will
give minimal output and store the results in a fit object for subsequent
interrogation by further R functions.
 
Next: Using R interactively, Previous: R and statistics, Up: Introduction and preliminaries   [Contents][Index] The most convenient way to use R is at a graphics workstation running
a windowing system.  This guide is aimed at users who have this
facility.  In particular we will occasionally refer to the use of R
on an X window system although the vast bulk of what is said applies
generally to any implementation of the R environment.
 Most users will find it necessary to interact directly with the
operating system on their computer from time to time.  In this guide, we
mainly discuss interaction with the operating system on UNIX machines.
If you are running R under Windows or OS X you will need to make
some small adjustments.
 Setting up a workstation to take full advantage of the customizable
features of R is a straightforward if somewhat tedious procedure, and
will not be considered further here.  Users in difficulty should seek
local expert help.
 
Next: Getting help, Previous: R and the window system, Up: Introduction and preliminaries   [Contents][Index] When you use the R program it issues a prompt when it expects input
commands.  The default prompt is ‘>’, which on UNIX might be
the same as the shell prompt, and so it may appear that nothing is
happening.  However, as we shall see, it is easy to change to a
different R prompt if you wish.  We will assume that the UNIX shell
prompt is ‘$’.
 In using R under UNIX the suggested procedure for the first occasion
is as follows:
 At this point you will be asked whether you want to save the data from
your R session.  On some systems this will bring up a dialog box, and
on others you will receive a text prompt to which you can respond
yes, no or cancel (a single letter abbreviation will
do) to save the data before quitting, quit without saving, or return to
the R session.  Data which is saved will be available in future R
sessions.
 Further R sessions are simple.
 To use R under Windows the procedure to
follow is basically the same.  Create a folder as the working directory,
and set that in the Start In field in your R shortcut.
Then launch R by double clicking on the icon.
 Readers wishing to get a feel for R at a computer before proceeding
are strongly advised to work through the introductory session
given in A sample session.
 
Next: R commands; case sensitivity etc, Previous: Using R interactively, Up: Introduction and preliminaries   [Contents][Index] R has an inbuilt help facility similar to the man facility of
UNIX.  To get more information on any specific named function, for
example solve, the command is
 An alternative is
 For a feature specified by special characters, the argument must be
enclosed in double or single quotes, making it a “character string”:
This is also necessary for a few words with syntactic meaning including
if, for and function.
 Either form of quote mark may be used to escape the other, as in the
string "It's important".  Our convention is to use
double quote marks for preference.
 On most R installations help is available in HTML format by
running
 which will launch a Web browser that allows the help pages to be browsed
with hyperlinks.  On UNIX, subsequent help requests are sent to the
HTML-based help system.  The ‘Search Engine and Keywords’ link in the
page loaded by help.start() is particularly useful as it is
contains a high-level concept list which searches though available
functions.  It can be a great way to get your bearings quickly and to
understand the breadth of what R has to offer.
 The help.search command (alternatively ??) 
allows searching for help in various
ways. For example,
 Try ?help.search for details and more examples.
 The examples on a help topic can normally be run by
 Windows versions of R have other optional help systems: use
 for further details.
 
Next: Recall and correction of previous commands, Previous: Getting help, Up: Introduction and preliminaries   [Contents][Index] Technically R is an expression language with a very simple
syntax.  It is case sensitive as are most UNIX based packages, so
A and a are different symbols and would refer to different
variables.  The set of symbols which can be used in R names depends
on the operating system and country within which R is being run
(technically on the locale in use).  Normally all alphanumeric
symbols are allowed2 (and in
some countries this includes accented letters) plus ‘.’ and
‘_’, with the restriction that a name must start with
‘.’ or a letter, and if it starts with ‘.’ the
second character must not be a digit.  Names are effectively
unlimited in length.
 Elementary commands consist of either expressions or
assignments.  If an expression is given as a command, it is
evaluated, printed (unless specifically made invisible), and the value
is lost.  An assignment also evaluates an expression and passes the
value to a variable but the result is not automatically printed.
 Commands are separated either by a semi-colon (‘;’), or by a
newline.  Elementary commands can be grouped together into one compound
expression by braces (‘{’ and ‘}’).
Comments can be put almost3 anywhere,
starting with a hashmark (‘#’), everything to the end of the
line is a comment.
 If a command is not complete at the end of a line, R will
give a different prompt, by default
 on second and subsequent lines and continue to read input until the
command is syntactically complete.  This prompt may be changed by the
user.  We will generally omit the continuation prompt
and indicate continuation by simple indenting.
 Command lines entered at the console are limited4 to about 4095 bytes (not characters).
 
Next: Executing commands from or diverting output to a file, Previous: R commands; case sensitivity etc, Up: Introduction and preliminaries   [Contents][Index] Under many versions of UNIX and on Windows, R provides a mechanism
for recalling and re-executing previous commands.  The vertical arrow
keys on the keyboard can be used to scroll forward and backward through
a command history.  Once a command is located in this way, the
cursor can be moved within the command using the horizontal arrow keys,
and characters can be removed with the DEL key or added with the
other keys.  More details are provided later: see The command-line editor.
 The recall and editing capabilities under UNIX are highly customizable.
You can find out how to do this by reading the manual entry for the
readline library.
 Alternatively, the Emacs text editor provides more general support
mechanisms (via ESS, Emacs Speaks Statistics) for
working interactively with R.  See R and Emacs in The R
statistical system FAQ.
 
Next: Data permanency and removing objects, Previous: Recall and correction of previous commands, Up: Introduction and preliminaries   [Contents][Index] If commands5 are stored in an external
file, say commands.R in the working directory work, they
may be executed at any time in an R session with the command
 For Windows Source is also available on the
File menu.  The function sink,
 will divert all subsequent output from the console to an external file,
record.lis.  The command
 restores it to the console once again.
 
Previous: Executing commands from or diverting output to a file, Up: Introduction and preliminaries   [Contents][Index] The entities that R creates and manipulates are known as
objects.  These may be variables, arrays of numbers, character
strings, functions, or more general structures built from such
components.
 During an R session, objects are created and stored by name (we
discuss this process in the next session).  The R command
 (alternatively, ls()) can be used to display the names of (most
of) the objects which are currently stored within R.  The collection
of objects currently stored is called the workspace.

 To remove objects the function rm is available:
 All objects created during an R session can be stored permanently in
a file for use in future R sessions.  At the end of each R session
you are given the opportunity to save all the currently available
objects.  If you indicate that you want to do this, the objects are
written to a file called .RData6 in the
current directory, and the command lines used in the session are saved
to a file called .Rhistory.
 When R is started at later time from the same directory it reloads
the workspace from this file.  At the same time the associated commands
history is reloaded.
 It is recommended that you should use separate working directories for
analyses conducted with R.  It is quite common for objects with names
x and y to be created during an analysis.  Names like this
are often meaningful in the context of a single analysis, but it can be
quite hard to decide what they might be when the several analyses have
been conducted in the same directory.
 
Next: Objects, Previous: Introduction and preliminaries, Up: Top   [Contents][Index] 
Next: Vector arithmetic, Previous: Simple manipulations numbers and vectors, Up: Simple manipulations numbers and vectors   [Contents][Index] R operates on named data structures.  The simplest such
structure is the numeric vector, which is a single entity
consisting of an ordered collection of numbers.  To set up a vector
named x, say, consisting of five numbers, namely 10.4, 5.6, 3.1,
6.4 and 21.7, use the R command
 This is an assignment statement using the function
c() which in this context can take an arbitrary number of vector
arguments and whose value is a vector got by concatenating its
arguments end to end.7
 A number occurring by itself in an expression is taken as a vector of
length one.
 Notice that the assignment operator (‘<-’), which consists
of the two characters ‘<’ (“less than”) and
‘-’ (“minus”) occurring strictly side-by-side and it
‘points’ to the object receiving the value of the expression.
In most contexts the ‘=’ operator can be used as an alternative.

 Assignment can also be made using the function assign().  An
equivalent way of making the same assignment as above is with:
 The usual operator, <-, can be thought of as a syntactic
short-cut to this.
 Assignments can also be made in the other direction, using the obvious
change in the assignment operator.  So the same assignment could be made
using
 If an expression is used as a complete command, the value is printed
and lost8.  So now if we
were to use the command
 the reciprocals of the five values would be printed at the terminal (and
the value of x, of course, unchanged).
 The further assignment
 would create a vector y with 11 entries consisting of two copies
of x with a zero in the middle place.
 
Next: Generating regular sequences, Previous: Vectors and assignment, Up: Simple manipulations numbers and vectors   [Contents][Index] Vectors can be used in arithmetic expressions, in which case the
operations are performed element by element.  Vectors occurring in the
same expression need not all be of the same length.  If they are not,
the value of the expression is a vector with the same length as the
longest vector which occurs in the expression.  Shorter vectors in the
expression are recycled as often as need be (perhaps
fractionally) until they match the length of the longest vector.  In
particular a constant is simply repeated.  So with the above assignments
the command

 generates a new vector v of length 11 constructed by adding
together, element by element, 2*x repeated 2.2 times, y
repeated just once, and 1 repeated 11 times.
 The elementary arithmetic operators are the usual +, -,
*, / and ^ for raising to a power.





In addition all of the common arithmetic functions are available.
log, exp, sin, cos, tan, sqrt,
and so on, all have their usual meaning.






max and min select the largest and smallest elements of a
vector respectively.


range is a function whose value is a vector of length two, namely
c(min(x), max(x)).

length(x) is the number of elements in x,

sum(x) gives the total of the elements in x,

and prod(x) their product.

 Two statistical functions are mean(x) which calculates the sample
mean, which is the same as sum(x)/length(x),

and var(x) which gives
 or sample variance.  If the argument to var() is an
n-by-p matrix the value is a p-by-p sample
covariance matrix got by regarding the rows as independent
p-variate sample vectors.
 sort(x) returns a vector of the same size as x with the
elements arranged in increasing order; however there are other more
flexible sorting facilities available (see order() or
sort.list() which produce a permutation to do the sorting).


 Note that max and min select the largest and smallest
values in their arguments, even if they are given several vectors.  The
parallel maximum and minimum functions pmax and
pmin return a vector (of length equal to their longest argument)
that contains in each element the largest (smallest) element in that
position in any of the input vectors.


 For most purposes the user will not be concerned if the “numbers” in a
numeric vector are integers, reals or even complex.  Internally
calculations are done as double precision real numbers, or double
precision complex numbers if the input data are complex.
 To work with complex numbers, supply an explicit complex part.  Thus
 will give NaN and a warning, but
 will do the computations as complex numbers.
 
Next: Logical vectors, Previous: Vector arithmetic, Up: Simple manipulations numbers and vectors   [Contents][Index] R has a number of facilities for generating commonly used sequences
of numbers.  For example 1:30 is the vector c(1, 2,
…, 29, 30).

The colon operator has high priority within an expression, so, for
example 2*1:15 is the vector c(2, 4, …, 28, 30).
Put n <- 10 and compare the sequences 1:n-1 and
1:(n-1).
 The construction 30:1 may be used to generate a sequence
backwards.
 The function seq() is a more general facility for generating
sequences.  It has five arguments, only some of which may be specified
in any one call.  The first two arguments, if given, specify the
beginning and end of the sequence, and if these are the only two
arguments given the result is the same as the colon operator.  That is
seq(2,10) is the same vector as 2:10.
 Arguments to seq(), and to many other R functions, can also
be given in named form, in which case the order in which they appear is
irrelevant.  The first two arguments may be named
from=value and to=value; thus
seq(1,30), seq(from=1, to=30) and seq(to=30,
from=1) are all the same as 1:30.  The next two arguments to
seq() may be named by=value and
length=value, which specify a step size and a length for
the sequence respectively.  If neither of these is given, the default
by=1 is assumed.
 For example
 generates in s3 the vector c(-5.0, -4.8, -4.6, …,
4.6, 4.8, 5.0).  Similarly
 generates the same vector in s4.
 The fifth argument may be named along=vector, which is
normally used as the only argument to create the sequence 1, 2,
…, length(vector), or the empty sequence if the vector is
empty (as it can be).
 A related function is rep()

which can be used for replicating an object in various complicated ways.
The simplest form is
 which will put five copies of x end-to-end in s5.  Another
useful version is
 which repeats each element of x five times before moving on to
the next.
 
Next: Missing values, Previous: Generating regular sequences, Up: Simple manipulations numbers and vectors   [Contents][Index] As well as numerical vectors, R allows manipulation of logical
quantities.  The elements of a logical vector can have the values
TRUE, FALSE, and NA (for “not available”, see
below).  The first two are often abbreviated as T and F,
respectively.  Note however that T and F are just
variables which are set to TRUE and FALSE by default, but
are not reserved words and hence can be overwritten by the user.  Hence,
you should always use TRUE and FALSE.




 Logical vectors are generated by conditions.  For example
 sets temp as a vector of the same length as x with values
FALSE corresponding to elements of x where the condition
is not met and TRUE where it is.
 The logical operators are <, <=, >, >=,
== for exact equality and != for inequality.






In addition if c1 and c2 are logical expressions, then
c1 & c2 is their intersection (“and”), c1 | c2
is their union (“or”), and !c1 is the negation of
c1.



 Logical vectors may be used in ordinary arithmetic, in which case they
are coerced into numeric vectors, FALSE becoming 0
and TRUE becoming 1.  However there are situations where
logical vectors and their coerced numeric counterparts are not
equivalent, for example see the next subsection.
 
Next: Character vectors, Previous: Logical vectors, Up: Simple manipulations numbers and vectors   [Contents][Index] In some cases the components of a vector may not be completely
known.  When an element or value is “not available” or a “missing
value” in the statistical sense, a place within a vector may be
reserved for it by assigning it the special value NA.

In general any operation on an NA becomes an NA.  The
motivation for this rule is simply that if the specification of an
operation is incomplete, the result cannot be known and hence is not
available.
 The function is.na(x) gives a logical vector of the same size as
x with value TRUE if and only if the corresponding element
in x is NA.
 Notice that the logical expression x == NA is quite different
from is.na(x) since NA is not really a value but a marker
for a quantity that is not available.  Thus x == NA is a vector
of the same length as x all of whose values are NA
as the logical expression itself is incomplete and hence undecidable.
 Note that there is a second kind of “missing” values which are
produced by numerical computation, the so-called Not a Number,
NaN,

values.  Examples are
 or
 which both give NaN since the result cannot be defined sensibly.
 In summary, is.na(xx) is TRUE both for NA
and NaN values.  To differentiate these, is.nan(xx) is only
TRUE for NaNs.

 Missing values are sometimes printed as <NA> when character
vectors are printed without quotes.
 
Next: Index vectors, Previous: Missing values, Up: Simple manipulations numbers and vectors   [Contents][Index] Character quantities and character vectors are used frequently in R,
for example as plot labels.  Where needed they are denoted by a sequence
of characters delimited by the double quote character, e.g.,
"x-values", "New iteration results".
 Character strings are entered using either matching double (") or
single (') quotes, but are printed using double quotes (or
sometimes without quotes).  They use C-style escape sequences, using
\ as the escape character, so \\ is entered and printed as
\\, and inside double quotes " is entered as \".
Other useful escape sequences are \n, newline, \t, tab and
\b, backspace—see ?Quotes for a full list.
 Character vectors may be concatenated into a vector by the c()
function; examples of their use will emerge frequently.

 The paste() function takes an arbitrary number of arguments and
concatenates them one by one into character strings.  Any numbers given
among the arguments are coerced into character strings in the evident
way, that is, in the same way they would be if they were printed.  The
arguments are by default separated in the result by a single blank
character, but this can be changed by the named argument,
sep=string, which changes it to string,
possibly empty.
 For example
 makes labs into the character vector
 Note particularly that recycling of short lists takes place here too;
thus c("X", "Y") is repeated 5 times to match the sequence
1:10.
9
 
Next: Other types of objects, Previous: Character vectors, Up: Simple manipulations numbers and vectors   [Contents][Index] Subsets of the elements of a vector may be selected by appending to the
name of the vector an index vector in square brackets.  More
generally any expression that evaluates to a vector may have subsets of
its elements similarly selected by appending an index vector in square
brackets immediately after the expression.
 Such index vectors can be any of four distinct types.
 creates (or re-creates) an object y which will contain the
non-missing values of x, in the same order.  Note that if
x has missing values, y will be shorter than x.
Also
 creates an object z and places in it the values of the vector
x+1 for which the corresponding value in x was both
non-missing and positive.
 selects the first 10 elements of x (assuming length(x) is
not less than 10).  Also
 (an admittedly unlikely thing to do) produces a character vector of
length 16 consisting of "x", "y", "y", "x" repeated four times.
 gives y all but the first five elements of x.
 The advantage is that alphanumeric names are often easier to
remember than numeric indices.  This option is particularly
useful in connection with data frames, as we shall see later.
 An indexed expression can also appear on the receiving end of an
assignment, in which case the assignment operation is performed
only on those elements of the vector.  The expression must be of
the form vector[index_vector] as having an arbitrary
expression in place of the vector name does not make much sense here.
 For example
 replaces any missing values in x by zeros and
 has the same effect as
 
Previous: Index vectors, Up: Simple manipulations numbers and vectors   [Contents][Index] Vectors are the most important type of object in R, but there are
several others which we will meet more formally in later sections.
 
Next: Factors, Previous: Simple manipulations numbers and vectors, Up: Top   [Contents][Index] 
Next: Changing the length of an object, Previous: Objects, Up: Objects   [Contents][Index] The entities R operates on are technically known as objects.
Examples are vectors of numeric (real) or complex values, vectors of
logical values and vectors of character strings.  These are known as
“atomic” structures since their components are all of the same type,
or mode, namely numeric10, complex,
logical, character and raw.
 Vectors must have their values all of the same mode.  Thus any
given vector must be unambiguously either logical,
numeric, complex, character or raw.  (The
only apparent exception to this rule is the special “value” listed as
NA for quantities not available, but in fact there are several
types of NA).  Note that a vector can be empty and still have a
mode.  For example the empty character string vector is listed as
character(0) and the empty numeric vector as numeric(0).
 R also operates on objects called lists, which are of mode
list.  These are ordered sequences of objects which individually
can be of any mode.  lists are known as “recursive” rather than
atomic structures since their components can themselves be lists in
their own right.
 The other recursive structures are those of mode function and
expression.  Functions are the objects that form part of the R
system along with similar user written functions, which we discuss in
some detail later.  Expressions as objects form an
advanced part of R which will not be discussed in this guide, except
indirectly when we discuss formulae used with modeling in R.
 By the mode of an object we mean the basic type of its
fundamental constituents.  This is a special case of a “property”
of an object.  Another property of every object is its length.  The
functions mode(object) and length(object) can be
used to find out the mode and length of any defined structure
11.
 Further properties of an object are usually provided by
attributes(object), see Getting and setting attributes.
Because of this, mode and length are also called “intrinsic
attributes” of an object.


 For example, if z is a complex vector of length 100, then in an
expression mode(z) is the character string "complex" and
length(z) is 100.
 R caters for changes of mode almost anywhere it could be considered
sensible to do so, (and a few where it might not be).  For example with
 we could put
 after which digits is the character vector c("0", "1", "2",
…, "9").  A further coercion, or change of mode,
reconstructs the numerical vector again:
 Now d and z are the same.12  There is a
large collection of functions of the form as.something()
for either coercion from one mode to another, or for investing an object
with some other attribute it may not already possess.  The reader should
consult the different help files to become familiar with them.
 
Next: Getting and setting attributes, Previous: The intrinsic attributes mode and length, Up: Objects   [Contents][Index] An “empty” object may still have a mode.  For example
 makes e an empty vector structure of mode numeric.  Similarly
character() is a empty character vector, and so on.  Once an
object of any size has been created, new components may be added to it
simply by giving it an index value outside its previous range.  Thus
 now makes e a vector of length 3, (the first two components of
which are at this point both NA).  This applies to any structure
at all, provided the mode of the additional component(s) agrees with the
mode of the object in the first place.
 This automatic adjustment of lengths of an object is used often, for
example in the scan() function for input.  (see The scan() function.)
 Conversely to truncate the size of an object requires only an assignment
to do so.  Hence if alpha is an object of length 10, then
 makes it an object of length 5 consisting of just the former components
with even index.  (The old indices are not retained, of course.)  We can
then retain just the first three values by
 and vectors can be extended (by missing values) in the same way.
 
Next: The class of an object, Previous: Changing the length of an object, Up: Objects   [Contents][Index] The function attributes(object)

returns a list of all the non-intrinsic attributes currently defined for
that object.  The function attr(object, name)

can be used to select a specific attribute.  These functions are rarely
used, except in rather special circumstances when some new attribute is
being created for some particular purpose, for example to associate a
creation date or an operator with an R object.  The concept, however,
is very important.
 Some care should be exercised when assigning or deleting attributes
since they are an integral part of the object system used in R.
 When it is used on the left hand side of an assignment it can be used
either to associate a new attribute with object or to
change an existing one.  For example
 allows R to treat z as if it were a 10-by-10 matrix.
 
Previous: Getting and setting attributes, Up: Objects   [Contents][Index] All objects in R have a class, reported by the function
class.  For simple vectors this is just the mode, for example
"numeric", "logical", "character" or "list",
but "matrix", "array", "factor" and
"data.frame" are other possible values.
 A special attribute known as the class of the object is used to
allow for an object-oriented style13 of
programming in R.  For example if an object has class
"data.frame", it will be printed in a certain way, the
plot() function will display it graphically in a certain way, and
other so-called generic functions such as summary() will react to
it as an argument in a way sensitive to its class.
 To remove temporarily the effects of class, use the function
unclass().

For example if winter has the class "data.frame" then
 will print it in data frame form, which is rather like a matrix, whereas
 will print it as an ordinary list.  Only in rather special situations do
you need to use this facility, but one is when you are learning to come
to terms with the idea of class and generic functions.
 Generic functions and classes will be discussed further in Object orientation, but only briefly.
 
Next: Arrays and matrices, Previous: Objects, Up: Top   [Contents][Index] A factor is a vector object used to specify a discrete
classification (grouping) of the components of other vectors of the same length.
R provides both ordered and unordered factors.
While the “real” application of factors is with model formulae
(see Contrasts), we here look at a specific example.
 Suppose, for example, we have a sample of 30 tax accountants from all
the states and territories of Australia14
and their individual state of origin is specified by a character vector
of state mnemonics as
 Notice that in the case of a character vector, “sorted” means sorted
in alphabetical order.
 A factor is similarly created using the factor() function:

 The print() function handles factors slightly differently from
other objects:
 To find out the levels of a factor the function levels() can be
used.

 
Next: Ordered factors, Previous: Factors, Up: Factors   [Contents][Index] To continue the previous example, suppose we have the incomes of the
same tax accountants in another vector (in suitably large units of
money)
 To calculate the sample mean income for each state we can now use the
special function tapply():
 giving a means vector with the components labelled by the levels
 The function tapply() is used to apply a function, here
mean(), to each group of components of the first argument, here
incomes, defined by the levels of the second component, here
statef15, as if they were separate vector
structures.  The result is a structure of the same length as the levels
attribute of the factor containing the results.  The reader should
consult the help document for more details.
 Suppose further we needed to calculate the standard errors of the state
income means.  To do this we need to write an R function to calculate
the standard error for any given vector.  Since there is an builtin
function var() to calculate the sample variance, such a function
is a very simple one liner, specified by the assignment:
 (Writing functions will be considered later in Writing your own functions, and in this case was unnecessary as R also has a builtin
function sd().)


After this assignment, the standard errors are calculated by
 and the values calculated are then
 As an exercise you may care to find the usual 95% confidence limits for
the state mean incomes.  To do this you could use tapply() once
more with the length() function to find the sample sizes, and the
qt() function to find the percentage points of the appropriate
t-distributions.  (You could also investigate R’s facilities
for t-tests.)
 The function tapply() can also be used to handle more complicated
indexing of a vector by multiple categories.  For example, we might wish
to split the tax accountants by both state and sex.  However in this
simple instance (just one factor) what happens can be thought of as
follows.  The values in the vector are collected into groups
corresponding to the distinct entries in the factor.  The function is
then applied to each of these groups individually.  The value is a
vector of function results, labelled by the levels attribute of
the factor.
 The combination of a vector and a labelling factor is an example of what
is sometimes called a ragged array, since the subclass sizes are
possibly irregular.  When the subclass sizes are all the same the
indexing may be done implicitly and much more efficiently, as we see in
the next section.
 
Previous: The function tapply() and ragged arrays, Up: Factors   [Contents][Index] The levels of factors are stored in alphabetical order, or in the order
they were specified to factor if they were specified explicitly.
 Sometimes the levels will have a natural ordering that we want to record
and want our statistical analysis to make use of.  The ordered()

function creates such ordered factors but is otherwise identical to
factor.  For most purposes the only difference between ordered
and unordered factors is that the former are printed showing the
ordering of the levels, but the contrasts generated for them in fitting
linear models are different.
 
Next: Lists and data frames, Previous: Factors, Up: Top   [Contents][Index] 
Next: Array indexing, Previous: Arrays and matrices, Up: Arrays and matrices   [Contents][Index] An array can be considered as a multiply subscripted collection of data
entries, for example numeric.  R allows simple facilities for
creating and handling arrays, and in particular the special case of
matrices.
 A dimension vector is a vector of non-negative integers.  If its length is
k then the array is k-dimensional, e.g. a matrix is a
2-dimensional array.  The dimensions are indexed from one up to
the values given in the dimension vector.
 A vector can be used by R as an array only if it has a dimension
vector as its dim attribute.  Suppose, for example, z is a
vector of 1500 elements.  The assignment
 gives it the dim attribute that allows it to be treated as a
3 by 5 by 100 array.
 Other functions such as matrix() and array() are available
for simpler and more natural looking assignments, as we shall see in
The array() function.
 The values in the data vector give the values in the array in the same
order as they would occur in FORTRAN, that is “column major order,”
with the first subscript moving fastest and the last subscript slowest.
 For example if the dimension vector for an array, say a, is
c(3,4,2) then there are 3 * 4 * 2
= 24 entries in a and the data vector holds them in the order
a[1,1,1], a[2,1,1], …, a[2,4,2], a[3,4,2].
 Arrays can be one-dimensional: such arrays are usually treated in the
same way as vectors (including when printing), but the exceptions can
cause confusion.
 
Next: Index matrices, Previous: Arrays, Up: Arrays and matrices   [Contents][Index] Individual elements of an array may be referenced by giving the name of
the array followed by the subscripts in square brackets, separated by
commas.
 More generally, subsections of an array may be specified by giving a
sequence of index vectors in place of subscripts; however
if any index position is given an empty index vector, then the
full range of that subscript is taken.
 Continuing the previous example, a[2,,] is a 4 *
2 array with dimension vector c(4,2) and data vector containing
the values
 in that order.  a[,,] stands for the entire array, which is the
same as omitting the subscripts entirely and using a alone.
 For any array, say Z, the dimension vector may be referenced
explicitly as dim(Z) (on either side of an assignment).
 Also, if an array name is given with just one subscript or index
vector, then the corresponding values of the data vector only are used;
in this case the dimension vector is ignored.  This is not the case,
however, if the single index is not a vector but itself an array, as we
next discuss.
 
Next: The array() function, Previous: Array indexing, Up: Arrays and matrices   [Contents][Index] As well as an index vector in any subscript position, a matrix may be
used with a single index matrix in order either to assign a vector
of quantities to an irregular collection of elements in the array, or to
extract an irregular collection as a vector.
 A matrix example makes the process clear.  In the case of a doubly
indexed array, an index matrix may be given consisting of two columns
and as many rows as desired.  The entries in the index matrix are the
row and column indices for the doubly indexed array.  Suppose for
example we have a 4 by 5 array X and we wish to do
the following:
 In this case we need a 3 by 2 subscript array, as in the
following example.
 Negative indices are not allowed in index matrices.  NA and zero
values are allowed: rows in the index matrix containing a zero are
ignored, and rows containing an NA produce an NA in the
result.
 As a less trivial example, suppose we wish to generate an (unreduced)
design matrix for a block design defined by factors blocks
(b levels) and varieties (v levels).  Further
suppose there are n plots in the experiment.  We could proceed as
follows:
 To construct the incidence matrix, N say, we could use
 However a simpler direct way of producing this matrix is to use
table():

 Index matrices must be numerical: any other form of matrix (e.g. a
logical or character matrix) supplied as a matrix is treated as an
indexing vector.
 
Next: The outer product of two arrays, Previous: Index matrices, Up: Arrays and matrices   [Contents][Index] As well as giving a vector structure a dim attribute, arrays can
be constructed from vectors by the array function, which has the
form
 For example, if the vector h contains 24 or fewer, numbers then
the command
 would use h to set up 3 by 4 by 2 array in
Z.  If the size of h is exactly 24 the result is the same as
 However if h is shorter than 24, its values are recycled from the
beginning again to make it up to size 24 (see The recycling rule)
but dim(h) <- c(3,4,2) would signal an error about mismatching
length.
As an extreme but common example
 makes Z an array of all zeros.
 At this point dim(Z) stands for the dimension vector
c(3,4,2), and Z[1:24] stands for the data vector as it was
in h, and Z[] with an empty subscript or Z with no
subscript stands for the entire array as an array.
 Arrays may be used in arithmetic expressions and the result is an array
formed by element-by-element operations on the data vector.  The
dim attributes of operands generally need to be the same, and
this becomes the dimension vector of the result.  So if A,
B and C are all similar arrays, then
 makes D a similar array with its data vector being the result of
the given element-by-element operations.  However the precise rule
concerning mixed array and vector calculations has to be considered a
little more carefully.
 
Previous: The array() function, Up: The array() function   [Contents][Index] The precise rule affecting element by element mixed calculations with
vectors and arrays is somewhat quirky and hard to find in the
references.  From experience we have found the following to be a reliable
guide.
 
Next: Generalized transpose of an array, Previous: The array() function, Up: Arrays and matrices   [Contents][Index] An important operation on arrays is the outer product.  If
a and b are two numeric arrays, their outer product is an
array whose dimension vector is obtained by concatenating their two
dimension vectors (order is important), and whose data vector is got by
forming all possible products of elements of the data vector of a
with those of b.  The outer product is formed by the special
operator %o%:

 An alternative is
 The multiplication function can be replaced by an arbitrary function of
two variables.  For example if we wished to evaluate the function
f(x; y) = cos(y)/(1 + x^2)
over a regular grid of values with x- and y-coordinates
defined by the R vectors x and y respectively, we could
proceed as follows:
 In particular the outer product of two ordinary vectors is a doubly
subscripted array (that is a matrix, of rank at most 1).  Notice that
the outer product operator is of course non-commutative.  Defining your
own R functions will be considered further in Writing your own functions.
 As an artificial but cute example, consider the determinants of 2
by 2 matrices [a, b; c, d] where each entry is a
non-negative integer in the range 0, 1, …, 9, that is a
digit.
 The problem is to find the determinants, ad - bc, of all possible
matrices of this form and represent the frequency with which each value
occurs as a high density plot.  This amounts to finding the
probability distribution of the determinant if each digit is chosen
independently and uniformly at random.
 A neat way of doing this uses the outer() function twice:
 Notice the coercion of the names attribute of the frequency table
to numeric in order to recover the range of the determinant values.  The
“obvious” way of doing this problem with for loops, to be
discussed in Loops and conditional execution, is so inefficient as
to be impractical.
 It is also perhaps surprising that about 1 in 20 such matrices is
singular.
 
Next: Matrix facilities, Previous: The outer product of two arrays, Up: Arrays and matrices   [Contents][Index] The function aperm(a, perm)

may be used to permute an array, a.  The argument perm
must be a permutation of the integers {1, …, k}, where
k is the number of subscripts in a.  The result of the
function is an array of the same size as a but with old dimension
given by perm[j] becoming the new j-th dimension.  The
easiest way to think of this operation is as a generalization of
transposition for matrices.  Indeed if A is a matrix, (that is, a
doubly subscripted array) then B given by
 is just the transpose of A.  For this special case a simpler
function t()

is available, so we could have used B <- t(A).
 
Next: Forming partitioned matrices, Previous: Generalized transpose of an array, Up: Arrays and matrices   [Contents][Index] As noted above, a matrix is just an array with two subscripts.  However
it is such an important special case it needs a separate discussion.
R contains many operators and functions that are available only for
matrices.  For example t(X) is the matrix transpose function, as
noted above.  The functions nrow(A) and ncol(A) give the
number of rows and columns in the matrix A respectively.


 
Next: Linear equations and inversion, Previous: Matrix facilities, Up: Matrix facilities   [Contents][Index] The operator %*% is used for matrix multiplication.

An n by 1 or 1 by n matrix may of course be
used as an n-vector if in the context such is appropriate.
Conversely, vectors which occur in matrix multiplication expressions are
automatically promoted either to row or column vectors, whichever is
multiplicatively coherent, if possible, (although this is not always
unambiguously possible, as we see later).
 If, for example, A and B are square matrices of the same
size, then
 is the matrix of element by element products and
 is the matrix product.  If x is a vector, then
 is a quadratic form.16
 The function crossprod() forms “crossproducts”, meaning that
crossprod(X, y) is the same as t(X) %*% y but the
operation is more efficient.  If the second argument to
crossprod() is omitted it is taken to be the same as the first.
 The meaning of diag() depends on its argument.  diag(v),
where v is a vector, gives a diagonal matrix with elements of the
vector as the diagonal entries.  On the other hand diag(M), where
M is a matrix, gives the vector of main diagonal entries of
M.  This is the same convention as that used for diag() in
MATLAB.  Also, somewhat confusingly, if k is a single
numeric value then diag(k) is the k by k identity
matrix!
 
Next: Eigenvalues and eigenvectors, Previous: Multiplication, Up: Matrix facilities   [Contents][Index] Solving linear equations is the inverse of matrix multiplication.
When after
 only A and b are given, the vector x is the
solution of that linear equation system.  In R,
 solves the system, returning x (up to some accuracy loss).
Note that in linear algebra, formally
x = A^{-1} %*% b
where
A^{-1} denotes the inverse of
A, which can be computed by
 but rarely is needed.  Numerically, it is both inefficient and
potentially unstable to compute x <- solve(A) %*% b instead of
solve(A,b).
 The quadratic form  x %*% A^{-1} %*%
x   which is used in multivariate computations, should be computed by
something like17 x %*% solve(A,x), rather
than computing the inverse of A.
 
Next: Singular value decomposition and determinants, Previous: Linear equations and inversion, Up: Matrix facilities   [Contents][Index] The function eigen(Sm) calculates the eigenvalues and
eigenvectors of a symmetric matrix Sm.  The result of this
function is a list of two components named values and
vectors.  The assignment
 will assign this list to ev.  Then ev$val is the vector of
eigenvalues of Sm and ev$vec is the matrix of
corresponding eigenvectors.  Had we only needed the eigenvalues we could
have used the assignment:
 evals now holds the vector of eigenvalues and the second
component is discarded.  If the expression
 is used by itself as a command the two components are printed, with
their names.  For large matrices it is better to avoid computing the
eigenvectors if they are not needed by using the expression
 
Next: Least squares fitting and the QR decomposition, Previous: Eigenvalues and eigenvectors, Up: Matrix facilities   [Contents][Index] The function svd(M) takes an arbitrary matrix argument, M,
and calculates the singular value decomposition of M.  This
consists of a matrix of orthonormal columns U with the same
column space as M, a second matrix of orthonormal columns
V whose column space is the row space of M and a diagonal
matrix of positive entries D such that M = U %*% D %*%
t(V).  D is actually returned as a vector of the diagonal
elements.  The result of svd(M) is actually a list of three
components named d, u and v, with evident meanings.
 If M is in fact square, then, it is not hard to see that
 calculates the absolute value of the determinant of M.  If this
calculation were needed often with a variety of matrices it could be
defined as an R function
 after which we could use absdet() as just another R function.
As a further trivial but potentially useful example, you might like to
consider writing a function, say tr(), to calculate the trace of
a square matrix.  [Hint: You will not need to use an explicit loop.
Look again at the diag() function.]
 R has a builtin function det to calculate a determinant,
including the sign, and another, determinant, to give the sign
and modulus (optionally on log scale),
 
Previous: Singular value decomposition and determinants, Up: Matrix facilities   [Contents][Index] The function lsfit() returns a list giving results of a least
squares fitting procedure.  An assignment such as
 gives the results of a least squares fit where y is the vector of
observations and X is the design matrix.  See the help facility
for more details, and also for the follow-up function ls.diag()
for, among other things, regression diagnostics.  Note that a grand mean
term is automatically included and need not be included explicitly as a
column of X.  Further note that you almost always will prefer
using lm(.) (see Linear models) to lsfit() for
regression modelling.
 Another closely related function is qr() and its allies.
Consider the following assignments
 These compute the orthogonal projection of y onto the range of
X in fit, the projection onto the orthogonal complement in
res and the coefficient vector for the projection in b,
that is, b is essentially the result of the MATLAB
‘backslash’ operator.
 It is not assumed that X has full column rank.  Redundancies will
be discovered and removed as they are found.
 This alternative is the older, low-level way to perform least squares
calculations.  Although still useful in some contexts, it would now
generally be replaced by the statistical models features, as will be
discussed in Statistical models in R.
 
Next: The concatenation function c() with arrays, Previous: Matrix facilities, Up: Arrays and matrices   [Contents][Index] As we have already seen informally, matrices can be built up from other
vectors and matrices by the functions cbind() and rbind().
Roughly cbind() forms matrices by binding together matrices
horizontally, or column-wise, and rbind() vertically, or
row-wise.
 In the assignment
 the arguments to cbind() must be either vectors of any length, or
matrices with the same column size, that is the same number of rows.
The result is a matrix with the concatenated arguments arg_1,
arg_2, … forming the columns.
 If some of the arguments to cbind() are vectors they may be
shorter than the column size of any matrices present, in which case they
are cyclically extended to match the matrix column size (or the length
of the longest vector if no matrices are given).
 The function rbind() does the corresponding operation for rows.
In this case any vector argument, possibly cyclically extended, are of
course taken as row vectors.
 Suppose X1 and X2 have the same number of rows.  To
combine these by columns into a matrix X, together with an
initial column of 1s we can use
 The result of rbind() or cbind() always has matrix status.
Hence cbind(x) and rbind(x) are possibly the simplest ways
explicitly to allow the vector x to be treated as a column or row
matrix respectively.
 
Next: Frequency tables from factors, Previous: Forming partitioned matrices, Up: Arrays and matrices   [Contents][Index] It should be noted that whereas cbind() and rbind() are
concatenation functions that respect dim attributes, the basic
c() function does not, but rather clears numeric objects of all
dim and dimnames attributes.  This is occasionally useful
in its own right.
 The official way to coerce an array back to a simple vector object is to
use as.vector()
 However a similar result can be achieved by using c() with just
one argument, simply for this side-effect:
 There are slight differences between the two, but ultimately the choice
between them is largely a matter of style (with the former being
preferable).
 
Previous: The concatenation function c() with arrays, Up: Arrays and matrices   [Contents][Index] Recall that a factor defines a partition into groups.  Similarly a pair
of factors defines a two way cross classification, and so on.

The function table() allows frequency tables to be calculated
from equal length factors.  If there are k factor arguments,
the result is a k-way array of frequencies.
 Suppose, for example, that statef is a factor giving the state
code for each entry in a data vector.  The assignment
 gives in statefr a table of frequencies of each state in the
sample.  The frequencies are ordered and labelled by the levels
attribute of the factor.  This simple case is equivalent to, but more
convenient than,
 Further suppose that incomef is a factor giving a suitably
defined “income class” for each entry in the data vector, for example
with the cut() function:
 Then to calculate a two-way table of frequencies:
 Extension to higher-way frequency tables is immediate.
 
Next: Reading data from files, Previous: Arrays and matrices, Up: Top   [Contents][Index] 
Next: Constructing and modifying lists, Previous: Lists and data frames, Up: Lists and data frames   [Contents][Index] An R list is an object consisting of an ordered collection of
objects known as its components.
 There is no particular need for the components to be of the same mode or
type, and, for example, a list could consist of a numeric vector, a
logical value, a matrix, a complex vector, a character array, a
function, and so on.  Here is a simple example of how to make a list:
 Components are always numbered and may always be referred to as
such.  Thus if Lst is the name of a list with four components,
these may be individually referred to as Lst[[1]],
Lst[[2]], Lst[[3]] and Lst[[4]].  If, further,
Lst[[4]] is a vector subscripted array then Lst[[4]][1] is
its first entry.
 If Lst is a list, then the function length(Lst) gives the
number of (top level) components it has.
 Components of lists may also be named, and in this case the
component may be referred to either by giving the component name as a
character string in place of the number in double square brackets, or,
more conveniently, by giving an expression of the form
 for the same thing.
 This is a very useful convention as it makes it easier to get the right
component if you forget the number.
 So in the simple example given above:
 Lst$name is the same as Lst[[1]] and is the string
"Fred",
 Lst$wife is the same as Lst[[2]] and is the string
"Mary",
 Lst$child.ages[1] is the same as Lst[[4]][1] and is the
number 4.
 Additionally, one can also use the names of the list components in
double square brackets, i.e., Lst[["name"]] is the same as
Lst$name.  This is especially useful, when the name of the
component to be extracted is stored in another variable as in
 It is very important to distinguish Lst[[1]] from Lst[1].
‘[[…]]’ is the operator used to select a single
element, whereas ‘[…]’ is a general subscripting
operator.  Thus the former is the first object in the list
Lst, and if it is a named list the name is not included.
The latter is a sublist of the list Lst consisting of the
first entry only.  If it is a named list, the names are transferred to
the sublist.
 The names of components may be abbreviated down to the minimum number of
letters needed to identify them uniquely.  Thus Lst$coefficients
may be minimally specified as Lst$coe and Lst$covariance
as Lst$cov.
 The vector of names is in fact simply an attribute of the list like any
other and may be handled as such.  Other structures besides lists may,
of course, similarly be given a names attribute also.
 
Next: Data frames, Previous: Lists, Up: Lists and data frames   [Contents][Index] New lists may be formed from existing objects by the function
list().  An assignment of the form
 sets up a list Lst of m components using object_1,
…, object_m for the components and giving them names as
specified by the argument names, (which can be freely chosen).  If these
names are omitted, the components are numbered only.  The components
used to form the list are copied when forming the new list and
the originals are not affected.
 Lists, like any subscripted object, can be extended by specifying
additional components.  For example
 
Previous: Constructing and modifying lists, Up: Constructing and modifying lists   [Contents][Index] When the concatenation function c() is given list arguments, the
result is an object of mode list also, whose components are those of the
argument lists joined together in sequence.
 Recall that with vector objects as arguments the concatenation function
similarly joined together all arguments into a single vector structure.
In this case all other attributes, such as dim attributes, are
discarded.
 
Previous: Constructing and modifying lists, Up: Lists and data frames   [Contents][Index] A data frame is a list with class "data.frame".  There are
restrictions on lists that may be made into data frames, namely
 A data frame may for many purposes be regarded as a matrix with columns
possibly of differing modes and attributes.  It may be displayed in
matrix form, and its rows and columns extracted using matrix indexing
conventions.
 
Next: attach() and detach(), Previous: Data frames, Up: Data frames   [Contents][Index] Objects satisfying the restrictions placed on the columns (components)
of a data frame may be used to form one using the function
data.frame:

 A list whose components conform to the restrictions of a data frame may
be coerced into a data frame using the function
as.data.frame()

 The simplest way to construct a data frame from scratch is to use the
read.table() function to read an entire data frame from an
external file.  This is discussed further in Reading data from files.
 
Next: Working with data frames, Previous: Making data frames, Up: Data frames   [Contents][Index] The $ notation, such as accountants$home, for list
components is not always very convenient.  A useful facility would be
somehow to make the components of a list or data frame temporarily
visible as variables under their component name, without the need to
quote the list name explicitly each time.
 The attach() function takes a ‘database’ such as a list or data
frame as its argument.  Thus suppose lentils is a
data frame with three variables lentils$u, lentils$v,
lentils$w.  The attach
 places the data frame in the search path at position 2, and provided
there are no variables u, v or w in position 1,
u, v and w are available as variables from the data
frame in their own right.  At this point an assignment such as
 does not replace the component u of the data frame, but rather
masks it with another variable u in the working directory at
position 1 on the search path.  To make a permanent change to the
data frame itself, the simplest way is to resort once again to the
$ notation:
 However the new value of component u is not visible until the
data frame is detached and attached again.
 To detach a data frame, use the function
 More precisely, this statement detaches from the search path the entity
currently at position 2.  Thus in the present context the variables
u, v and w would be no longer visible, except under
the list notation as lentils$u and so on.  Entities at positions
greater than 2 on the search path can be detached by giving their number
to detach, but it is much safer to always use a name, for example
by detach(lentils) or detach("lentils")
 Note: In R lists and data frames can only be attached at position 2 or
above, and what is attached is a copy of the original object.
You can alter the attached values via assign, but the
original list or data frame is unchanged.
 
Next: Attaching arbitrary lists, Previous: attach() and detach(), Up: Data frames   [Contents][Index] A useful convention that allows you to work with many different problems
comfortably together in the same working directory is
 In this way it is quite simple to work with many problems in the same
directory, all of which have variables named x, y and
z, for example.
 
Next: Managing the search path, Previous: Working with data frames, Up: Data frames   [Contents][Index] attach() is a generic function that allows not only directories
and data frames to be attached to the search path, but other classes of
object as well.  In particular any object of mode "list" may be
attached in the same way:
 Anything that has been attached can be detached by detach, by
position number or, preferably, by name.
 
Previous: Attaching arbitrary lists, Up: Data frames   [Contents][Index] The function search shows the current search path and so is
a very useful way to keep track of which data frames and lists (and
packages) have been attached and detached.  Initially it gives
 where .GlobalEnv is the workspace.19
 After lentils is attached we have
 and as we see ls (or objects) can be used to examine the
contents of any position on the search path.
 Finally, we detach the data frame and confirm it has been removed from
the search path.
 
Next: Probability distributions, Previous: Lists and data frames, Up: Top   [Contents][Index] Large data objects will usually be read as values from external files
rather than entered during an R session at the keyboard.  R input
facilities are simple and their requirements are fairly strict and even
rather inflexible.  There is a clear presumption by the designers of
R that you will be able to modify your input files using other tools,
such as file editors or Perl20 to fit in with the
requirements of R.  Generally this is very simple.
 If variables are to be held mainly in data frames, as we strongly
suggest they should be, an entire data frame can be read directly with
the read.table() function.  There is also a more primitive input
function, scan(), that can be called directly.
 For more details on importing data into R and also exporting data,
see the R Data Import/Export manual.
 
Next: The scan() function, Previous: Reading data from files, Up: Reading data from files   [Contents][Index] To read an entire data frame directly, the external file will normally
have a special form.
 If the file has one fewer item in its first line than in its second, this
arrangement is presumed to be in force.  So the first few lines of a file
to be read as a data frame might look as follows.
 By default numeric items (except row labels) are read as numeric
variables and non-numeric variables, such as Cent.heat in the
example, as factors.  This can be changed if necessary.
 The function read.table() can then be used to read the data frame
directly
 Often you will want to omit including the row labels directly and use the
default labels.  In this case the file may omit the row label column as in
the following.
 The data frame may then be read as
 where the header=TRUE option specifies that the first line is a
line of headings, and hence, by implication from the form of the file,
that no explicit row labels are given.
 
Next: Accessing builtin datasets, Previous: The read.table() function, Up: Reading data from files   [Contents][Index] Suppose the data vectors are of equal length and are to be read in
parallel.  Further suppose that there are three vectors, the first of
mode character and the remaining two of mode numeric, and the file is
input.dat.  The first step is to use scan() to read in the
three vectors as a list, as follows
 The second argument is a dummy list structure that establishes the mode
of the three vectors to be read.  The result, held in inp, is a
list whose components are the three vectors read in.  To separate the
data items into three separate vectors, use assignments like
 More conveniently, the dummy list can have named components, in which
case the names can be used to access the vectors read in.  For example
 If you wish to access the variables separately they may either be
re-assigned to variables in the working frame:
 or the list may be attached at position 2 of the search path
(see Attaching arbitrary lists).
 If the second argument is a single value and not a list, a single vector
is read in, all components of which must be of the same mode as the
dummy value.
 There are more elaborate input facilities available and these are
detailed in the manuals.
 
Next: Editing data, Previous: The scan() function, Up: Reading data from files   [Contents][Index] Around 100 datasets are supplied with R (in package datasets),
and others are available in packages (including the recommended packages
supplied with R).  To see the list of datasets currently available
use
 All the datasets supplied with R are available directly by name.
However, many packages still use the obsolete convention in which
data was also used to load datasets into R, for example
 and this can still be used with the standard packages (as in this
example).  In most cases this will load an R object of the same name.
However, in a few cases it loads several objects, so see the on-line
help for the object to see what to expect.
 To access data from a particular package, use the package
argument, for example
 If a package has been attached by library, its datasets are
automatically included in the search.
 User-contributed packages can be a rich source of datasets.
 
Previous: Accessing builtin datasets, Up: Reading data from files   [Contents][Index] When invoked on a data frame or matrix, edit brings up a separate
spreadsheet-like environment for editing.  This is useful for making
small changes once a data set has been read.  The command
 will allow you to edit your data set xold, and on completion the
changed object is assigned to xnew.  If you want to alter the
original dataset xold, the simplest way is to use
fix(xold), which is equivalent to xold <- edit(xold).
 Use
 to enter new data via the spreadsheet interface.
 
Next: Loops and conditional execution, Previous: Reading data from files, Up: Top   [Contents][Index] 
Next: Examining the distribution of a set of data, Previous: Probability distributions, Up: Probability distributions   [Contents][Index] One convenient use of R is to provide a comprehensive set of
statistical tables.  Functions are provided to evaluate the cumulative
distribution function P(X <= x),
the probability density function and the quantile function (given
q, the smallest x such that P(X <= x) > q),
and to simulate from the distribution.
 Prefix the name given here by ‘d’ for the density, ‘p’ for the
CDF, ‘q’ for the quantile function and ‘r’ for simulation
(random deviates).  The first argument is x for
dxxx, q for pxxx, p for
qxxx and n for rxxx (except for
rhyper, rsignrank and rwilcox, for which it is
nn).  In not quite all cases is the non-centrality parameter
ncp currently available: see the on-line help for details.
 The pxxx and qxxx functions all have logical
arguments lower.tail and log.p and the dxxx
ones have log.  This allows, e.g., getting the cumulative (or
“integrated”) hazard function, H(t) = - log(1 - F(t)), by
 or more accurate log-likelihoods (by dxxx(..., log =
TRUE)), directly.
 In addition there are functions ptukey and qtukey for the
distribution of the studentized range of samples from a normal
distribution, and dmultinom and rmultinom for the
multinomial distribution. Further distributions are available in
contributed packages, notably SuppDists.
 Here are some examples
 See the on-line help on RNG for how random-number generation is
done in R.
 
Next: One- and two-sample tests, Previous: R as a set of statistical tables, Up: Probability distributions   [Contents][Index] Given a (univariate) set of data we can examine its distribution in a
large number of ways.  The simplest is to examine the numbers.  Two
slightly different summaries are given by summary and
fivenum


and a display of the numbers by stem (a “stem and leaf” plot).

 A stem-and-leaf plot is like a histogram, and R has a function
hist to plot histograms.

 More elegant density plots can be made by density, and we added a
line produced by density in this example.  The bandwidth
bw was chosen by trial-and-error as the default gives too much
smoothing (it usually does for “interesting” densities).  (Better
automated methods of bandwidth choice are available, and in this example
bw = "SJ" gives a good result.)
 We can plot the empirical cumulative distribution function by using the
function ecdf.


 This distribution is obviously far from any standard distribution.
How about the right-hand mode, say eruptions of longer than 3 minutes?
Let us fit a normal distribution and overlay the fitted CDF.
 Quantile-quantile (Q-Q) plots can help us examine this more carefully.



 which shows a reasonable fit but a shorter right tail than one would
expect from a normal distribution.  Let us compare this with some
simulated data from a t distribution
 which will usually (if it is a random sample) show longer tails than
expected for a normal.  We can make a Q-Q plot against the generating
distribution by
 Finally, we might want a more formal test of agreement with normality
(or not).  R provides the Shapiro-Wilk test


 and the Kolmogorov-Smirnov test


 (Note that the distribution theory is not valid here as we
have estimated the parameters of the normal distribution from the same
sample.)
 
Previous: Examining the distribution of a set of data, Up: Probability distributions   [Contents][Index] So far we have compared a single sample to a normal distribution.  A
much more common operation is to compare aspects of two samples.  Note
that in R, all “classical” tests including the ones used below are
in package stats which is normally loaded.
 Consider the following sets of data on the latent heat of the fusion of
ice (cal/gm) from Rice (1995, p.490)
 Boxplots provide a simple graphical comparison of the two samples.
 which indicates that the first group tends to give higher results than
the second.
 To test for the equality of the means of the two examples, we can use
an unpaired t-test by


 which does indicate a significant difference, assuming normality.  By
default the R function does not assume equality of variances in the
two samples (in contrast to the similar S-PLUS t.test
function).  We can use the F test to test for equality in the variances,
provided that the two samples are from normal populations.
 which shows no evidence of a significant difference, and so we can use
the classical t-test that assumes equality of the variances.
 All these tests assume normality of the two samples.  The two-sample
Wilcoxon (or Mann-Whitney) test only assumes a common continuous
distribution under the null hypothesis.
 Note the warning: there are several ties in each sample, which suggests
strongly that these data are from a discrete distribution (probably due
to rounding).
 There are several ways to compare graphically the two samples.  We have
already seen a pair of boxplots.  The following
 will show the two empirical CDFs, and qqplot will perform a Q-Q
plot of the two samples.  The Kolmogorov-Smirnov test is of the maximal
vertical distance between the two ecdf’s, assuming a common continuous
distribution:
 
Next: Writing your own functions, Previous: Probability distributions, Up: Top   [Contents][Index] 
Next: Control statements, Previous: Loops and conditional execution, Up: Loops and conditional execution   [Contents][Index] R is an expression language in the sense that its only command type
is a function or expression which returns a result.  Even an assignment
is an expression whose result is the value assigned, and it may be used
wherever any expression may be used; in particular multiple assignments
are possible.
 Commands may be grouped together in braces, {expr_1;
…; expr_m}, in which case the value of the group
is the result of the last expression in the group evaluated.  Since such
a group is also an expression it may, for example, be itself included in
parentheses and used a part of an even larger expression, and so on.
 
Previous: Grouped expressions, Up: Loops and conditional execution   [Contents][Index] 
Next: Repetitive execution, Previous: Control statements, Up: Control statements   [Contents][Index] The language has available a conditional construction of the form
 where expr_1 must evaluate to a single logical value and the
result of the entire expression is then evident.
 The “short-circuit” operators && and || are often used
as part of the condition in an if statement.  Whereas &
and | apply element-wise to vectors, && and ||
apply to vectors of length one, and only evaluate their second argument
if necessary.
 There is a vectorized version of the if/else construct,
the ifelse function.  This has the form ifelse(condition, a,
b) and returns a vector of the length of its longest argument, with
elements a[i] if condition[i] is true, otherwise
b[i].
 
Previous: Conditional execution, Up: Control statements   [Contents][Index] There is also a for loop construction which has the form
 where name is the loop variable.  expr_1 is a
vector expression, (often a sequence like 1:20), and
expr_2 is often a grouped expression with its sub-expressions
written in terms of the dummy name.  expr_2 is repeatedly
evaluated as name ranges through the values in the vector result
of expr_1.
 As an example, suppose ind is a vector of class indicators and we
wish to produce separate plots of y versus x within
classes.  One possibility here is to use coplot(),21
which will produce an array of plots corresponding to each level of the
factor.  Another way to do this, now putting all plots on the one
display, is as follows:
 (Note the function split() which produces a list of vectors
obtained by splitting a larger vector according to the classes specified
by a factor.  This is a useful function, mostly used in connection
with boxplots.  See the help facility for further details.)
 Warning: for() loops are used in R code much less
often than in compiled languages.  Code that takes a ‘whole object’ view
is likely to be both clearer and faster in R.
 Other looping facilities include the
 statement and the
 statement.
 The break statement can be used to terminate any loop, possibly
abnormally.  This is the only way to terminate repeat loops.

 The next statement can be used to discontinue one particular
cycle and skip to the “next”.

 Control statements are most often used in connection with
functions which are discussed in Writing your own functions, and where more examples will emerge.
 
Next: Statistical models in R, Previous: Loops and conditional execution, Up: Top   [Contents][Index] As we have seen informally along the way, the R language allows the
user to create objects of mode function.  These are true R
functions that are stored in a special internal form and may be used in
further expressions and so on.  In the process, the language gains
enormously in power, convenience and elegance, and learning to write
useful functions is one of the main ways to make your use of R
comfortable and productive.
 It should be emphasized that most of the functions supplied as part of
the R system, such as mean(), var(),
postscript() and so on, are themselves written in R and thus
do not differ materially from user written functions.
 A function is defined by an assignment of the form
 The expression is an R expression, (usually a grouped
expression), that uses the arguments, arg_i, to calculate a value.
The value of the expression is the value returned for the function.
 A call to the function then usually takes the form
name(expr_1, expr_2, …) and may occur
anywhere a function call is legitimate.
 
Next: Defining new binary operators, Previous: Writing your own functions, Up: Writing your own functions   [Contents][Index] As a first example, consider a function to calculate the two sample
t-statistic, showing “all the steps”.  This is an artificial
example, of course, since there are other, simpler ways of achieving the
same end.
 The function is defined as follows:
 With this function defined, you could perform two sample t-tests
using a call such as
 As a second example, consider a function to emulate directly the
MATLAB backslash command, which returns the coefficients of the
orthogonal projection of the vector y onto the column space of
the matrix, X.  (This is ordinarily called the least squares
estimate of the regression coefficients.)  This would ordinarily be
done with the qr() function; however this is sometimes a bit
tricky to use directly and it pays to have a simple function such as the
following to use it safely.
 Thus given a n by 1 vector y and an n by
p matrix X then X \ y is defined as
(X’X)^{-}X’y, where (X’X)^{-}
is a generalized inverse of X'X.
 After this object is created it may be used in statements such as
 and so on.
 The classical R function lsfit() does this job quite well, and
more22.  It in turn uses the functions qr() and qr.coef()
in the slightly counterintuitive way above to do this part of the
calculation.  Hence there is probably some value in having just this
part isolated in a simple to use function if it is going to be in
frequent use.  If so, we may wish to make it a matrix binary operator
for even more convenient use.
 
Next: Named arguments and defaults, Previous: Simple examples, Up: Writing your own functions   [Contents][Index] Had we given the bslash() function a different name, namely one of
the form
 it could have been used as a binary operator in expressions
rather than in function form.  Suppose, for example, we choose !
for the internal character.  The function definition would then start as
 (Note the use of quote marks.)  The function could then be used as
X %!% y.  (The backslash symbol itself is not a convenient choice
as it presents special problems in this context.)
 The matrix multiplication operator, %*%, and the outer product
matrix operator %o% are other examples of binary operators
defined in this way.
 
Next: The three dots argument, Previous: Defining new binary operators, Up: Writing your own functions   [Contents][Index] As first noted in Generating regular sequences, if arguments to
called functions are given in the “name=object”
form, they may be given in any order.  Furthermore the argument sequence
may begin in the unnamed, positional form, and specify named arguments
after the positional arguments.
 Thus if there is a function fun1 defined by
 then the function may be invoked in several ways, for example
 are all equivalent.
 In many cases arguments can be given commonly appropriate default
values, in which case they may be omitted altogether from the call when
the defaults are appropriate.  For example, if fun1 were defined
as
 it could be called as
 which is now equivalent to the three cases above, or as
 which changes one of the defaults.
 It is important to note that defaults may be arbitrary expressions, even
involving other arguments to the same function; they are not restricted
to be constants as in our simple example here.
 
Next: Assignment within functions, Previous: Named arguments and defaults, Up: Writing your own functions   [Contents][Index] Another frequent requirement is to allow one function to pass on
argument settings to another.  For example many graphics functions use
the function par() and functions like plot() allow the
user to pass on graphical parameters to par() to control the
graphical output.  (See The par() function, for more details on the
par() function.)  This can be done by including an extra
argument, literally ‘…’, of the function, which may then be
passed on.  An outline example is given below.
 Less frequently, a function will need to refer to components of
‘…’.  The expression list(...) evaluates all such
arguments and returns them in a named list, while ..1,
..2, etc. evaluate them one at a time, with ‘..n’
returning the n’th unmatched argument.
 
Next: More advanced examples, Previous: The three dots argument, Up: Writing your own functions   [Contents][Index] Note that any ordinary assignments done within the function are
local and temporary and are lost after exit from the function.  Thus
the assignment X <- qr(X) does not affect the value of the
argument in the calling program.
 To understand completely the rules governing the scope of R assignments
the reader needs to be familiar with the notion of an evaluation
frame.  This is a somewhat advanced, though hardly difficult,
topic and is not covered further here.
 If global and permanent assignments are intended within a function, then
either the “superassignment” operator, <<- or the function
assign() can be used.  See the help document for details.
S-PLUS users should be aware that <<- has different semantics
in R.  These are discussed further in Scope.
 
Next: Scope, Previous: Assignment within functions, Up: Writing your own functions   [Contents][Index] 
Next: Dropping all names in a printed array, Previous: More advanced examples, Up: More advanced examples   [Contents][Index] As a more complete, if a little pedestrian, example of a function,
consider finding the efficiency factors for a block design.  (Some
aspects of this problem have already been discussed in Index matrices.)
 A block design is defined by two factors, say blocks (b
levels) and varieties (v levels).  If R and
K are the v by v and b by b
replications and block size matrices, respectively, and
N is the b by v incidence matrix, then the
efficiency factors are defined as the eigenvalues of the matrix
E = I_v - R^{-1/2}N’K^{-1}NR^{-1/2} = I_v - A’A, where
A = K^{-1/2}NR^{-1/2}.
One way to write the function is given below.
 It is numerically slightly better to work with the singular value
decomposition on this occasion rather than the eigenvalue routines.
 The result of the function is a list giving not only the efficiency
factors as the first component, but also the block and variety canonical
contrasts, since sometimes these give additional useful qualitative
information.
 
Next: Recursive numerical integration, Previous: Efficiency factors in block designs, Up: More advanced examples   [Contents][Index] For printing purposes with large matrices or arrays, it is often useful
to print them in close block form without the array names or numbers.
Removing the dimnames attribute will not achieve this effect, but
rather the array must be given a dimnames attribute consisting of
empty strings.  For example to print a matrix, X
 This can be much more conveniently done using a function,
no.dimnames(), shown below, as a “wrap around” to achieve the
same result.  It also illustrates how some effective and useful user
functions can be quite short.
 With this function defined, an array may be printed in close format
using
 This is particularly useful for large integer arrays, where patterns are
the real interest rather than the values.
 
Previous: Dropping all names in a printed array, Up: More advanced examples   [Contents][Index] Functions may be recursive, and may themselves define functions within
themselves.  Note, however, that such functions, or indeed variables,
are not inherited by called functions in higher evaluation frames as
they would be if they were on the search path.
 The example below shows a naive way of performing one-dimensional
numerical integration.  The integrand is evaluated at the end points of
the range and in the middle.  If the one-panel trapezium rule answer is
close enough to the two panel, then the latter is returned as the value.
Otherwise the same process is recursively applied to each panel.  The
result is an adaptive integration process that concentrates function
evaluations in regions where the integrand is farthest from linear.
There is, however, a heavy overhead, and the function is only
competitive with other algorithms when the integrand is both smooth and
very difficult to evaluate.
 The example is also given partly as a little puzzle in R programming.
 
Next: Customizing the environment, Previous: More advanced examples, Up: Writing your own functions   [Contents][Index] The discussion in this section is somewhat more technical than in other
parts of this document.  However, it details one of the major differences
between S-PLUS and R.
 The symbols which occur in the body of a function can be divided into
three classes; formal parameters, local variables and free variables.
The formal parameters of a function are those occurring in the argument
list of the function.  Their values are determined by the process of
binding the actual function arguments to the formal parameters.
Local variables are those whose values are determined by the evaluation
of expressions in the body of the functions.  Variables which are not
formal parameters or local variables are called free variables.  Free
variables become local variables if they are assigned to.  Consider the
following function definition.
 In this function, x is a formal parameter, y is a local
variable and z is a free variable.
 In R the free variable bindings are resolved by first looking in the
environment in which the function was created.  This is called
lexical scope.  First we define a function called cube.
 The variable n in the function sq is not an argument to that
function.  Therefore it is a free variable and the scoping rules must be
used to ascertain the value that is to be associated with it.  Under static
scope (S-PLUS) the value is that associated with a global variable named
n.  Under lexical scope (R) it is the parameter to the function
cube since that is the active binding for the variable n at
the time the function sq was defined.  The difference between
evaluation in R and evaluation in S-PLUS is that S-PLUS looks for a
global variable called n while R first looks for a variable
called n in the environment created when cube was invoked.
 Lexical scope can also be used to give functions mutable state.
In the following example we show how R can be used to mimic a bank
account.  A functioning bank account needs to have a balance or total, a
function for making withdrawals, a function for making deposits and a
function for stating the current balance.  We achieve this by creating
the three functions within account and then returning a list
containing them.  When account is invoked it takes a numerical
argument total and returns a list containing the three functions.
Because these functions are defined in an environment which contains
total, they will have access to its value.
 The special assignment operator, <<-,

is used to change the value associated with total.  This operator
looks back in enclosing environments for an environment that contains
the symbol total and when it finds such an environment it
replaces the value, in that environment, with the value of right hand
side.  If the global or top-level environment is reached without finding
the symbol total then that variable is created and assigned to
there.  For most users <<- creates a global variable and assigns
the value of the right hand side to it23.  Only when <<- has
been used in a function that was returned as the value of another
function will the special behavior described here occur.
 
Next: Object orientation, Previous: Scope, Up: Writing your own functions   [Contents][Index] Users can customize their environment in several different ways.  There
is a site initialization file and every directory can have its own
special initialization file.  Finally, the special functions
.First and .Last can be used.
 The location of the site initialization file is taken from the value of
the R_PROFILE environment variable.  If that variable is unset,
the file Rprofile.site in the R home subdirectory etc is
used.  This file should contain the commands that you want to execute
every time R is started under your system.  A second, personal,
profile file named .Rprofile24 can be placed in any directory.  If R is invoked in that
directory then that file will be sourced.  This file gives individual
users control over their workspace and allows for different startup
procedures in different working directories.  If no .Rprofile
file is found in the startup directory, then R looks for a
.Rprofile file in the user’s home directory and uses that (if it
exists).  If the environment variable R_PROFILE_USER is set, the
file it points to is used instead of the .Rprofile files.
 Any function named .First() in either of the two profile files or
in the .RData image has a special status.  It is automatically
performed at the beginning of an R session and may be used to
initialize the environment.  For example, the definition in the example
below alters the prompt to $ and sets up various other useful
things that can then be taken for granted in the rest of the session.
 Thus, the sequence in which files are executed is, Rprofile.site,
the user profile, .RData and then .First().  A definition
in later files will mask definitions in earlier files.
 Similarly a function .Last(), if defined, is (normally) executed
at the very end of the session.  An example is given below.
 
Previous: Customizing the environment, Up: Writing your own functions   [Contents][Index] The class of an object determines how it will be treated by what are
known as generic functions.  Put the other way round, a generic
function performs a task or action on its arguments specific to
the class of the argument itself.  If the argument lacks any class
attribute, or has a class not catered for specifically by the generic
function in question, there is always a default action provided.
 An example makes things clearer.  The class mechanism offers the user
the facility of designing and writing generic functions for special
purposes.  Among the other generic functions are plot() for
displaying objects graphically, summary() for summarizing
analyses of various types, and anova() for comparing statistical
models.
 The number of generic functions that can treat a class in a specific way
can be quite large.  For example, the functions that can accommodate in
some fashion objects of class "data.frame" include
 A currently complete list can be got by using the methods()
function:
 Conversely the number of classes a generic function can handle can also
be quite large.  For example the plot() function has a default
method and variants for objects of classes "data.frame",
"density", "factor", and more.  A complete list can be got
again by using the methods() function:
 For many generic functions the function body is quite short, for example
 The presence of UseMethod indicates this is a generic function.
To see what methods are available we can use methods()
 In this example there are six methods, none of which can be seen by
typing its name.  We can read these by either of
 A function named gen.cl will be invoked by the
generic gen for class cl, so do not name
functions in this style unless they are intended to be methods.
 The reader is referred to the R Language Definition for a more
complete discussion of this mechanism.
 
Next: Graphics, Previous: Writing your own functions, Up: Top   [Contents][Index] This section presumes the reader has some familiarity with statistical
methodology, in particular with regression analysis and the analysis of
variance.  Later we make some rather more ambitious presumptions, namely
that something is known about generalized linear models and nonlinear
regression.
 The requirements for fitting statistical models are sufficiently well
defined to make it possible to construct general tools that apply in a
broad spectrum of problems.
 R provides an interlocking suite of facilities that make fitting
statistical models very simple.  As we mention in the introduction, the
basic output is minimal, and one needs to ask for the details by calling
extractor functions.
 
Next: Linear models, Previous: Statistical models in R, Up: Statistical models in R   [Contents][Index] The template for a statistical model is a linear regression model with
independent, homoscedastic errors
 where the e_i are NID(0, sigma^2).
In matrix terms this would be written
 where the y is the response vector, X is the model
matrix or design matrix and has columns
x_0, x_1, …, x_p,
the determining variables.  Very often x_0
will be a column of ones defining an intercept term.
 Before giving a formal specification, a few examples may usefully set
the picture.
 Suppose y, x, x0, x1, x2, … are
numeric variables, X is a matrix and A, B,
C, … are factors.  The following formulae on the left
side below specify statistical models as described on the right.
 Both imply the same simple linear regression model of y on
x.  The first has an implicit intercept term, and the second an
explicit one.
 Simple linear regression of y on x through the origin
(that is, without an intercept term).
 Multiple regression of the transformed variable,
log(y),
on x1 and x2 (with an implicit intercept term).
 Polynomial regression of y on x of degree 2.  The first
form uses orthogonal polynomials, and the second uses explicit powers,
as basis.
 Multiple regression y with model matrix consisting of the matrix
X as well as polynomial terms in x to degree 2.
 Single classification analysis of variance model of y, with
classes determined by A.
 Single classification analysis of covariance model of y, with
classes determined by A, and with covariate x.
 Two factor non-additive model of y on A and B.  The
first two specify the same crossed classification and the second two
specify the same nested classification.  In abstract terms all four
specify the same model subspace.
 Three factor experiment but with a model containing main effects and two
factor interactions only.  Both formulae specify the same model.
 Separate simple linear regression models of y on x within
the levels of A, with different codings.  The last form produces
explicit estimates of as many different intercepts and slopes as there
are levels in A.
 An experiment with two treatment factors, A and B, and
error strata determined by factor C.  For example a split plot
experiment, with whole plots (and hence also subplots), determined by
factor C.
 The operator ~ is used to define a model formula in R.
The form, for an ordinary linear model, is
 where
 is a vector or matrix, (or expression evaluating to a vector or matrix)
defining the response variable(s).
 is an operator, either + or -, implying the inclusion or
exclusion of a term in the model, (the first is optional).
 is either
 In all cases each term defines a collection of columns either to be
added to or removed from the model matrix.  A 1 stands for an
intercept column and is by default included in the model matrix unless
explicitly removed.
 The formula operators are similar in effect to the Wilkinson and
Rogers notation used by such programs as Glim and Genstat.  One
inevitable change is that the operator ‘.’ becomes
‘:’ since the period is a valid name character in R.
 The notation is summarized below (based on Chambers & Hastie, 1992,
p.29):
 Y is modeled as M.
 Include M_1 and M_2.
 Include M_1 leaving out terms of M_2.
 The tensor product of M_1 and M_2.  If both terms are
factors, then the “subclasses” factor.
 Similar to M_1:M_2, but with a different coding.
 M_1 + M_2 + M_1:M_2.
 M_1 + M_2 %in% M_1.
 All terms in M together with “interactions” up to order n
 Insulate M.  Inside M all operators have their normal
arithmetic meaning, and that term appears in the model matrix.
 Note that inside the parentheses that usually enclose function arguments
all operators have their normal arithmetic meaning.  The function
I() is an identity function used to allow terms in model formulae
to be defined using arithmetic operators.
 Note particularly that the model formulae specify the columns
of the model matrix, the specification of the parameters being
implicit.  This is not the case in other contexts, for example in
specifying nonlinear models.
 
Previous: Formulae for statistical models, Up: Formulae for statistical models   [Contents][Index] We need at least some idea how the model formulae specify the columns of
the model matrix.  This is easy if we have continuous variables, as each
provides one column of the model matrix (and the intercept will provide
a column of ones if included in the model).
 What about a k-level factor A?  The answer differs for
unordered and ordered factors.  For unordered factors k -
1 columns are generated for the indicators of the second, …,
kth levels of the factor. (Thus the implicit parameterization is
to contrast the response at each level with that at the first.)  For
ordered factors the k - 1 columns are the orthogonal
polynomials on 1, …, k, omitting the constant term.
 Although the answer is already complicated, it is not the whole story.
First, if the intercept is omitted in a model that contains a factor
term, the first such term is encoded into k columns giving the
indicators for all the levels.  Second, the whole behavior can be
changed by the options setting for contrasts.  The default
setting in R is
 The main reason for mentioning this is that R and S have
different defaults for unordered factors, S using Helmert
contrasts.  So if you need to compare your results to those of a textbook
or paper which used S-PLUS, you will need to set
 This is a deliberate difference, as treatment contrasts (R’s default)
are thought easier for newcomers to interpret.
 We have still not finished, as the contrast scheme to be used can be set
for each term in the model using the functions contrasts and
C.


 We have not yet considered interaction terms: these generate the
products of the columns introduced for their component terms.
 Although the details are complicated, model formulae in R will
normally generate the models that an expert statistician would expect,
provided that marginality is preserved.  Fitting, for example, a model
with an interaction but not the corresponding main effects will in
general lead to surprising results, and is for experts only.
 
Next: Generic functions for extracting model information, Previous: Formulae for statistical models, Up: Statistical models in R   [Contents][Index] The basic function for fitting ordinary multiple models is lm(),
and a streamlined version of the call is as follows:

 For example
 would fit a multiple regression model of y on x1 and
x2 (with implicit intercept term).
 The important (but technically optional) parameter data =
production specifies that any variables needed to construct the model
should come first from the production data frame.
This is the case regardless of whether data frame
production has been attached on the search path or not.
 
Next: Analysis of variance and model comparison, Previous: Linear models, Up: Statistical models in R   [Contents][Index] The value of lm() is a fitted model object; technically a list of
results of class "lm".  Information about the fitted model can
then be displayed, extracted, plotted and so on by using generic
functions that orient themselves to objects of class "lm".  These
include
 A brief description of the most commonly used ones is given below.
 Compare a submodel with an outer model and produce an analysis of
variance table.
 Extract the regression coefficient (matrix).
 Long form: coefficients(object).
 Residual sum of squares, weighted if appropriate.
 Extract the model formula.
 Produce four plots, showing residuals, fitted values and some
diagnostics.
 The data frame supplied must have variables specified with the same
labels as the original.  The value is a vector or matrix of predicted
values corresponding to the determining variable values in
data.frame.
 Print a concise version of the object.  Most often used implicitly.
 Extract the (matrix of) residuals, weighted as appropriate.
 Short form: resid(object).
 Select a suitable model by adding or dropping terms and preserving
hierarchies.  The model with the smallest value of AIC (Akaike’s An
Information Criterion) discovered in the stepwise search is returned.
 Print a comprehensive summary of the results of the regression analysis.
 Returns the variance-covariance matrix of the main parameters of a
fitted model object.
 
Next: Updating fitted models, Previous: Generic functions for extracting model information, Up: Statistical models in R   [Contents][Index] The model fitting function aov(formula,
data=data.frame)

operates at the simplest level in a very similar way to the function
lm(), and most of the generic functions listed in the table in
Generic functions for extracting model information apply.
 It should be noted that in addition aov() allows an analysis of
models with multiple error strata such as split plot experiments, or
balanced incomplete block designs with recovery of inter-block
information.  The model formula
 specifies a multi-stratum experiment with error strata defined by the
strata.formula.  In the simplest case, strata.formula is
simply a factor, when it defines a two strata experiment, namely between
and within the levels of the factor.
 For example, with all determining variables factors, a model formula such
as that in:
 would typically be used to describe an experiment with mean model
v + n*p*k and three error strata, namely “between farms”,
“within farms, between blocks” and “within blocks”.
 
Previous: Analysis of variance and model comparison, Up: Analysis of variance and model comparison   [Contents][Index] Note also that the analysis of variance table (or tables) are for a
sequence of fitted models.  The sums of squares shown are the decrease
in the residual sums of squares resulting from an inclusion of
that term in the model at that place in the sequence.
Hence only for orthogonal experiments will the order of inclusion be
inconsequential.
 For multistratum experiments the procedure is first to project the
response onto the error strata, again in sequence, and to fit the mean
model to each projection.  For further details, see Chambers & Hastie
(1992).
 A more flexible alternative to the default full ANOVA table is to
compare two or more models directly using the anova() function.

 The display is then an ANOVA table showing the differences between the
fitted models when fitted in sequence.  The fitted models being compared
would usually be an hierarchical sequence, of course.  This does not
give different information to the default, but rather makes it easier to
comprehend and control.
 
Next: Generalized linear models, Previous: Analysis of variance and model comparison, Up: Statistical models in R   [Contents][Index] The update() function is largely a convenience function that
allows a model to be fitted that differs from one previously fitted
usually by just a few additional or removed terms.  Its form is

 In the new.formula the special name consisting of a period,
‘.’,

only, can be used to stand for “the corresponding part of the old model
formula”.  For example,
 would fit a five variate multiple regression with variables (presumably)
from the data frame production, fit an additional model including
a sixth regressor variable, and fit a variant on the model where the
response had a square root transform applied.
 Note especially that if the data= argument is specified on the
original call to the model fitting function, this information is passed on
through the fitted model object to update() and its allies.
 The name ‘.’ can also be used in other contexts, but with slightly
different meaning.  For example
 would fit a model with response y and regressor variables
all other variables in the data frame production.
 Other functions for exploring incremental sequences of models are
add1(), drop1() and step().



The names of these give a good clue to their purpose, but for full
details see the on-line help.
 
Next: Nonlinear least squares and maximum likelihood models, Previous: Updating fitted models, Up: Statistical models in R   [Contents][Index] Generalized linear modeling is a development of linear models to
accommodate both non-normal response distributions and transformations
to linearity in a clean and straightforward way.  A generalized linear
model may be described in terms of the following sequence of
assumptions:
 hence x_i has no influence on the distribution of y if and only if
beta_i is zero.
 where phi is a scale parameter (possibly known), and is constant
for all observations, A represents a prior weight, assumed known
but possibly varying with the observations, and $\mu$ is the mean of
y.
So it is assumed that the distribution of y is determined by its
mean and possibly a scale parameter as well.
 and this inverse function, ell(), is called the link function.
 These assumptions are loose enough to encompass a wide class of models
useful in statistical practice, but tight enough to allow the
development of a unified methodology of estimation and inference, at
least approximately.  The reader is referred to any of the current
reference works on the subject for full details, such as McCullagh &
Nelder (1989) or Dobson (1990).
 
Next: The glm() function, Previous: Generalized linear models, Up: Generalized linear models   [Contents][Index] The class of generalized linear models handled by facilities supplied in
R includes gaussian, binomial, poisson,
inverse gaussian and gamma response distributions and also
quasi-likelihood models where the response distribution is not
explicitly specified.  In the latter case the variance function
must be specified as a function of the mean, but in other cases this
function is implied by the response distribution.
 Each response distribution admits a variety of link functions to connect
the mean with the linear predictor.  Those automatically available are
shown in the following table:
 The combination of a response distribution, a link function and various
other pieces of information that are needed to carry out the modeling
exercise is called the family of the generalized linear model.
 
Previous: Families, Up: Generalized linear models   [Contents][Index] Since the distribution of the response depends on the stimulus variables
through a single linear function only, the same mechanism as was
used for linear models can still be used to specify the linear part of a
generalized model.  The family has to be specified in a different way.
 The R function to fit a generalized linear model is glm()
which uses the form
 The only new feature is the family.generator, which is the
instrument by which the family is described.  It is the name of a
function that generates a list of functions and expressions that
together define and control the model and estimation process.  Although
this may seem a little complicated at first sight, its use is quite
simple.
 The names of the standard, supplied family generators are given under
“Family Name” in the table in Families.  Where there is a choice
of links, the name of the link may also be supplied with the family
name, in parentheses as a parameter.  In the case of the quasi
family, the variance function may also be specified in this way.
 Some examples make the process clear.
 A call such as
 achieves the same result as
 but much less efficiently.  Note how the gaussian family is not
automatically provided with a choice of links, so no parameter is
allowed.  If a problem requires a gaussian family with a nonstandard
link, this can usually be achieved through the quasi family, as
we shall see later.
 Consider a small, artificial example, from Silvey (1970).
 On the Aegean island of Kalythos the male inhabitants suffer from a
congenital eye disease, the effects of which become more marked with
increasing age.  Samples of islander males of various ages were tested
for blindness and the results recorded.  The data is shown below:
 The problem we consider is to fit both logistic and probit models to
this data, and to estimate for each model the LD50, that is the age at
which the chance of blindness for a male inhabitant is 50%.
 If y is the number of blind at age x and n the
number tested, both models have the form
y ~ B(n, F(beta_0 + beta_1 x))
where for the probit case,
F(z) = Phi(z)
is the standard normal distribution function, and in the logit case
(the default),
F(z) = e^z/(1+e^z).
In both cases the LD50 is
LD50 = - beta_0/beta_1
that is, the point at which the argument of the distribution function is
zero.
 The first step is to set the data up as a data frame
 To fit a binomial model using glm() there are three possibilities
for the response:
 Here we need the second of these conventions, so we add a matrix to our
data frame:
 To fit the models we use
 Since the logit link is the default the parameter may be omitted on the
second call.  To see the results of each fit we could use
 Both models fit (all too) well.  To find the LD50 estimate we can use a
simple function:
 The actual estimates from this data are 43.663 years and 43.601 years
respectively.
 With the Poisson family the default link is the log, and in
practice the major use of this family is to fit surrogate Poisson
log-linear models to frequency data, whose actual distribution is often
multinomial.  This is a large and important subject we will not discuss
further here.  It even forms a major part of the use of non-gaussian
generalized models overall.
 Occasionally genuinely Poisson data arises in practice and in the past
it was often analyzed as gaussian data after either a log or a
square-root transformation.  As a graceful alternative to the latter, a
Poisson generalized linear model may be fitted as in the following
example:
 For all families the variance of the response will depend on the mean
and will have the scale parameter as a multiplier.  The form of
dependence of the variance on the mean is a characteristic of the
response distribution; for example for the poisson distribution
Var(y) = mu.
 For quasi-likelihood estimation and inference the precise response
distribution is not specified, but rather only a link function and the
form of the variance function as it depends on the mean.  Since
quasi-likelihood estimation uses formally identical techniques to those
for the gaussian distribution, this family provides a way of fitting
gaussian models with non-standard link functions or variance functions,
incidentally.
 For example, consider fitting the non-linear regression
y = theta_1 z_1 / (z_2 - theta_2) + e
which may be written alternatively as
y = 1 / (beta_1 x_1 + beta_2 x_2) + e
where
x_1 = z_2/z_1, x_2 = -1/z_1, beta_1 = 1/theta_1, and beta_2 =
theta_2/theta_1.
Supposing a suitable data frame to be set up we could fit this
non-linear regression as
 The reader is referred to the manual and the help document for further
information, as needed.
 
Next: Some non-standard models, Previous: Generalized linear models, Up: Statistical models in R   [Contents][Index] Certain forms of nonlinear model can be fitted by Generalized Linear
Models (glm()).  But in the majority of cases we have to approach
the nonlinear curve fitting problem as one of nonlinear optimization.
R’s nonlinear optimization routines are optim(), nlm()
and nlminb(),



which provide the functionality (and more) of S-PLUS’s ms() and
nlminb().  We seek the parameter values that minimize some index
of lack-of-fit, and they do this by trying out various parameter values
iteratively.  Unlike linear regression for example, there is no
guarantee that the procedure will converge on satisfactory estimates.
All the methods require initial guesses about what parameter values to
try, and convergence may depend critically upon the quality of the
starting values.
 
Next: Maximum likelihood, Previous: Nonlinear least squares and maximum likelihood models, Up: Nonlinear least squares and maximum likelihood models   [Contents][Index] One way to fit a nonlinear model is by minimizing the sum of the squared
errors (SSE) or residuals.  This method makes sense if the observed
errors could have plausibly arisen from a normal distribution.
 Here is an example from Bates & Watts (1988), page 51.  The data are:
 The fit criterion to be minimized is:
 In order to do the fit we need initial estimates of the parameters.  One
way to find sensible starting values is to plot the data, guess some
parameter values, and superimpose the model curve using those values.
 We could do better, but these starting values of 200 and 0.1 seem
adequate.  Now do the fit:
 After the fitting, out$minimum is the SSE, and
out$estimate are the least squares estimates of the parameters.
To obtain the approximate standard errors (SE) of the estimates we do:
 The 2 which is subtracted in the line above represents the number
of parameters.  A 95% confidence interval would be the parameter
estimate +/- 1.96 SE.  We can superimpose the least squares
fit on a new plot:
 The standard package stats provides much more extensive facilities
for fitting non-linear models by least squares.  The model we have just
fitted is the Michaelis-Menten model, so we can use
 
Previous: Least squares, Up: Nonlinear least squares and maximum likelihood models   [Contents][Index] Maximum likelihood is a method of nonlinear model fitting that applies
even if the errors are not normal.  The method finds the parameter values
which maximize the log likelihood, or equivalently which minimize the
negative log-likelihood.  Here is an example from Dobson (1990), pp.
108–111.  This example fits a logistic model to dose-response data,
which clearly could also be fit by glm().  The data are:
 The negative log-likelihood to minimize is:
 We pick sensible starting values and do the fit:
 After the fitting, out$minimum is the negative log-likelihood,
and out$estimate are the maximum likelihood estimates of the
parameters.  To obtain the approximate SEs of the estimates we do:
 A 95% confidence interval would be the parameter estimate +/-
1.96 SE.
 
Previous: Nonlinear least squares and maximum likelihood models, Up: Statistical models in R   [Contents][Index] We conclude this chapter with just a brief mention of some of the other
facilities available in R for special regression and data analysis
problems.
 Function loess is in the standard package stats, together
with code for projection pursuit regression.

 Models are again specified in the ordinary linear model form.  The model
fitting function is tree(),

but many other generic functions such as plot() and text()
are well adapted to displaying the results of a tree-based model fit in
a graphical way.
 Tree models are available in R via the user-contributed
packages rpart and tree.
 
Next: Packages, Previous: Statistical models in R, Up: Top   [Contents][Index] Graphical facilities are an important and extremely versatile component
of the R environment.  It is possible to use the facilities to
display a wide variety of statistical graphs and also to build entirely
new types of graph.
 The graphics facilities can be used in both interactive and batch modes,
but in most cases, interactive use is more productive.  Interactive use
is also easy because at startup time R initiates a graphics
device driver which opens a special graphics window for
the display of interactive graphics.  Although this is done
automatically, it may useful to know that the command used is
X11() under UNIX, windows() under Windows and
quartz() under OS X.  A new device can always be opened by
dev.new().
 Once the device driver is running, R plotting commands can be used to
produce a variety of graphical displays and to create entirely new kinds
of display.
 Plotting commands are divided into three basic groups:
 In addition, R maintains a list of graphical parameters which
can be manipulated to customize your plots.
 This manual only describes what are known as ‘base’ graphics.  A
separate graphics sub-system in package grid coexists with base –
it is more powerful but harder to use.  There is a recommended package
lattice which builds on grid and provides ways to produce
multi-panel plots akin to those in the Trellis system in S.
 
Next: Low-level plotting commands, Previous: Graphics, Up: Graphics   [Contents][Index] High-level plotting functions are designed to generate a complete plot
of the data passed as arguments to the function.  Where appropriate,
axes, labels and titles are automatically generated (unless you request
otherwise.) High-level plotting commands always start a new plot,
erasing the current plot if necessary.
 
Next: Displaying multivariate data, Previous: High-level plotting commands, Up: High-level plotting commands   [Contents][Index] One of the most frequently used plotting functions in R is the
plot() function.  This is a generic function: the type of
plot produced is dependent on the type or class of the first
argument.
 If x and y are vectors, plot(x, y)
produces a scatterplot of y against x.  The same effect can
be produced by supplying one argument (second form) as either a list
containing two elements x and y or a two-column matrix.
 If x is a time series, this produces a time-series plot. If
x is a numeric vector, it produces a plot of the values in the
vector against their index in the vector.  If x is a complex
vector, it produces a plot of imaginary versus real parts of the vector
elements.
 f is a factor object, y is a numeric vector.  The first form
generates a bar plot of f; the second form produces boxplots of
y for each level of f.
 df is a data frame, y is any object, expr is a list
of object names separated by ‘+’ (e.g., a + b + c).  The
first two forms produce distributional plots of the variables in a data
frame (first form) or of a number of named objects (second form).  The
third form plots y against every object named in expr.
 
Next: Display graphics, Previous: The plot() function, Up: High-level plotting commands   [Contents][Index] R provides two very useful functions for representing multivariate
data.  If X is a numeric matrix or data frame, the command
 produces a pairwise scatterplot matrix of the variables defined by the
columns of X, that is, every column of X is plotted
against every other column of X and the resulting n(n-1)
plots are arranged in a matrix with plot scales constant over the rows
and columns of the matrix.
 When three or four variables are involved a coplot may be more
enlightening.  If a and b are numeric vectors and c
is a numeric vector or factor object (all of the same length), then
the command
 produces a number of scatterplots of a against b for given
values of c.  If c is a factor, this simply means that
a is plotted against b for every level of c.  When
c is numeric, it is divided into a number of conditioning
intervals and for each interval a is plotted against b
for values of c within the interval.  The number and position of
intervals can be controlled with given.values= argument to
coplot()—the function co.intervals() is useful for
selecting intervals.  You can also use two given variables with a
command like
 which produces scatterplots of a against b for every joint
conditioning interval of c and d.
 The coplot() and pairs() function both take an argument
panel= which can be used to customize the type of plot which
appears in each panel.  The default is points() to produce a
scatterplot but by supplying some other low-level graphics function of
two vectors x and y as the value of panel= you can
produce any type of plot you wish.  An example panel function useful for
coplots is panel.smooth().
 
Next: Arguments to high-level plotting functions, Previous: Displaying multivariate data, Up: High-level plotting commands   [Contents][Index] Other high-level graphics functions produce different types of plots.
Some examples are:
 Distribution-comparison plots.  The first form plots the numeric vector
x against the expected Normal order scores (a normal scores plot)
and the second adds a straight line to such a plot by drawing a line
through the distribution and data quartiles.  The third form plots the
quantiles of x against those of y to compare their
respective distributions.
 Produces a histogram of the numeric vector x.  A sensible number
of classes is usually chosen, but a recommendation can be given with the
nclass= argument.  Alternatively, the breakpoints can be
specified exactly with the breaks= argument.  If the
probability=TRUE argument is given, the bars represent relative
frequencies divided by bin width instead of counts.
 Constructs a dotchart of the data in x.  In a dotchart the
y-axis gives a labelling of the data in x and the
x-axis gives its value.  For example it allows easy visual
selection of all data entries with values lying in specified ranges.
 Plots of three variables.  The image plot draws a grid of rectangles
using different colours to represent the value of z, the contour
plot draws contour lines to represent the value of z, and the
persp plot draws a 3D surface.
 
Previous: Display graphics, Up: High-level plotting commands   [Contents][Index] There are a number of arguments which may be passed to high-level
graphics functions, as follows:
 Forces the function to act as a low-level graphics function,
superimposing the plot on the current plot (some functions only).
 Suppresses generation of axes—useful for adding your own custom axes
with the axis() function.  The default, axes=TRUE, means
include axes.
 Causes the x, y or both axes to be logarithmic.  This will
work for many, but not all, types of plot.
 The type= argument controls the type of plot produced, as
follows:
 Plot individual points (the default)
 Plot lines
 Plot points connected by lines (both)
 Plot points overlaid by lines
 Plot vertical lines from points to the zero axis (high-density)
 Step-function plots.  In the first form, the top of the vertical defines
the point; in the second, the bottom.
 No plotting at all.  However axes are still drawn (by default) and the
coordinate system is set up according to the data.  Ideal for creating
plots with subsequent low-level graphics functions.
 Axis labels for the x and y axes.  Use these arguments to
change the default labels, usually the names of the objects used in the
call to the high-level plotting function.
 Figure title, placed at the top of the plot in a large font.
 Sub-title, placed just below the x-axis in a smaller font.
 
Next: Interacting with graphics, Previous: High-level plotting commands, Up: Graphics   [Contents][Index] Sometimes the high-level plotting functions don’t produce exactly the
kind of plot you desire.  In this case, low-level plotting commands can
be used to add extra information (such as points, lines or text) to the
current plot.
 Some of the more useful low-level plotting functions are:
 Adds points or connected lines to the current plot.  plot()’s
type= argument can also be passed to these functions (and
defaults to "p" for points() and "l" for
lines().)
 Add text to a plot at points given by x, y.  Normally
labels is an integer or character vector in which case
labels[i] is plotted at point (x[i], y[i]).  The default
is 1:length(x).
 Note: This function is often used in the sequence
 The graphics parameter type="n" suppresses the points but sets up
the axes, and the text() function supplies special characters, as
specified by the character vector names for the points.
 Adds a line of slope b and intercept a to the current
plot.  h=y may be used to specify y-coordinates for
the heights of horizontal lines to go across a plot, and
v=x similarly for the x-coordinates for vertical
lines.  Also lm.obj may be list with a coefficients
component of length 2 (such as the result of model-fitting functions,)
which are taken as an intercept and slope, in that order.
 Draws a polygon defined by the ordered vertices in (x, y)
and (optionally) shade it in with hatch lines, or fill it if the
graphics device allows the filling of figures.
 Adds a legend to the current plot at the specified position.  Plotting
characters, line styles, colors etc., are identified with the labels in
the character vector legend.  At least one other argument v
(a vector the same length as legend) with the corresponding
values of the plotting unit must also be given, as follows:
 Colors for filled boxes
 Colors in which points or lines will be drawn
 Line styles
 Line widths
 Plotting characters (character vector)
 Adds a title main to the top of the current plot in a large font
and (optionally) a sub-title sub at the bottom in a smaller font.
 Adds an axis to the current plot on the side given by the first argument
(1 to 4, counting clockwise from the bottom.)  Other arguments control
the positioning of the axis within or beside the plot, and tick
positions and labels.  Useful for adding custom axes after calling
plot() with the axes=FALSE argument.
 Low-level plotting functions usually require some positioning
information (e.g., x and y coordinates) to determine where
to place the new plot elements.  Coordinates are given in terms of
user coordinates which are defined by the previous high-level
graphics command and are chosen based on the supplied data.
 Where x and y arguments are required, it is also
sufficient to supply a single argument being a list with elements named
x and y.  Similarly a matrix with two columns is also
valid input.  In this way functions such as locator() (see below)
may be used to specify positions on a plot interactively.
 
Next: Hershey vector fonts, Previous: Low-level plotting commands, Up: Low-level plotting commands   [Contents][Index] In some cases, it is useful to add mathematical symbols and formulae to a
plot.  This can be achieved in R by specifying an expression rather
than a character string in any one of text, mtext, axis,
or title.   For example, the following code draws the formula for
the Binomial probability function:
 More information, including a full listing of the features available can
obtained from within R using the commands:
 
Previous: Mathematical annotation, Up: Low-level plotting commands   [Contents][Index] It is possible to specify Hershey vector fonts for rendering text when using
the text and contour functions.  There are three reasons for
using the Hershey fonts:
 More information, including tables of Hershey characters can be obtained from
within R using the commands:
 
Next: Using graphics parameters, Previous: Low-level plotting commands, Up: Graphics   [Contents][Index] R also provides functions which allow users to extract or add
information to a plot using a mouse.  The simplest of these is the
locator() function:
 Waits for the user to select locations on the current plot using the
left mouse button.  This continues until n (default 512) points
have been selected, or another mouse button is pressed.  The
type argument allows for plotting at the selected points and has
the same effect as for high-level graphics commands; the default is no
plotting.  locator() returns the locations of the points selected
as a list with two components x and y.
 locator() is usually called with no arguments.  It is
particularly useful for interactively selecting positions for graphic
elements such as legends or labels when it is difficult to calculate in
advance where the graphic should be placed.  For example, to place some
informative text near an outlying point, the command
 may be useful.  (locator() will be ignored if the current device,
such as postscript does not support interactive pointing.)
 Allow the user to highlight any of the points defined by x and
y (using the left mouse button) by plotting the corresponding
component of labels nearby (or the index number of the point if
labels is absent).  Returns the indices of the selected points
when another button is pressed.
 Sometimes we want to identify particular points on a plot, rather
than their positions.  For example, we may wish the user to select some
observation of interest from a graphical display and then manipulate
that observation in some way.  Given a number of (x, y)
coordinates in two numeric vectors x and y, we could use
the identify() function as follows:
 The identify() functions performs no plotting itself, but simply
allows the user to move the mouse pointer and click the left mouse
button near a point.  If there is a point near the mouse pointer it will
be marked with its index number (that is, its position in the
x/y vectors) plotted nearby.  Alternatively, you could use
some informative string (such as a case name) as a highlight by using
the labels argument to identify(), or disable marking
altogether with the plot = FALSE argument.  When the process is
terminated (see above), identify() returns the indices of the
selected points; you can use these indices to extract the selected
points from the original vectors x and y.
 
Next: Graphics parameters, Previous: Interacting with graphics, Up: Graphics   [Contents][Index] When creating graphics, particularly for presentation or publication
purposes, R’s defaults do not always produce exactly that which is
required.  You can, however, customize almost every aspect of the
display using graphics parameters.  R maintains a list of a
large number of graphics parameters which control things such as line
style, colors, figure arrangement and text justification among many
others.  Every graphics parameter has a name (such as ‘col’,
which controls colors,) and a value (a color number, for example.)
 A separate list of graphics parameters is maintained for each active
device, and each device has a default set of parameters when
initialized.  Graphics parameters can be set in two ways: either
permanently, affecting all graphics functions which access the current
device; or temporarily, affecting only a single graphics function call.
 
Next: Arguments to graphics functions, Previous: Using graphics parameters, Up: Using graphics parameters   [Contents][Index] The par() function is used to access and modify the list of
graphics parameters for the current graphics device.
 Without arguments, returns a list of all graphics parameters and their
values for the current device.
 With a character vector argument, returns only the named graphics
parameters (again, as a list.)
 With named arguments (or a single list argument), sets the values of
the named graphics parameters, and returns the original values of the
parameters as a list.
 Setting graphics parameters with the par() function changes the
value of the parameters permanently, in the sense that all future
calls to graphics functions (on the current device) will be affected by
the new value.  You can think of setting graphics parameters in this way
as setting “default” values for the parameters, which will be used by
all graphics functions unless an alternative value is given.
 Note that calls to par() always affect the global values
of graphics parameters, even when par() is called from within a
function.  This is often undesirable behavior—usually we want to set
some graphics parameters, do some plotting, and then restore the
original values so as not to affect the user’s R session.  You can
restore the initial values by saving the result of par() when
making changes, and restoring the initial values when plotting is
complete.
 To save and restore all settable25 graphical parameters use
 
Previous: The par() function, Up: Using graphics parameters   [Contents][Index] Graphics parameters may also be passed to (almost) any graphics function
as named arguments.  This has the same effect as passing the arguments
to the par() function, except that the changes only last for the
duration of the function call.  For example:
 produces a scatterplot using a plus sign as the plotting character,
without changing the default plotting character for future plots.
 Unfortunately, this is not implemented entirely consistently and it is
sometimes necessary to set and reset graphics parameters using
par().
 
Next: Device drivers, Previous: Using graphics parameters, Up: Graphics   [Contents][Index] The following sections detail many of the commonly-used graphical
parameters.  The R help documentation for the par() function
provides a more concise summary; this is provided as a somewhat more
detailed alternative.
 Graphics parameters will be presented in the following form:
 A description of the parameter’s effect.  name is the name of the
parameter, that is, the argument name to use in calls to par() or
a graphics function.  value is a typical value you might use when
setting the parameter.
 Note that axes is not a graphics parameter but an
argument to a few plot methods: see xaxt and yaxt.
 
Next: Axes and tick marks, Previous: Graphics parameters, Up: Graphics parameters   [Contents][Index] R plots are made up of points, lines, text and polygons (filled
regions.) Graphical parameters exist which control how these
graphical elements are drawn, as follows:
 Character to be used for plotting points.  The default varies with
graphics drivers, but it is usually
a circle.
Plotted points tend to appear slightly above or below the appropriate
position unless you use "." as the plotting character, which
produces centered points.
 When pch is given as an integer between 0 and 25 inclusive, a
specialized plotting symbol is produced.  To see what the symbols are,
use the command
 Those from 21 to 25 may appear to duplicate earlier symbols, but can be
coloured in different ways: see the help on points and its
examples.
 In addition, pch can be a character or a number in the range
32:255 representing a character in the current font.
 Line types.  Alternative line styles are not supported on all graphics
devices (and vary on those that do) but line type 1 is always a solid
line, line type 0 is always invisible, and line types 2 and onwards are
dotted or dashed lines, or some combination of both.
 Line widths.  Desired width of lines, in multiples of the “standard”
line width.  Affects axis lines as well as lines drawn with
lines(), etc.  Not all devices support this, and some have
restrictions on the widths that can be used.
 Colors to be used for points, lines, text, filled regions and images.
A number from the current palette (see ?palette) or a named colour.
 The color to be used for axis annotation, x and y labels,
main and sub-titles, respectively.
 An integer which specifies which font to use for text.  If possible,
device drivers arrange so that 1 corresponds to plain text,
2 to bold face, 3 to italic, 4 to bold italic
and 5 to a symbol font (which include Greek letters).
 The font to be used for axis annotation, x and y labels,
main and sub-titles, respectively.
 Justification of text relative to the plotting position.  0 means
left justify, 1 means right justify and 0.5 means to
center horizontally about the plotting position.  The actual value is
the proportion of text that appears to the left of the plotting
position, so a value of -0.1 leaves a gap of 10% of the text width
between the text and the plotting position.
 Character expansion.  The value is the desired size of text characters
(including plotting characters) relative to the default text size.
 The character expansion to be used for axis annotation, x and
y labels, main and sub-titles, respectively.
 
Next: Figure margins, Previous: Graphical elements, Up: Graphics parameters   [Contents][Index] Many of R’s high-level plots have axes, and you can construct axes
yourself with the low-level axis() graphics function.  Axes have
three main components: the axis line (line style controlled by the
lty graphics parameter), the tick marks (which mark off unit
divisions along the axis line) and the tick labels (which mark the
units.) These components can be customized with the following graphics
parameters.
 The first two numbers are the desired number of tick intervals on the
x and y axes respectively.  The third number is the
desired length of axis labels, in characters (including the decimal
point.)  Choosing a too-small value for this parameter may result in all
tick labels being rounded to the same number!
 Orientation of axis labels.  0 means always parallel to axis,
1 means always horizontal, and 2 means always
perpendicular to the axis.
 Positions of axis components.  The first component is the distance from
the axis label to the axis position, in text lines.  The second
component is the distance to the tick labels, and the final component is
the distance from the axis position to the axis line (usually zero).
Positive numbers measure outside the plot region, negative numbers
inside.
 Length of tick marks, as a fraction of the size of the plotting region.
When tck is small (less than 0.5) the tick marks on the x
and y axes are forced to be the same size.  A value of 1 gives
grid lines.  Negative values give tick marks outside the plotting
region.  Use tck=0.01 and mgp=c(1,-1.5,0) for internal
tick marks.
 Axis styles for the x and y axes, respectively.   With
styles "i" (internal) and "r" (the default) tick marks
always fall within the range of the data, however style "r"
leaves a small amount of space at the edges.  (S has other styles
not implemented in R.)
 
Next: Multiple figure environment, Previous: Axes and tick marks, Up: Graphics parameters   [Contents][Index] A single plot in R is known as a figure and comprises a
plot region surrounded by margins (possibly containing axis
labels, titles, etc.) and (usually) bounded by the axes themselves.
 A typical figure is
 Graphics parameters controlling figure layout include:
 Widths of the bottom, left, top and right margins, respectively,
measured in inches.
 Similar to mai, except the measurement unit is text lines.
 mar and mai are equivalent in the sense that setting one
changes the value of the other.  The default values chosen for this
parameter are often too large; the right-hand margin is rarely needed,
and neither is the top margin if no title is being used.  The bottom and
left margins must be large enough to accommodate the axis and tick
labels.  Furthermore, the default is chosen without regard to the size
of the device surface: for example, using the postscript() driver
with the height=4 argument will result in a plot which is about
50% margin unless mar or mai are set explicitly.  When
multiple figures are in use (see below) the margins are reduced, however
this may not be enough when many figures share the same page.
 
Previous: Figure margins, Up: Graphics parameters   [Contents][Index] R allows you to create an n by m array of figures on a
single page.  Each figure has its own margins, and the array of figures
is optionally surrounded by an outer margin, as shown in the
following figure.
 The graphical parameters relating to multiple figures are as follows:
 Set the size of a multiple figure array.  The first value is the number of
rows; the second is the number of columns.  The only difference between
these two parameters is that setting mfcol causes figures to be
filled by column; mfrow fills by rows.
 The layout in the Figure could have been created by setting
mfrow=c(3,2); the figure shows the page after four plots have
been drawn.
 Setting either of these can reduce the base size of symbols and text
(controlled by par("cex") and the pointsize of the device).  In a
layout with exactly two rows and columns the base size is reduced by a
factor of 0.83: if there are three or more of either rows or columns,
the reduction factor is 0.66.
 Position of the current figure in a multiple figure environment.  The first
two numbers are the row and column of the current figure; the last two
are the number of rows and columns in the multiple figure array.  Set
this parameter to jump between figures in the array.  You can even use
different values for the last two numbers than the true values
for unequally-sized figures on the same page.
 Position of the current figure on the page.  Values are the positions of
the left, right, bottom and top edges respectively, as a percentage of
the page measured from the bottom left corner.  The example value would
be for a figure in the bottom right of the page.  Set this parameter for
arbitrary positioning of figures within a page.  If you want to add a
figure to a current page, use new=TRUE as well (unlike S).
 Size of outer margins.  Like mar and mai, the first
measures in text lines and the second in inches, starting with the
bottom margin and working clockwise.
 Outer margins are particularly useful for page-wise titles, etc.  Text
can be added to the outer margins with the mtext() function with
argument outer=TRUE.  There are no outer margins by default,
however, so you must create them explicitly using oma or
omi.
 More complicated arrangements of multiple figures can be produced by the
split.screen() and layout() functions, as well as by the
grid and lattice packages.
 
Next: Dynamic graphics, Previous: Graphics parameters, Up: Graphics   [Contents][Index] R can generate graphics (of varying levels of quality) on almost any
type of display or printing device.  Before this can begin, however,
R needs to be informed what type of device it is dealing with.  This
is done by starting a device driver.  The purpose of a device
driver is to convert graphical instructions from R (“draw a line,”
for example) into a form that the particular device can understand.
 Device drivers are started by calling a device driver function.  There
is one such function for every device driver: type help(Devices)
for a list of them all.  For example, issuing the command
 causes all future graphics output to be sent to the printer in
PostScript format.  Some commonly-used device drivers are:
 For use with the X11 window system on Unix-alikes
 For use on Windows
 For use on OS X
 For printing on PostScript printers, or creating PostScript graphics
files.
 Produces a PDF file, which can also be included into PDF files.
 Produces a bitmap PNG file. (Not always available: see its help page.)
 Produces a bitmap JPEG file, best used for image plots.
(Not always available: see its help page.)
 When you have finished with a device, be sure to terminate the device
driver by issuing the command
 This ensures that the device finishes cleanly; for example in the case
of hardcopy devices this ensures that every page is completed and has
been sent to the printer.  (This will happen automatically at the normal
end of a session.)
 
Next: Multiple graphics devices, Previous: Device drivers, Up: Device drivers   [Contents][Index] By passing the file argument to the postscript() device
driver function, you may store the graphics in PostScript format in a
file of your choice.  The plot will be in landscape orientation unless
the horizontal=FALSE argument is given, and you can control the
size of the graphic with the width and height arguments
(the plot will be scaled as appropriate to fit these dimensions.) For
example, the command
 will produce a file containing PostScript code for a figure five inches
high, perhaps for inclusion in a document.  It is important to note that
if the file named in the command already exists, it will be overwritten.
This is the case even if the file was only created earlier in the same
R session.
 Many usages of PostScript output will be to incorporate the figure in
another document.  This works best when encapsulated PostScript
is produced: R always produces conformant output, but only marks the
output as such when the onefile=FALSE argument is supplied.  This
unusual notation stems from S-compatibility: it really means that
the output will be a single page (which is part of the EPSF
specification).  Thus to produce a plot for inclusion use something like
 
Previous: PostScript diagrams for typeset documents, Up: Device drivers   [Contents][Index] In advanced use of R it is often useful to have several graphics
devices in use at the same time.  Of course only one graphics device can
accept graphics commands at any one time, and this is known as the
current device.  When multiple devices are open, they form a
numbered sequence with names giving the kind of device at any position.
 The main commands used for operating with multiple devices, and their
meanings are as follows:
 [UNIX]
 [Windows]
 [OS X]
 Each new call to a device driver function opens a new graphics device,
thus extending by one the device list.  This device becomes the current
device, to which graphics output will be sent.
 Returns the number and name of all active devices.  The device at
position 1 on the list is always the null device which does not
accept graphics commands at all.
 Returns the number and name of the graphics device next to, or previous
to the current device, respectively.
 Can be used to change the current graphics device to the one at position
k of the device list.  Returns the number and label of the device.
 Terminate the graphics device at point k of the device list.  For
some devices, such as postscript devices, this will either print
the file immediately or correctly complete the file for later printing,
depending on how the device was initiated.
 Make a copy of the device k.  Here device is a device
function, such as postscript, with extra arguments, if needed,
specified by ‘…’.  dev.print is similar, but the
copied device is immediately closed, so that end actions, such as
printing hardcopies, are immediately performed.
 Terminate all graphics devices on the list, except the null device.
 
Previous: Device drivers, Up: Graphics   [Contents][Index] R does not have builtin capabilities for dynamic or
interactive graphics, e.g. rotating point clouds or to “brushing”
(interactively highlighting) points. However, extensive dynamic graphics
facilities are available in the system GGobi by Swayne, Cook and Buja
available from
 http://www.ggobi.org/
 and these can be accessed from R via the package rggobi, described at
http://www.ggobi.org/rggobi.
 Also, package rgl provides ways to interact with 3D plots, for example
of surfaces.
 
Next: OS facilities, Previous: Graphics, Up: Top   [Contents][Index] All R functions and datasets are stored in packages.  Only
when a package is loaded are its contents available.  This is done both
for efficiency (the full list would take more memory and would take
longer to search than a subset), and to aid package developers, who are
protected from name clashes with other code.  The process of developing
packages is described in Creating R
packages in Writing R Extensions.  Here, we will describe them
from a user’s point of view.
 To see which packages are installed at your site, issue the command
 with no arguments.  To load a particular package (e.g., the boot
package containing functions from Davison & Hinkley (1997)), use a
command like
 Users connected to the Internet can use the install.packages()
and update.packages() functions (available through the
Packages menu in the Windows and OS X GUIs, see Installing
packages in R Installation and Administration) to install
and update packages.
 To see which packages are currently loaded, use
 to display the search list.  Some packages may be loaded but not
available on the search list (see Namespaces): these will be
included in the list given by
 To see a list of all available help topics in an installed package,
use
 to start the HTML help system, and then navigate to the package
listing in the Reference section.
 
Next: Contributed packages and CRAN, Previous: Packages, Up: Packages   [Contents][Index] The standard (or base) packages are considered part of the R
source code.  They contain the basic functions that allow R to work,
and the datasets and standard statistical and graphical functions that
are described in this manual.  They should be automatically available in
any R installation.  See R
packages in R FAQ, for a complete list.
 
Next: Namespaces, Previous: Standard packages, Up: Packages   [Contents][Index] There are thousands of contributed packages for R, written by many
different authors.  Some of these packages implement specialized
statistical methods, others give access to data or hardware, and others
are designed to complement textbooks.  Some (the recommended
packages) are distributed with every binary distribution of R.  Most
are available for download from CRAN
(https://CRAN.R-project.org/ and its mirrors) and other
repositories such as Bioconductor (https://www.bioconductor.org/).
and Omegahat (http://www.omegahat.net/).  The R FAQ
contains a list of CRAN packages current at the time of release, but the
collection of available packages changes very frequently.
 
Previous: Contributed packages and CRAN, Up: Packages   [Contents][Index] All packages have namespaces, and have since R 2.14.0.
Namespaces do three things: they allow the package writer to hide
functions and data that are meant only for internal use, they prevent
functions from breaking when a user (or other package writer) picks a
name that clashes with one in the package, and they provide a way to
refer to an object within a particular package.
 For example, t() is the transpose function in R, but users
might define their own function named t.  Namespaces prevent
the user’s definition from taking precedence, and breaking every
function that tries to transpose a matrix.
 There are two operators that work with namespaces.  The double-colon
operator :: selects definitions from a particular namespace.
In the example above, the transpose function will always be available
as base::t, because it is defined in the base package.
Only functions that are exported from the package can be retrieved in
this way.
 The triple-colon operator ::: may be seen in a few places in R
code: it acts like the double-colon operator but also allows access to
hidden objects.  Users are more likely to use the getAnywhere()
function, which searches multiple packages.
 Packages are often inter-dependent, and loading one may cause others to
be automatically loaded.  The colon operators described above will also
cause automatic loading of the associated package.  When packages with
namespaces are loaded automatically they are not added to the search
list.
 
Next: A sample session, Previous: Packages, Up: Top   [Contents][Index] R has quite extensive facilities to access the OS under which it is
running: this allows it to be used as a scripting language and that
ability is much used by R itself, for example to install packages.
 Because R’s own scripts need to work across all platforms,
considerable effort has gone into make the scripting facilities as
platform-independent as is feasible.
 
Next: Filepaths, Previous: OS facilities, Up: OS facilities   [Contents][Index] There are many functions to manipulate files and directories. Here are
pointers to some of the more commonly used ones.
 To create an (empty) file or directory, use file.create or
create.dir.  (These are the analogues of the POSIX utilities
touch and mkdir.)  For temporary files and
directories in the R session directory see tempfile.
 Files can be removed by either file.remove or unlink: the
latter can remove directory trees.
 For directory listings use list.files (also available as
dir) or list.dirs. These can select files using a regular
expression: to select by wildcards use Sys.glob.
 Many types of information on a filepath (including for example if it is
a file or directory) can be found by file.info.
 There are several ways to find out if a file ‘exists’ (a file can
exist on the filesystem and not be visible to the current user).
There are functions file.exists, file.access and
file_test with various versions of this test: file_test is
a version of the POSIX test command for those familiar with
shell scripting.
 Function file.copy is the R analogue of the POSIX command
cp.
 Choosing files can be done interactively by file.choose: the
Windows port has the more versatile functions choose.files and
choose.dir and there are similar functions in the tcltk
package: tk_choose.files and tk_choose.dir.
 Functions file.show and file.edit will display and edit
one or more files in a way appropriate to the R port, using the
facilities of a console (such as RGui on Windows or R.app on OS X) if
one is in use.
 There is some support for links in the filesystem: see functions
file.link and Sys.readlink.
 
Next: System commands, Previous: Files and directories, Up: OS facilities   [Contents][Index] With a few exceptions, R relies on the underlying OS functions to
manipulate filepaths.  Some aspects of this are allowed to depend on the
OS, and do, even down to the version of the OS.  There are POSIX
standards for how OSes should interpret filepaths and many R users
assume POSIX compliance: but Windows does not claim to be compliant and
other OSes may be less than completely compliant.
 The following are some issues which have been encountered with filepaths.
 Functions basename and dirname select parts of a file
path: the recommended way to assemble a file path from components is
file.path.  Function pathexpand does ‘tilde expansion’,
substituting values for home directories (the current user’s, and
perhaps those of other users).
 On filesystems with links, a single file can be referred to by many
filepaths.  Function normalizePath will find a canonical
filepath.
 Windows has the concepts of short (‘8.3’) and long file names:
normalizePath will return an absolute path using long file names
and shortPathName will return a version using short names.  The
latter does not contain spaces and uses backslash as the separator, so
is sometimes useful for exporting names from R.
 File permissions are a related topic.  R has support for the
POSIX concepts of read/write/execute permission for owner/group/all but
this may be only partially supported on the filesystem (so for example
on Windows only read-only files (for the account running the R
session) are recognized.  Access Control Lists (ACLs) are employed on
several filesystems, but do not have an agreed standard and R has no
facilities to control them.  Use Sys.chmod to change permissions.
 
Next: Compression and Archives, Previous: Filepaths, Up: OS facilities   [Contents][Index] Functions system and system2 are used to invoke a system
command and optionally collect its output.  system2 is a little
more general but its main advantage is that it is easier to write
cross-platform code using it.
 system behaves differently on Windows from other OSes (because
the API C call of that name does).  Elsewhere it invokes a shell to run
the command: the Windows port of R has a function shell to do
that.
 To find out if the OS includes a command, use Sys.which, which
attempts to do this in a cross-platform way (unfortunately it is not a
standard OS service).
 Function shQuote will quote filepaths as needed for commands in
the current OS.
 
Previous: System commands, Up: OS facilities   [Contents][Index] Recent versions of R have extensive facilities to read and write
compressed files, often transparently.  Reading of files in R is to a
vey large extent done by connections, and the file
function which is used to open a connection to a file (or a URL) and is
able to identify the compression used from the ‘magic’ header of the
file.
 The type of compression which has been supported for longest is
gzip compression, and that remains a good general compromise.
Files compressed by the earlier Unix compress utility can also
be read, but these are becoming rare.  Two other forms of compression,
those of the bzip2 and xz utilities are also
available.  These generally achieve higher rates of compression
(depending on the file, much higher) at the expense of slower
decompression and much slower compression.
 There is some confusion between xz and lzma
compression (see https://en.wikipedia.org/wiki/Xz and
https://en.wikipedia.org/wiki/LZMA): R can read files
compressed by most versions of either.
 File archives are single files which contain a collection of files, the
most common ones being ‘tarballs’ and zip files as used to distribute
R packages.  R can list and unpack both (see functions untar
and unzip) and create both (for zip with the help of an
external program).
 
Next: Invoking R, Previous: OS facilities, Up: Top   [Contents][Index] The following session is intended to introduce to you some features of
the R environment by using them.  Many features of the system will be
unfamiliar and puzzling at first, but this puzzlement will soon
disappear.
 The R program begins, with a banner.
 (Within R code, the prompt on the left hand side will not be shown to
avoid confusion.)
 Start the HTML interface to on-line help (using a web browser
available at your machine).  You should briefly explore the features of
this facility with the mouse.
 Iconify the help window and move on to the next part.
 Generate two pseudo-random normal vectors of x- and
y-coordinates.
 Plot the points in the plane.  A graphics window will appear automatically.
 See which R objects are now in the R workspace.
 Remove objects no longer needed. (Clean up).
 Make x = (1, 2, …, 20).
 A ‘weight’ vector of standard deviations.
 Make a data frame of two columns, x and y, and look
at it.
 Fit a simple linear regression and look at the
analysis.  With y to the left of the tilde,
we are modelling y dependent on x.
 Since we know the standard deviations, we can do a weighted regression.
 Make the columns in the data frame visible as variables.
 Make a nonparametric local regression function.
 Standard point plot.
 Add in the local regression.
 The true regression line: (intercept 0, slope 1).
 Unweighted regression line.
 Weighted regression line.
 Remove data frame from the search path.
 A standard regression diagnostic plot to check for heteroscedasticity.
Can you see it?
 A normal scores plot to check for skewness, kurtosis and outliers.  (Not
very useful here.)
 Clean up again.
 The next section will look at data from the classical experiment of
Michelson to measure the speed of light.  This dataset is available in
the morley object, but we will read it to illustrate the
read.table function.
 Get the path to the data file.
 Optional.  Look at the file.
 Read in the Michelson data as a data frame, and look at it.
There are five experiments (column Expt) and each has 20 runs
(column Run) and sl is the recorded speed of light,
suitably coded.
 Change Expt and Run into factors.
 Make the data frame visible at position 3 (the default).
 Compare the five experiments with simple boxplots.
 Analyze as a randomized block, with ‘runs’ and ‘experiments’ as factors.
 Fit the sub-model omitting ‘runs’, and compare using a formal analysis
of variance.
 Clean up before moving on.
 We now look at some more graphical features: contour and image plots.
 x is a vector of 50 equally spaced values in
the interval [-pi\, pi].
y is the same.
 f is a square matrix, with rows and columns indexed by x
and y respectively, of values of the function
cos(y)/(1 + x^2).
 Save the plotting parameters and set the plotting region to “square”.
 Make a contour map of f; add in more lines for more detail.
 fa is the “asymmetric part” of f.  (t() is
transpose).
 Make a contour plot, …
 … and restore the old graphics parameters.
 Make some high density image plots, (of which you can get
hardcopies if you wish), …
 … and clean up before moving on.
 R can do complex arithmetic, also.
 1i is used for the complex number i.
 Plotting complex arguments means plot imaginary versus real parts.  This
should be a circle.
 Suppose we want to sample points within the unit circle.  One method
would be to take complex numbers with standard normal real and imaginary
parts …
 … and to map any outside the circle onto their reciprocal.
 All points are inside the unit circle, but the distribution is not
uniform.
 The second method uses the uniform distribution.  The points should now
look more evenly spaced over the disc.
 Clean up again.
 Quit the R program.  You will be asked if you want to save the R
workspace, and for an exploratory session like this, you probably do not
want to save it.
 
Next: The command-line editor, Previous: A sample session, Up: Top   [Contents][Index] Users of R on Windows or OS X should read the OS-specific section
first, but command-line use is also supported.
 
Next: Invoking R under Windows, Previous: Invoking R, Up: Invoking R   [Contents][Index] When working at a command line on UNIX or Windows, the command ‘R’
can be used both for starting the main R program in the form
 or, via the R CMD interface, as a wrapper to various R tools
(e.g., for processing files in R documentation format or manipulating
add-on packages) which are not intended to be called “directly”.
 At the Windows command-line, Rterm.exe is preferred to
R.
 You need to ensure that either the environment variable TMPDIR is
unset or it points to a valid place to create temporary files and
directories.
 Most options control what happens at the beginning and at the end of an
R session.  The startup mechanism is as follows (see also the on-line
help for topic ‘Startup’ for more information, and the section below
for some Windows-specific details).
 In addition, there are options for controlling the memory available to
the R process (see the on-line help for topic ‘Memory’ for more
information).  Users will not normally need to use these unless they
are trying to limit the amount of memory used by R.
 R accepts the following command-line options.
 Print short help message to standard output and exit successfully.
 Print version information to standard output and exit successfully.
 Specify the encoding to be assumed for input from the console or
stdin.  This needs to be an encoding known to iconv: see
its help page.  (--encoding enc is also accepted.)  The
input is re-encoded to the locale R is running in and needs to be
representable in the latter’s encoding (so e.g. you cannot re-encode
Greek text in a French locale unless that locale uses the UTF-8
encoding).
 Print the path to the R “home directory” to standard output and
exit successfully.  Apart from the front-end shell script and the man
page, R installation puts everything (executables, packages, etc.)
into this directory.
 Control whether data sets should be saved or not at the end of the R
session.  If neither is given in an interactive session, the user is
asked for the desired behavior when ending the session with q();
in non-interactive use one of these must be specified or implied by some
other option (see below).
 Do not read any user file to set environment variables.
 Do not read the site-wide profile at startup.
 Do not read the user’s profile at startup.
 Control whether saved images (file .RData in the directory where
R was started) should be restored at startup or not.  The default is
to restore. (--no-restore implies all the specific
--no-restore-* options.)
 Control whether the history file (normally file .Rhistory in the
directory where R was started, but can be set by the environment
variable R_HISTFILE) should be restored at startup or not.  The
default is to restore.
 (Windows only) Prevent loading the Rconsole file at startup.
 Combine --no-save, --no-environ,
--no-site-file, --no-init-file and
--no-restore.  Under Windows, this also includes
--no-Rconsole.
 (not Rgui.exe) Take input from file: ‘-’ means
stdin.  Implies --no-save unless --save has
been set.  On a Unix-alike, shell metacharacters should be avoided in
file (but spaces are allowed).
 (not Rgui.exe) Use expression as an input line.  One or
more -e options can be used, but not together with -f
or --file.  Implies --no-save unless --save
has been set.  (There is a limit of 10,000 bytes on the total length of
expressions used in this way.  Expressions containing spaces or shell
metacharacters will need to be quoted.)
 (UNIX only) Turn off command-line editing via readline.  This
is useful when running R from within Emacs using the ESS
(“Emacs Speaks Statistics”) package.  See The command-line editor,
for more information.  Command-line editing is enabled for default
interactive use (see --interactive).  This option also affects
tilde-expansion: see the help for path.expand.
 For expert use only: set the initial trigger sizes for garbage
collection of vector heap (in bytes) and cons cells (number)
respectively.  Suffix ‘M’ specifies megabytes or millions of cells
respectively.  The defaults are 6Mb and 350k respectively and can also
be set by environment variables R_NSIZE and R_VSIZE.
 Specify the maximum size of the pointer protection stack as N
locations.  This defaults to 10000, but can be increased to allow
large and complicated calculations to be done.  Currently the maximum
value accepted is 100000.
 (Windows only) Specify a limit for the amount of memory to be used both
for R objects and working areas.  This is set by default to the
smaller of the amount of physical RAM in the machine and for 32-bit
R, 1.5Gb26, and must be between 32Mb and the
maximum allowed on that version of Windows.
 Do not print out the initial copyright and welcome messages.
 Make R run as quietly as possible.  This option is intended to
support programs which use R to compute results for them.  It implies
--quiet and --no-save.
 (UNIX only) Assert that R really is being run interactively even if
input has been redirected: use if input is from a FIFO or pipe and fed
from an interactive program.  (The default is to deduce that R is
being run interactively if and only if stdin is connected to a
terminal or pty.)  Using -e, -f or
--file asserts non-interactive use even if
--interactive is given.
 Note that this does not turn on command-line editing.
 (Windows only) Set Rterm up for use by R-inferior-mode in
ESS, including asserting interactive use (without the
command-line editor) and no buffering of stdout.
 Print more information about progress, and in particular set R’s
option verbose to TRUE.  R code uses this option to
control the printing of diagnostic messages.
 (UNIX only) Run R through debugger name.  For most debuggers
(the exceptions are valgrind and recent versions of
gdb), further command line options are disregarded, and should
instead be given when starting the R executable from inside the
debugger.
 (UNIX only) Use type as graphical user interface (note that this
also includes interactive graphics).  Currently, possible values for
type are ‘X11’ (the default) and, provided that ‘Tcl/Tk’
support is available, ‘Tk’. (For back-compatibility, ‘x11’ and
‘tk’ are accepted.)
 (UNIX only) Run the specified sub-architecture.
 This flag does nothing except cause the rest of the command line to be
skipped: this can be useful to retrieve values from it with
commandArgs(TRUE).
 Note that input and output can be redirected in the usual way (using
‘<’ and ‘>’), but the line length limit of 4095 bytes still
applies.  Warning and error messages are sent to the error channel
(stderr).
 The command R CMD allows the invocation of various tools which
are useful in conjunction with R, but not intended to be called
“directly”.  The general form is
 where command is the name of the tool and args the arguments
passed on to it.
 Currently, the following tools are available.
 Run R in batch mode.  Runs R --restore --save with possibly
further options (see ?BATCH).
 (UNIX only) Compile C, C++, Fortran … files for use with R.
 Build shared library for dynamic loading.
 Install add-on packages.
 Remove add-on packages.
 Build (that is, package) add-on packages.
 Check add-on packages.
 (UNIX only) Front-end for creating executable programs.
 Post-process R profiling files.
 Convert Rd format to various other formats, including HTML, LaTeX,
plain text, and extracting the examples.  Rd2txt can be used as
shorthand for Rd2conv -t txt.
 Convert Rd format to PDF.
 Extract S/R code from Sweave or other vignette documentation
 Process Sweave or other vignette documentation
 Diff R output ignoring headers etc
 Obtain configuration information
 (Unix only) Update the Java configuration variables
 (Unix only) Create Emacs-style tag files from C, R, and Rd files
 (Windows only) Open a file via Windows’ file associations
 (Windows only) Process (La)TeX files with R’s style files
 Use
 to obtain usage information for each of the tools accessible via the
R CMD interface.
 In addition, you can use options --arch=,
--no-environ, --no-init-file, --no-site-file
and --vanilla between R and CMD: these
affect any R processes run by the tools.  (Here --vanilla is
equivalent to --no-environ --no-site-file --no-init-file.)
However, note that R CMD does not of itself use any R
startup files (in particular, neither user nor site Renviron
files), and all of the R processes run by these tools (except
BATCH) use --no-restore.  Most use --vanilla
and so invoke no R startup files: the current exceptions are
INSTALL, REMOVE, Sweave and
SHLIB (which uses --no-site-file --no-init-file).
 for any other executable cmd on the path or given by an
absolute filepath: this is useful to have the same environment as R
or the specific commands run under, for example to run ldd or
pdflatex.  Under Windows cmd can be an executable or a
batch file, or if it has extension .sh or .pl the
appropriate interpreter (if available) is called to run it.
 
Next: Invoking R under OS X, Previous: Invoking R from the command line, Up: Invoking R   [Contents][Index] There are two ways to run R under Windows.  Within a terminal window
(e.g. cmd.exe or a more capable shell), the methods described in
the previous section may be used, invoking by R.exe or more
directly by Rterm.exe.  For interactive use, there is a
console-based GUI (Rgui.exe).
 The startup procedure under Windows is very similar to that under
UNIX, but references to the ‘home directory’ need to be clarified, as
this is not always defined on Windows.  If the environment variable
R_USER is defined, that gives the home directory.  Next, if the
environment variable HOME is defined, that gives the home
directory.  After those two user-controllable settings, R tries to
find system defined home directories.  It first tries to use the
Windows "personal" directory (typically C:\Documents and
Settings\username\My Documents in Windows XP).  If that fails, and
environment variables HOMEDRIVE and HOMEPATH are defined
(and they normally are) these define the home directory.  Failing all
those, the home directory is taken to be the starting directory.
 You need to ensure that either the environment variables TMPDIR,
TMP and TEMP are either unset or one of them points to a
valid place to create temporary files and directories.
 Environment variables can be supplied as ‘name=value’
pairs on the command line.
 If there is an argument ending .RData (in any case) it is
interpreted as the path to the workspace to be restored: it implies
--restore and sets the working directory to the parent of the
named file.  (This mechanism is used for drag-and-drop and file
association with RGui.exe, but also works for Rterm.exe.
If the named file does not exist it sets the working directory
if the parent directory exists.)
 The following additional command-line options are available when
invoking RGui.exe.
 Control whether Rgui will operate as an MDI program 
(with multiple child windows within one main window) or an SDI application
(with multiple top-level windows for the console, graphics and pager).  The 
command-line setting overrides the setting in the user’s Rconsole file.
 Enable the “Break to debugger” menu item in Rgui, and trigger
a break to the debugger during command line processing.
 Under Windows with R CMD you may also specify your own
.bat, .exe, .sh or .pl file.  It will be run
under the appropriate interpreter (Perl for .pl) with several
environment variables set appropriately, including R_HOME,
R_OSTYPE, PATH, BSTINPUTS and TEXINPUTS.  For
example, if you already have latex.exe on your path, then
 will run LaTeX on mydoc.tex, with the path to R’s
share/texmf macros appended to TEXINPUTS.  (Unfortunately,
this does not help with the MiKTeX build of LaTeX, but
R CMD texify mydoc will work in that case.)
 
Next: Scripting with R, Previous: Invoking R under Windows, Up: Invoking R   [Contents][Index] There are two ways to run R under OS X.  Within a Terminal.app
window by invoking R, the methods described in the first
subsection apply.  There is also console-based GUI (R.app) that by
default is installed in the Applications folder on your
system.  It is a standard double-clickable OS X application.
 The startup procedure under OS X is very similar to that under UNIX, but
R.app does not make use of command-line arguments.  The ‘home
directory’ is the one inside the R.framework, but the startup and
current working directory are set as the user’s home directory unless a
different startup directory is given in the Preferences window
accessible from within the GUI.
 
Previous: Invoking R under OS X, Up: Invoking R   [Contents][Index] If you just want to run a file foo.R of R commands, the
recommended way is to use R CMD BATCH foo.R.  If you want to
run this in the background or as a batch job use OS-specific facilities
to do so: for example in most shells on Unix-alike OSes R CMD
BATCH foo.R & runs a background job.
 You can pass parameters to scripts via additional arguments on the
command line: for example (where the exact quoting needed will depend on
the shell in use)
 will pass arguments to a script which can be retrieved as a character
vector by
 This is made simpler by the alternative front-end Rscript,
which can be invoked by
 and this can also be used to write executable script files like (at
least on Unix-alikes, and in some Windows shells)
 If this is entered into a text file runfoo and this is made
executable (by chmod 755 runfoo), it can be invoked for
different arguments by
 For further options see help("Rscript").  This writes R
output to stdout and stderr, and this can be redirected in
the usual way for the shell running the command.
 If you do not wish to hardcode the path to Rscript but have it
in your path (which is normally the case for an installed R except on
Windows, but e.g. OS X users may need to add /usr/local/bin
to their path), use
 At least in Bourne and bash shells, the #! mechanism does
not allow extra arguments like 
#! /usr/bin/env Rscript --vanilla.
 One thing to consider is what stdin() refers to.  It is
commonplace to write R scripts with segments like
 and stdin() refers to the script file to allow such traditional
usage.  If you want to refer to the process’s stdin, use
"stdin" as a file connection, e.g. scan("stdin", ...).
 Another way to write executable script files (suggested by François
Pinard) is to use a here document like
 but here stdin() refers to the program source and
"stdin" will not be usable.
 Short scripts can be passed to Rscript on the command-line
via the -e flag.  (Empty scripts are not accepted.)
 Note that on a Unix-alike the input filename (such as foo.R)
should not contain spaces nor shell metacharacters.
 
Next: Function and variable index, Previous: Invoking R, Up: Top   [Contents][Index] When the GNU readline library is available at the
time R is configured for compilation under UNIX, an inbuilt command
line editor allowing recall, editing and re-submission of prior commands
is used.  Note that other versions of readline exist and may be
used by the inbuilt command line editor: this used to happen on OS X.
 It can be disabled (useful for usage with ESS 27) using the startup option
--no-readline.
 Windows versions of R have somewhat simpler command-line editing: see
‘Console’ under the ‘Help’ menu of the GUI, and the
file README.Rterm for command-line editing under
Rterm.exe.
 When using R with GNU28 readline capabilities, the functions described
below are available, as well as others (probably) documented in
man readline or info readline on your system.
 Many of these use either Control or Meta characters.  Control
characters, such as Control-m, are obtained by holding the
CTRL down while you press the m key, and are written as
C-m below.  Meta characters, such as Meta-b, are typed by
holding down META29 and pressing b, and written as M-b
in the following.  If your terminal does not have a META key
enabled, you can still type Meta characters using two-character
sequences starting with ESC.  Thus, to enter M-b, you could
type ESCb.  The ESC character sequences are also
allowed on terminals with real Meta keys.  Note that case is significant
for Meta characters.
 Some but not all versions30 of readline
will recognize resizing of the terminal window so this is best avoided.
 The R program keeps a history of the command lines you type,
including the erroneous lines, and commands in your history may be
recalled, changed if necessary, and re-submitted as new commands.  In
Emacs-style command-line editing any straight typing you do while in
this editing phase causes the characters to be inserted in the command
you are editing, displacing any characters to the right of the cursor.
In vi mode character insertion mode is started by M-i or
M-a, characters are typed and insertion mode is finished by typing
a further ESC.  (The default is Emacs-style, and only that is
described here: for vi mode see the readline
documentation.)
 Pressing the RET command at any time causes the command to be
re-submitted.
 Other editing actions are summarized in the following table.
 Go to the previous command (backwards in the history).
 Go to the next command (forwards in the history).
 Find the last command with the text string in it.  This can be
cancelled by C-g (and on some versions of R by C-c).
 On most terminals, you can also use the up and down arrow keys instead
of C-p and C-n, respectively.
 Go to the beginning of the command.
 Go to the end of the line.
 Go back one word.
 Go forward one word.
 Go back one character.
 Go forward one character.
 On most terminals, you can also use the left and right arrow keys
instead of C-b and C-f, respectively.
 Insert text at the cursor.
 Append text after the cursor.
 Delete the previous character (left of the cursor).
 Delete the character under the cursor.
 Delete the rest of the word under the cursor, and “save” it.
 Delete from cursor to end of command, and “save” it.
 Insert (yank) the last “saved” text here.
 Transpose the character under the cursor with the next.
 Change the rest of the word to lower case.
 Change the rest of the word to upper case.
 Re-submit the command to R.
 The final RET terminates the command line editing sequence.
 The readline key bindings can be customized in the usual way
via a ~/.inputrc file.  These customizations can be
conditioned on application R, that is by including a section like
 
Next: Concept index, Previous: The command-line editor, Up: Top   [Contents][Index] 
Next: References, Previous: Function and variable index, Up: Top   [Contents][Index] 
Previous: Concept index, Up: Top   [Contents][Index] D. M. Bates and  D. G. Watts (1988), Nonlinear Regression
Analysis and Its Applications. John Wiley & Sons, New York.
 Richard A. Becker, John M. Chambers and Allan R. Wilks (1988),
The New S Language. Chapman & Hall, New York.
This book is often called the “Blue Book”.
 John M. Chambers and Trevor J. Hastie eds. (1992),
Statistical Models in S. Chapman & Hall, New York.
This is also called the “White Book”.
 John M. Chambers (1998)
Programming with Data. Springer, New York.
This is also called the “Green Book”.
 A. C. Davison and D. V. Hinkley (1997), Bootstrap Methods
and Their Applications, Cambridge University Press.
 Annette J. Dobson (1990), An Introduction to Generalized Linear
Models, Chapman and Hall, London.
 Peter McCullagh and John A. Nelder (1989), Generalized Linear
Models. Second edition, Chapman and Hall, London.
 John A. Rice (1995), Mathematical Statistics and Data Analysis.
Second edition.  Duxbury Press, Belmont, CA.
 S. D. Silvey (1970), Statistical Inference. Penguin, London.
 ACM Software Systems award, 1998:
https://awards.acm.org/award_winners/chambers_6640862.cfm. For portable R code (including that to
be used in R packages) only A–Za–z0–9 should be used. not inside strings,
nor within the argument list of a function definition some of the
consoles will not allow you to enter more, and amongst those which do
some will silently discard the excess and some will use it as the start
of the next line. of unlimited length. The leading “dot” in
this file name makes it invisible in normal file listings in
UNIX, and in default GUI file listings on OS X and Windows. With other than vector types of argument,
such as list mode arguments, the action of c() is rather
different.  See Concatenating lists. Actually, it is still available as
.Last.value before any other statements are executed. paste(..., collapse=ss) joins the
arguments into a single character string putting ss in between, e.g.,
ss <- "|".  There are more tools for character manipulation, see the help
for sub and substring. numeric mode is
actually an amalgam of two distinct modes, namely integer and
double precision, as explained in the manual. Note however that length(object) does not always
contain intrinsic useful information, e.g., when object is a
function. In general, coercion
from numeric to character and back again will not be exactly reversible,
because of roundoff errors in the character representation. A different style using
‘formal’ or ‘S4’ classes is provided in package methods. Readers should note
that there are eight states and territories in Australia, namely the
Australian Capital Territory, New South Wales, the Northern Territory,
Queensland, South Australia, Tasmania, Victoria and Western Australia. Note that tapply() also works in this case
when its second argument is not a factor, e.g.,
‘tapply(incomes, state)’, and this is true for quite a few
other functions, since arguments are coerced to factors when
necessary (using as.factor()). Note that x %*% x is ambiguous, as
it could mean either x’x or x x’, where x is the
column form.  In such cases the smaller matrix seems implicitly to be
the interpretation adopted, so the scalar x’x is in this case the
result.  The matrix x x’ may be calculated either by cbind(x)
%*% x or x %*% rbind(x) since the result of rbind() or
cbind() is always a matrix.  However, the best way to compute
x’x or x x’ is crossprod(x) or x %o% x respectively. Even better would be to form a matrix square
root B with A = BB’ and find the squared length
of the solution of By = x , perhaps using the Cholesky or
eigen decomposition of A.   Conversion of character columns to factors is
overridden using the stringsAsFactors argument to the
data.frame() function. See the on-line help
for autoload for the meaning of the second term. Under UNIX, the utilities
sed orawk can be used. to be
discussed later, or use xyplot from package lattice. See also the methods described in Statistical models in R In some sense this
mimics the behavior in S-PLUS since in S-PLUS this operator always
creates or assigns to a global variable. So it is hidden under
UNIX. Some graphics
parameters such as the size of the current device are for information
only. 2.5Gb on versions of Windows that support 3Gb per
process and have the support enabled: see the rw-FAQ Q2.9; 3.5Gb
on most 64-bit versions of Windows. The
‘Emacs Speaks Statistics’ package; see the URL
http://ESS.R-project.org It is possible to build R using an
emulation of GNU readline, such as one based on NetBSD’s
editline, it which case only a subset of the capabilities may
be provided. On a PC keyboard this is usually the
Alt key, occasionally the ‘Windows’ key.  On a Mac keyboard normally no
meta key is available. In particular, not versions 6.3 or
later: this is worked around as from R 3.4.0. 
Next: Acknowledgements   [Contents][Index] This is a guide to extending R, describing the process of creating
R add-on packages, writing R documentation, R’s system and
foreign language interfaces, and the R API.
 This manual is for R, version 3.3.1 (2016-06-21).
 Copyright © 1999–2016 R Core Team
 Permission is granted to make and distribute verbatim copies of this
manual provided the copyright notice and this permission notice are
preserved on all copies.
 Permission is granted to copy and distribute modified versions of this
manual under the conditions for verbatim copying, provided that the
entire resulting derived work is distributed under the terms of a
permission notice identical to this one.
 Permission is granted to copy and distribute translations of this manual
into another language, under the above conditions for modified versions,
except that this permission notice may be stated in a translation
approved by the R Core Team.
 
Next: Creating R packages, Previous: Top, Up: Top   [Contents][Index] The contributions to early versions of this manual by Saikat DebRoy
(who wrote the first draft of a guide to using .Call and
.External) and Adrian Trapletti (who provided information on the
C++ interface) are gratefully acknowledged.
 
Next: Writing R documentation files, Previous: Acknowledgements, Up: Top   [Contents][Index] Packages provide a mechanism for loading optional code, data and
documentation as needed.  The R distribution itself includes about 30
packages.
 In the following, we assume that you know the library() command,
including its lib.loc argument, and we also assume basic
knowledge of the R CMD INSTALL utility.  Otherwise, please
look at R’s help pages on
 before reading on.
 For packages which contain code to be compiled, a computing environment
including a number of tools is assumed; the “R Installation and
Administration” manual describes what is needed for each OS.
 Once a source package is created, it must be installed by
the command R CMD INSTALL.
See Add-on-packages in R Installation and Administration.
 Other types of extensions are supported (but rare): See Package types.
 Some notes on terminology complete this introduction.  These will help
with the reading of this manual, and also in describing concepts
accurately when asking for help.
 A package is a directory of files which extend R, a
source package (the master files of a package), or a tarball
containing the files of a source package, or an installed
package, the result of running R CMD INSTALL on a source
package.  On some platforms (notably OS X and Windows) there are also
binary packages, a zip file or tarball containing the files of an
installed package which can be unpacked rather than installing from
sources.
 A package is not1 a
library.  The latter is used in two senses in R documentation.
 There are a number of well-defined operations on source packages.
 The concept of lazy loading of code or data is mentioned at
several points.  This is part of the installation, always selected for
R code but optional for data.  When used the R objects of the
package are created at installation time and stored in a database in the
R directory of the installed package, being loaded into the
session at first use.  This makes the R session start up faster and
use less (virtual) memory.
(For technical details,
see Lazy loading in R Internals.)
 CRAN is a network of WWW sites holding the R distributions
and contributed code, especially R packages.  Users of R are
encouraged to join in the collaborative project and to submit their own
packages to CRAN: current instructions are linked from
https://CRAN.R-project.org/banner.shtml#submitting.
 
Next: Configure and cleanup, Previous: Creating R packages, Up: Creating R packages   [Contents][Index] The sources of an R package consists of a subdirectory containing a
files DESCRIPTION and NAMESPACE, and the subdirectories
R, data, demo, exec, inst,
man, po, src, tests, tools and
vignettes (some of which can be missing, but which should not be
empty).  The package subdirectory may also contain files INDEX,
configure, cleanup, LICENSE, LICENCE and
NEWS.  Other files such as INSTALL (for non-standard
installation instructions), README/README.md2, or ChangeLog will be ignored by R, but may
be useful to end users.  The utility R CMD build may add files
in a build directory (but this should not be used for other
purposes).
 Except where specifically mentioned,3 packages should not contain
Unix-style ‘hidden’ files/directories (that is, those whose name starts
with a dot).
 The DESCRIPTION and INDEX files are described in the
subsections below.  The NAMESPACE file is described in the
section on Package namespaces.
 The optional files configure and cleanup are (Bourne)
shell scripts which are, respectively, executed before and (if option
--clean was given) after installation on Unix-alikes, see
Configure and cleanup.  The analogues on Windows are
configure.win and cleanup.win.
 For the conventions for files NEWS and ChangeLog in the
GNU project see
https://www.gnu.org/prep/standards/standards.html#Documentation.
 The package subdirectory should be given the same name as the package.
Because some file systems (e.g., those on Windows and by default on OS
X) are not case-sensitive, to maintain portability it is strongly
recommended that case distinctions not be used to distinguish different
packages.  For example, if you have a package named foo, do not
also create a package named Foo.
 To ensure that file names are valid across file systems and supported
operating systems, the ASCII control characters as well as the
characters ‘"’, ‘*’, ‘:’, ‘/’, ‘<’, ‘>’,
‘?’, ‘\’, and ‘|’ are not allowed in file names.  In
addition, files with names ‘con’, ‘prn’, ‘aux’,
‘clock$’, ‘nul’, ‘com1’ to ‘com9’, and ‘lpt1’
to ‘lpt9’ after conversion to lower case and stripping possible
“extensions” (e.g., ‘lpt5.foo.bar’), are disallowed.  Also, file
names in the same directory must not differ only by case (see the
previous paragraph).  In addition, the basenames of ‘.Rd’ files may
be used in URLs and so must be ASCII and not contain %.
For maximal portability filenames should only contain only
ASCII characters not excluded already (that is
A-Za-z0-9._!#$%&+,;=@^(){}'[] — we exclude space as many
utilities do not accept spaces in file paths): non-English alphabetic
characters cannot be guaranteed to be supported in all locales.  It
would be good practice to avoid the shell metacharacters
(){}'[]$~: ~ is also used as part of ‘8.3’ filenames on
Windows.  In addition, packages are normally distributed as tarballs,
and these have a limit on path lengths: for maximal portability 100
bytes.
 A source package if possible should not contain binary executable files:
they are not portable, and a security risk if they are of the
appropriate architecture.  R CMD check will warn about
them4 unless they are listed (one filepath per line) in a file
BinaryFiles at the top level of the package.  Note that
CRAN will not accept submissions containing binary files
even if they are listed.
 The R function package.skeleton can help to create the
structure for a new package: see its help page for details.
 
Next: Licensing, Previous: Package structure, Up: Package structure   [Contents][Index] The DESCRIPTION file contains basic information about the package
in the following format:
 The format is that of a version of a ‘Debian Control File’ (see the help
for ‘read.dcf’ and
https://www.debian.org/doc/debian-policy/ch-controlfields.html:
R does not require encoding in UTF-8 and does not support comments
starting with ‘#’).  Fields start with an ASCII name
immediately followed by a colon: the value starts after the colon and a
space.  Continuation lines (for example, for descriptions longer than
one line) start with a space or tab.  Field names are case-sensitive:
all those used by R are capitalized.
 For maximal portability, the DESCRIPTION file should be written
entirely in ASCII — if this is not possible it must contain
an ‘Encoding’ field (see below).
 Several optional fields take logical values: these can be
specified as ‘yes’, ‘true’, ‘no’ or ‘false’:
capitalized values are also accepted.
 The ‘Package’, ‘Version’, ‘License’, ‘Description’,
‘Title’, ‘Author’, and ‘Maintainer’ fields are mandatory,
all other fields are optional.  Fields ‘Author’ and
‘Maintainer’ can be auto-generated from ‘Authors@R’, and may
be omitted if the latter is provided: however if they are not
ASCII we recommend that they are provided.
 The mandatory ‘Package’ field gives the name of the package.  This
should contain only (ASCII) letters, numbers and dot, have at
least two characters and start with a letter and not end in a dot.  If
it needs explaining, this should be done in the ‘Description’ field
(and not the ‘Title’ field).
 The mandatory ‘Version’ field gives the version of the package.
This is a sequence of at least two (and usually three)
non-negative integers separated by single ‘.’ or ‘-’
characters.  The canonical form is as shown in the example, and a
version such as ‘0.01’ or ‘0.01.0’ will be handled as if it
were ‘0.1-0’.  It is not a decimal number, so for example
0.9 < 0.75 since 9 < 75.
 The mandatory ‘License’ field is discussed in the next subsection.
 The mandatory ‘Title’ field should give a short description
of the package.  Some package listings may truncate the title to 65
characters.  It should use title case (that is, use capitals for
the principal words: tools::toTitleCase can help you with this),
not use any markup, not have any continuation lines, and not end in a
period (unless part of …).  Do not repeat the package name: it is
often used prefixed by the name.  Refer to other packages and external
software in single quotes, and to book titles (and similar) in double
quotes.
 The mandatory ‘Description’ field should give a
comprehensive description of what the package does.  One can use
several (complete) sentences, but only one paragraph. It should be
intelligible to all the intended readership (e.g. for a CRAN
package to all CRAN users).  It is good practice not to start
with the package name, ‘This package’ or similar.  As with the
‘Title’ field, double quotes should be used for quotations
(including titles of books and articles), and single quotes for
non-English usage, including names of other packages and external
software.  This field should also be used for explaining the package
name if necessary.  URLs should be enclosed in angle brackets, e.g.
‘<https://www.r-project.org>’: see also Specifying URLs.
 The mandatory ‘Author’ field describes who wrote the
package.  It is a plain text field intended for human readers, but not
for automatic processing (such as extracting the email addresses of all
listed contributors: for that use ‘Authors@R’).  Note that all
significant contributors must be included: if you wrote an R wrapper
for the work of others included in the src directory, you are not
the sole (and maybe not even the main) author.
 The mandatory ‘Maintainer’ field should give a single name
followed by a valid (RFC 2822) email address in angle brackets.  It
should not end in a period or comma.  This field is what is reported by
the maintainer function and used by bug.report.  For a
CRAN package it should be a person, not a mailing list
and not a corporate entity: do ensure that it is valid and will remain
valid for the lifetime of the package.
 Note that the display name (the part before the address in angle
brackets) should be enclosed in double quotes if it contains
non-alphanumeric characters such as comma or period.  (The current
standard, RFC 5322, allows periods but RFC 2822 did not.)
 Both ‘Author’ and ‘Maintainer’ fields can be omitted if a
suitable ‘Authors@R’ field is given.  This field can be used to
provide a refined and machine-readable description of the package
“authors” (in particular specifying their precise roles), via
suitable R code. It should create an object of class "person',
by either a call to person or a series of calls (one per
“author”) concatenated by c()): see the example
DESCRIPTION file above.  The roles can include ‘"aut"’
(author) for full authors, ‘"cre"’ (creator) for the package
maintainer, and ‘"ctb"’ (contributor) for other contributors,
‘"cph"’ (copyright holder), among others.  See ?person for
more information.  Note that no role is assumed by default.
Auto-generated package citation information takes advantage of this
specification. The ‘Author’ and ‘Maintainer’ fields are
auto-generated from it if needed when building5 or installing.
 An optional ‘Copyright’ field can be used where the copyright
holder(s) are not the authors.  If necessary, this can refer to an
installed file: the convention is to use file inst/COPYRIGHTS.
 The optional ‘Date’ field gives the release date of the
current version of the package.  It is strongly recommended6 to use the ‘yyyy-mm-dd’ format conforming to the ISO
8601 standard.
 The ‘Depends’, ‘Imports’, ‘Suggests’, ‘Enhances’,
‘LinkingTo’ and ‘Additional_repositories’ fields are discussed
in a later subsection.
 Dependencies external to the R system should be listed in the
‘SystemRequirements’ field, possibly amplified in a separate
README file.
 The ‘URL’ field may give a list of URLs
separated by commas or whitespace, for example the homepage of the
author or a page where additional material describing the software can
be found.  These URLs are converted to active hyperlinks in
CRAN package listings.  See Specifying URLs.
 The ‘BugReports’ field may contain a single
URL to which bug reports about the package should be
submitted.  This URL will be used by bug.report
instead of sending an email to the maintainer.
 Base and recommended packages (i.e., packages contained in the R
source distribution or available from CRAN and recommended to
be included in every binary distribution of R) have a ‘Priority’
field with value ‘base’ or ‘recommended’, respectively.  These
priorities must not be used by other packages.
 A ‘Collate’ field can be used for controlling the collation order
for the R code files in a package when these are processed for
package installation.  The default is to collate according to the
‘C’ locale.  If present, the collate specification must list
all R code files in the package (taking possible OS-specific
subdirectories into account, see Package subdirectories) as a
whitespace separated list of file paths relative to the R
subdirectory.
Paths containing white space or quotes need to be quoted.  An
OS-specific collation field (‘Collate.unix’ or
‘Collate.windows’) will be used in preference to ‘Collate’.
 The ‘LazyData’ logical field controls whether the R datasets use
lazy-loading.  A ‘LazyLoad’ field was used in versions prior to
2.14.0, but now is ignored.
 The ‘KeepSource’ logical field controls if the package code is sourced
using keep.source = TRUE or FALSE: it might be needed
exceptionally for a package designed to always be used with
keep.source = TRUE.
 The ‘ByteCompile’ logical field controls if the package code is to
be byte-compiled on installation: the default is currently not to, so
this may be useful for a package known to benefit particularly from
byte-compilation (which can take quite a long time and increases the
installed size of the package).  It is used for the recommended
packages, as they are byte-compiled when R is installed and for
consistency should be byte-compiled when updated. This can be overridden
by installing with flag --no-byte-compile.
 The ‘ZipData’ logical field was used to control whether the automatic
Windows build would zip up the data directory or not prior to R
2.13.0: it is now ignored.
 The ‘Biarch’ logical field is used on Windows to select the
INSTALL option --force-biarch for this package.
(Introduced in R 3.0.0.)
 The ‘BuildVignettes’ logical field can be set to a false value to
stop R CMD build from attempting to build the vignettes, as
well as preventing7  R CMD check from testing
this.  This should only be used exceptionally, for example if the PDFs
include large figures which are not part of the package sources (and
hence only in packages which do not have an Open Source license).
 The ‘VignetteBuilder’ field names (in a comma-separated list)
packages that provide an engine for building vignettes.  These may
include the current package, or ones listed in ‘Depends’,
‘Suggests’ or ‘Imports’. The utils package is always
implicitly appended.  See Non-Sweave vignettes for
details.
 If the DESCRIPTION file is not entirely in ASCII it
should contain an ‘Encoding’ field specifying an encoding.  This is
used as the encoding of the DESCRIPTION file itself and of the
R and NAMESPACE files, and as the default encoding of
.Rd files.  The examples are assumed to be in this encoding when
running R CMD check, and it is used for the encoding of the
CITATION file.  Only encoding names latin1, latin2
and UTF-8 are known to be portable.  (Do not specify an encoding
unless one is actually needed: doing so makes the package less
portable.  If a package has a specified encoding, you should run
R CMD build etc in a locale using that encoding.)
 The ‘NeedsCompilation’ field should be set to "yes" if the
package contains code which to be compiled, otherwise "no" (when
the package could be installed from source on any platform without
additional tools).  This is used by install.packages(type =
"both") in R >= 2.15.2 on platforms where binary packages are the
norm: it is normally set by R CMD build or the repository
assuming compilation is required if and only if the package has a
src directory.
 The ‘OS_type’ field specifies the OS(es) for which the
package is intended.  If present, it should be one of unix or
windows, and indicates that the package can only be installed
on a platform with ‘.Platform$OS.type’ having that value.
 The ‘Type’ field specifies the type of the package:
see Package types.
 One can add subject classifications for the content of the package using
the fields ‘Classification/ACM’ or ‘Classification/ACM-2012’
(using the Computing Classification System of the Association for
Computing Machinery, http://www.acm.org/about/class/; the former refers
to the 1998 version), ‘Classification/JEL’ (the Journal of Economic
Literature Classification System,
https://www.aeaweb.org/econlit/jelCodes.php, or
‘Classification/MSC’ or ‘Classification/MSC-2010’ (the
Mathematics Subject Classification of the American Mathematical Society,
http://www.ams.org/msc/; the former refers to the 2000 version).
The subject classifications should be comma-separated lists of the
respective classification codes, e.g., ‘Classification/ACM: G.4,
H.2.8, I.5.1’.
 A ‘Language’ field can be used to indicate if the package
documentation is not in English: this should be a comma-separated list
of standard (not private use or grandfathered) IETF language tags as
currently defined by RFC 5646
(https://tools.ietf.org/html/rfc5646, see also
https://en.wikipedia.org/wiki/IETF_language_tag), i.e., use
language subtags which in essence are 2-letter ISO 639-1
(https://en.wikipedia.org/wiki/ISO_639-1) or 3-letter ISO
639-3 (https://en.wikipedia.org/wiki/ISO_639-3) language
codes.
 An ‘RdMacros’ field can be used to hold a comma-separated list of
packages from which the current package will import Rd macro
definitions.  These will be imported after the system macros, in the
order listed in the ‘RdMacros’ field, before any macro definitions
in the current package are loaded.  Macro definitions in individual
.Rd files in the man directory are loaded last, and are
local to later parts of that file.  In case of duplicates, the last
loaded definition will be used8  Both R CMD
Rd2pdf and R CMD Rdconv have an optional flag
--RdMacros=pkglist.  The option is also a comma-separated list
of package names, and has priority over the value given in
DESCRIPTION.  Packages using Rd macros should depend on
R 3.2.0 or later.
 Note: There should be no ‘Built’ or ‘Packaged’ fields, as these are
added by the package management tools.
 There is no restriction on the use of other fields not mentioned here
(but using other capitalizations of these field names would cause
confusion).  Fields Note, Contact (for contacting the
authors/developers) and MailingList are in common use. Some
repositories (including CRAN and R-forge) add their own
fields.
 
Next: Package Dependencies, Previous: The DESCRIPTION file, Up: Package structure   [Contents][Index] Licensing for a package which might be distributed is an important but
potentially complex subject.
 It is very important that you include license information!  Otherwise,
it may not even be legally correct for others to distribute copies of
the package, let alone use it.
 The package management tools use the concept of
‘free or open source software’
(FOSS, e.g., https://en.wikipedia.org/wiki/FOSS)
licenses: the idea being that some users of R and its packages want
to restrict themselves to such software.  Others need to ensure that
there are no restrictions stopping them using a package, e.g.
forbidding commercial or military use.  It is a central tenet of FOSS
software that there are no restrictions on users nor usage.
 Do not use the ‘License’ field for information on copyright
holders: if needed, use a ‘Copyright’ field.
 The mandatory ‘License’ field in the DESCRIPTION file should
specify the license of the package in a standardized form.  Alternatives
are indicated via vertical bars.  Individual specifications must
be one of
 as made available via https://www.R-project.org/Licenses/ and
contained in subdirectory share/licenses of the R source or home
directory.
 Abbreviations GPL and LGPL are ambiguous and usually taken
to mean any version of the license: but it is better not to use them.
 If a package license restricts a base license (where permitted,
e.g., using GPL-3 or AGPL-3 with an attribution clause), the additional
terms should be placed in file LICENSE (or LICENCE), and
the string ‘+ file LICENSE’ (or ‘+ file LICENCE’,
respectively) should be appended to the corresponding individual license
specification.  Note that several commonly used licenses do not permit
restrictions: this includes GPL-2 and hence any specification which
includes it.
 Examples of standardized specifications include
 Please note in particular that “Public domain” is not a valid license,
since it is not recognized in some jurisdictions.
 Please ensure that the license you choose also covers any dependencies
(including system dependencies) of your package: it is particularly
important that any restrictions on the use of such dependencies are
evident to people reading your DESCRIPTION file.
 Fields ‘License_is_FOSS’ and ‘License_restricts_use’ may be
added by repositories where information cannot be computed from the name
of the license.  ‘License_is_FOSS: yes’ is used for licenses which
are known to be FOSS, and ‘License_restricts_use’ can have values
‘yes’ or ‘no’ if the LICENSE file is known to restrict
users or usage, or known not to.  These are used by, e.g., the
available.packages filters.
 The optional file LICENSE/LICENCE contains a copy of the
license of the package.  To avoid any confusion only include such a file
if it is referred to in the ‘License’ field of the
DESCRIPTION file.
 Whereas you should feel free to include a license file in your
source distribution, please do not arrange to install yet
another copy of the GNU COPYING or COPYING.LIB
files but refer to the copies on
https://www.R-project.org/Licenses/ and included in the R
distribution (in directory share/licenses).  Since files named
LICENSE or LICENCE will be installed, do not use
these names for standard license files.  To include comments about the
licensing rather than the body of a license, use a file named something
like LICENSE.note.
 A few “standard” licenses are rather license templates which need
additional information to be completed via ‘+ file LICENSE’.
 
Next: The INDEX file, Previous: Licensing, Up: Package structure   [Contents][Index] The ‘Depends’ field gives a comma-separated list of package names
which this package depends on.  Those packages will be attached before
the current package when library or require is called.
Each package name may be optionally followed by a comment in parentheses
specifying a version requirement.  The comment should contain a
comparison operator, whitespace and a valid version number,
e.g. ‘MASS (>= 3.1-20)’.
 The ‘Depends’ field can also specify a dependence on a certain
version of R — e.g., if the package works only with R
version 3.0.0 or later, include ‘R (>= 3.0.0)’ in the
‘Depends’ field.  You can also require a certain SVN revision for
R-devel or R-patched, e.g. ‘R (>= 2.14.0), R (>= r56550)’
requires a version later than R-devel of late July 2011 (including
released versions of 2.14.0).
 It makes no sense to declare a dependence on R without a version
specification, nor on the package base: this is an R package
and package base is always available.
 A package or ‘R’ can appear more than once in the ‘Depends’
field, for example to give upper and lower bounds on acceptable versions.
 Both library and the R package checking facilities use this
field: hence it is an error to use improper syntax or misuse the
‘Depends’ field for comments on other software that might be
needed.  The R INSTALL facilities check if the version of
R used is recent enough for the package being installed, and the list
of packages which is specified will be attached (after checking version
requirements) before the current package.
 The ‘Imports’ field lists packages whose namespaces are imported
from (as specified in the NAMESPACE file) but which do not need
to be attached.  Namespaces accessed by the ‘::’ and ‘:::’
operators must be listed here, or in ‘Suggests’ or ‘Enhances’
(see below).  Ideally this field will include all the standard packages
that are used, and it is important to include S4-using packages (as
their class definitions can change and the DESCRIPTION file is
used to decide which packages to re-install when this happens).
Packages declared in the ‘Depends’ field should not also be in the
‘Imports’ field.  Version requirements can be specified and are
checked when the namespace is loaded (since R >= 3.0.0).
 The ‘Suggests’ field uses the same syntax as ‘Depends’ and
lists packages that are not necessarily needed.  This includes packages
used only in examples, tests or vignettes (see Writing package vignettes), and packages loaded in the body of functions.  E.g.,
suppose an example9 from
package foo uses a dataset from package bar. Then it is not
necessary to have bar use foo unless one wants to execute
all the examples/tests/vignettes: it is useful to have bar, but
not necessary.  Version requirements can be specified, and will be used
by R CMD check.
 Finally, the ‘Enhances’ field lists packages “enhanced” by the
package at hand, e.g., by providing methods for classes from these
packages, or ways to handle objects from these packages (so several
packages have ‘Enhances: chron’ because they can handle datetime
objects from chron even though they prefer R’s native
datetime functions).  Version requirements can be specified, but are
currently not used.  Such packages cannot be required to check the
package: any tests which use them must be conditional on the presence
of the package.  (If your tests use e.g. a dataset from another
package it should be in ‘Suggests’ and not ‘Enhances’.)
 The general rules are
 In particular, packages providing “only” data for examples or
vignettes should be listed in ‘Suggests’ rather than ‘Depends’
in order to make lean installations possible.
 Version dependencies in the ‘Depends’ and ‘Imports’ fields are
used by library when it loads the package, and
install.packages checks versions for the ‘Depends’,
‘Imports’ and (for dependencies = TRUE) ‘Suggests’
fields.
 It is increasingly important that the information in these fields is
complete and accurate: it is for example used to compute which packages
depend on an updated package and which packages can safely be installed
in parallel.
 This scheme was developed before all packages had namespaces (R
2.14.0 in October 2011), and good practice changed once that was in
place.
 Field ‘Depends’ should nowadays be used rarely, only for packages
which are intended to be put on the search path to make their facilities
available to the end user (and not to the package itself): for example
it makes sense that a user of package latticeExtra would want
the functions of package lattice made available.
 Almost always packages mentioned in ‘Depends’ should also be
imported from in the NAMESPACE file: this ensures that any needed
parts of those packages are available when some other package imports
the current package.
 The ‘Imports’ field should not contain packages which are not
imported from (via the NAMESPACE file or :: or
::: operators), as all the packages listed in that field need to
be installed for the current package to be installed.  (This is checked
by R CMD check.)
 R code in the package should call library or require
only exceptionally.  Such calls are never needed for packages listed in
‘Depends’ as they will already be on the search path.  It used to
be common practice to use require calls for packages listed in
‘Suggests’ in functions which used their functionality, but
nowadays it is better to access such functionality via ::
calls.
 A package that wishes to make use of header files in other packages needs
to declare them as a comma-separated list in the field ‘LinkingTo’
in the DESCRIPTION file.  For example
 As from R 3.0.2 the ‘LinkingTo’ field can have a version
requirement which is checked at installation.  (In earlier versions of
R it would cause the specification to be ignored.)
 Specifying a package in ‘LinkingTo’ suffices if these are C++
headers containing source code or static linking is done at
installation: the packages do not need to be (and usually should not be)
listed in the ‘Depends’ or ‘Imports’ fields.  This includes
CRAN package BH and almost all users of
RcppArmadillo and RcppEigen.
 For another use of ‘LinkingTo’ see Linking to native routines in other packages.
 The ‘Additional_repositories’ field is a comma-separated list of
repository URLs where the packages named in the other fields may be
found.  It is currently used by R CMD check to check that the
packages can be found, at least as source packages (which can be
installed on any platform).
 
Previous: Package Dependencies, Up: Package Dependencies   [Contents][Index] Note that someone wanting to run the examples/tests/vignettes may not
have a suggested package available (and it may not even be possible to
install it for that platform).  The recommendation used to be to make
their use conditional via if(require("pkgname"))):
this is fine if that conditioning is done in examples/tests/vignettes.
 However, using require for conditioning in package code is
not good practice as it alters the search path for the rest of the
session and relies on functions in that package not being masked by
other require or library calls.  It is better practice to
use code like
 Note the use of rgl:: as that object would not necessarily be
visible (and if it is, it need not be the one from that namespace:
plot3d occurs in several other packages).  If the intention is to
give an error if the suggested package is not available, simply use
e.g. rgl::plot3d.
 Note that the recommendation to use suggested packages conditionally in
tests does also apply to packages used to manage test suites: a
notorious example was testthat which in version 1.0.0 contained
illegal C++ code and hence could not be installed on standards-compliant
platforms.
 As noted above, packages in ‘Enhances’ must be used
conditionally and hence objects within them should always be accessed
via ::.
 
Next: Package subdirectories, Previous: Package Dependencies, Up: Package structure   [Contents][Index] The optional file INDEX contains a line for each sufficiently
interesting object in the package, giving its name and a description
(functions such as print methods not usually called explicitly might not
be included).  Normally this file is missing and the corresponding
information is automatically generated from the documentation sources
(using tools::Rdindex()) when installing from source.
 The file is part of the information given by library(help =
pkgname).
 Rather than editing this file, it is preferable to put customized
information about the package into an overview help page
(see Documenting packages) and/or a vignette (see Writing package vignettes).
 
Next: Data in packages, Previous: The INDEX file, Up: Package structure   [Contents][Index] The R subdirectory contains R code files, only.  The code
files to be installed must start with an ASCII (lower or upper
case) letter or digit and have one of the extensions11 .R,
.S, .q, .r, or .s.  We recommend using
.R, as this extension seems to be not used by any other software.
It should be possible to read in the files using source(), so
R objects must be created by assignments.  Note that there need be no
connection between the name of the file and the R objects created by
it.  Ideally, the R code files should only directly assign R
objects and definitely should not call functions with side effects such
as require and options.  If computations are required to
create objects these can use code ‘earlier’ in the package (see the
‘Collate’ field) plus functions in the ‘Depends’ packages
provided that the objects created do not depend on those packages except
via namespace imports.
 Two exceptions are allowed: if the R subdirectory contains a file
sysdata.rda (a saved image of one or more R objects: please
use suitable compression as suggested by tools::resaveRdaFiles,
and see also the ‘SysDataCompression’ DESCRIPTION field.)
this will be lazy-loaded into the namespace environment – this is
intended for system datasets that are not intended to be user-accessible
via data.  Also, files ending in ‘.in’ will be
allowed in the R directory to allow a configure script to
generate suitable files.
 Only ASCII characters (and the control characters tab,
formfeed, LF and CR) should be used in code files.  Other characters are
accepted in comments12, but then the comments may not
be readable in e.g. a UTF-8 locale.  Non-ASCII characters in
object names will normally13 fail when the package is installed.  Any byte will
be allowed in a quoted character string but \uxxxx escapes should
be used for non-ASCII characters.  However,
non-ASCII character strings may not be usable in some locales
and may display incorrectly in others.
 Various R functions in a package can be used to initialize and
clean up.  See Load hooks.
 The man subdirectory should contain (only) documentation files
for the objects in the package in R documentation (Rd) format.
The documentation filenames must start with an ASCII (lower or
upper case) letter or digit and have the extension .Rd (the
default) or .rd.  Further, the names must be valid in
‘file://’ URLs, which means14
they must be entirely ASCII and not contain ‘%’.
See Writing R documentation files, for more information.  Note that
all user-level objects in a package should be documented; if a package
pkg contains user-level objects which are for “internal” use
only, it should provide a file pkg-internal.Rd which
documents all such objects, and clearly states that these are not meant
to be called by the user.  See e.g. the sources for package grid
in the R distribution.  Note that packages which use internal objects
extensively should not export those objects from their namespace, when
they do not need to be documented (see Package namespaces).
 Having a man directory containing no documentation files may give
an installation error.
 The man subdirectory may contain a subdirectory named macros;
this will contain source for user-defined Rd macros.
(See User-defined macros.)  These use the Rd format, but may
not contain anything but macro definitions, comments and whitespace.
 The R and man subdirectories may contain OS-specific
subdirectories named unix or windows.
 The sources and headers for the compiled code are in src, plus
optionally a file Makevars or Makefile.  When a package is
installed using R CMD INSTALL, make is used to control
compilation and linking into a shared object for loading into R.
There are default make variables and rules for this
(determined when R is configured and recorded in
R_HOME/etcR_ARCH/Makeconf), providing support for C,
C++, FORTRAN 77, Fortran 9x15, Objective C and Objective
C++16 with associated extensions .c, .cc or
.cpp, .f, .f90 or .f95, .m, and
.mm, respectively.  We recommend using .h for headers,
also for C++17 or Fortran 9x include files.  (Use of extension .C for
C++ is no longer supported.)  Files in the src directory should
not be hidden (start with a dot), and hidden files will under some
versions of R be ignored.
 It is not portable (and may not be possible at all) to mix all these
languages in a single package, and we do not support using both C++ and
Fortran 9x. Because R itself uses it, we know that C and FORTRAN 77
can be used together and mixing C and C++ seems to be widely successful.
 If your code needs to depend on the platform there are certain defines
which can used in C or C++.  On all Windows builds (even 64-bit ones)
‘_WIN32’ will be defined: on 64-bit Windows builds also
‘_WIN64’, and on OS X ‘__APPLE__’ is defined.18
 The default rules can be tweaked by setting macros19 in a file
src/Makevars (see Using Makevars).  Note that this mechanism
should be general enough to eliminate the need for a package-specific
src/Makefile.  If such a file is to be distributed, considerable
care is needed to make it general enough to work on all R platforms.
If it has any targets at all, it should have an appropriate first target
named ‘all’ and a (possibly empty) target ‘clean’ which
removes all files generated by running make (to be used by
‘R CMD INSTALL --clean’ and ‘R CMD INSTALL --preclean’).
There are platform-specific file names on Windows:
src/Makevars.win takes precedence over src/Makevars and
src/Makefile.win must be used.  Some make programs
require makefiles to have a complete final line, including a newline.
 A few packages use the src directory for purposes other than
making a shared object (e.g. to create executables).  Such packages
should have files src/Makefile and src/Makefile.win
(unless intended for only Unix-alikes or only Windows).
 In very special cases packages may create binary files other than the
shared objects/DLLs in the src directory.  Such files will not be
installed in a multi-architecture setting since R CMD INSTALL
--libs-only is used to merge multiple sub-architectures and it only
copies shared objects/DLLs.  If a package wants to install other
binaries (for example executable programs), it should provide an R
script src/install.libs.R which will be run as part of the
installation in the src build directory instead of copying
the shared objects/DLLs. The script is run in a separate R
environment containing the following variables: R_PACKAGE_NAME
(the name of the package), R_PACKAGE_SOURCE (the path to the
source directory of the package), R_PACKAGE_DIR (the path of the
target installation directory of the package), R_ARCH (the
arch-dependent part of the path, often empty), SHLIB_EXT (the
extension of shared objects) and WINDOWS (TRUE on Windows,
FALSE elsewhere).  Something close to the default behavior could
be replicated with the following src/install.libs.R file:
 On the other hand, executable programs could be installed along the
lines of
 Note the use of architecture-specific subdirectories of bin where
needed.
 The data subdirectory is for data files: See Data in packages.
 The demo subdirectory is for R scripts (for running via
demo()) that demonstrate some of the functionality of the
package.  Demos may be interactive and are not checked automatically, so
if testing is desired use code in the tests directory to achieve
this.  The script files must start with a (lower or upper case) letter
and have one of the extensions .R or .r.  If present, the
demo subdirectory should also have a 00Index file with one
line for each demo, giving its name and a description separated by a
tab or at least three spaces. (This index file is not generated
automatically.)  Note that a demo does not have a specified encoding and
so should be an ASCII file (see Encoding issues).  As from
R 3.0.0 demo() will use the package encoding if there is one,
but this is mainly useful for non-ASCII comments.
 The contents of the inst subdirectory will be copied recursively
to the installation directory.  Subdirectories of inst should not
interfere with those used by R (currently, R, data,
demo, exec, libs, man, help,
html and Meta, and earlier versions used latex,
R-ex).  The copying of the inst happens after src
is built so its Makefile can create files to be installed.  To
exclude files from being installed, one can specify a list of exclude
patterns in file .Rinstignore in the top-level source directory.
These patterns should be Perl-like regular expressions (see the help for
regexp in R for the precise details), one per line, to be
matched case-insensitively20
against the file and directory paths, e.g. doc/.*[.]png$ will
exclude all PNG files in inst/doc based on the extension.
 Note that with the exceptions of INDEX,
LICENSE/LICENCE and NEWS, information files at the
top level of the package will not be installed and so not be
known to users of Windows and OS X compiled packages (and not seen
by those who use R CMD INSTALL or install.packages
on the tarball).  So any information files you wish an end user to see
should be included in inst.  Note that if the named exceptions
also occur in inst, the version in inst will be that seen
in the installed package.
 Things you might like to add to inst are a CITATION file
for use by the citation function, and a NEWS.Rd file for
use by the news function.  See its help page for the specific
format restrictions of the NEWS.Rd file.
 Another file sometimes needed in inst is AUTHORS or
COPYRIGHTS to specify the authors or copyright holders when this
is too complex to put in the DESCRIPTION file.
 Subdirectory tests is for additional package-specific test code,
similar to the specific tests that come with the R distribution.
Test code can either be provided directly in a .R file, or
via a .Rin file containing code which in turn creates the
corresponding .R file (e.g., by collecting all function objects
in the package and then calling them with the strangest arguments).  The
results of running a .R file are written to a .Rout file.
If there is a corresponding21 .Rout.save file, these two are
compared, with differences being reported but not causing an error.  The
directory tests is copied to the check area, and the tests are
run with the copy as the working directory and with R_LIBS set to
ensure that the copy of the package installed during testing will be
found by library(pkg_name).  Note that the package-specific
tests are run in a vanilla R session without setting the
random-number seed, so tests which use random numbers will need to set
the seed to obtain reproducible results (and it can be helpful to do so
in all cases, to avoid occasional failures when tests are run).
 If directory tests has a subdirectory Examples containing
a file pkg-Ex.Rout.save, this is compared to the output
file for running the examples when the latter are checked.  Reference
output should be produced without having the --timings option
set (and note that --as-cran sets it).
 Subdirectory exec could contain additional executable scripts the
package needs, typically scripts for interpreters such as the shell,
Perl, or Tcl.  NB: only files (and not directories) under exec are
installed (and those with names starting with a dot are ignored), and
they are all marked as executable (mode 755, moderated by
‘umask’) on POSIX platforms.  Note too that this is not suitable
for executable programs since some platforms (including Windows)
support multiple architectures using the same installed package
directory.
 Subdirectory po is used for files related to localization:
see Internationalization.
 Subdirectory tools is the preferred place for auxiliary files
needed during configuration, and also for sources need to re-create
scripts (e.g. M4 files for autoconf).
 
Next: Non-R scripts in packages, Previous: Package subdirectories, Up: Package structure   [Contents][Index] The data subdirectory is for data files, either to be made
available via lazy-loading or for loading using data().
(The choice is made by the ‘LazyData’ field in the
DESCRIPTION file: the default is not to do so.)  It should not be
used for other data files needed by the package, and the convention has
grown up to use directory inst/extdata for such files.
 Data files can have one of three types as indicated by their extension:
plain R code (.R or .r), tables (.tab,
.txt, or .csv, see ?data for the file formats, and
note that .csv is not the standard22 CSV format), or
save() images (.RData or .rda).  The files should
not be hidden (have names starting with a dot).  Note that R code
should be “self-sufficient” and not make use of extra functionality
provided by the package, so that the data file can also be used without
having to load the package or its namespace.
 Images (extensions .RData23 or .rda) can contain
references to the namespaces of packages that were used to create them.
Preferably there should be no such references in data files, and in any
case they should only be to packages listed in the Depends and
Imports fields, as otherwise it may be impossible to install the
package.  To check for such references, load all the images into a
vanilla R session, and look at the output of
loadedNamespaces().
 If your data files are large and you are not using ‘LazyData’ you
can speed up installation by providing a file datalist in the
data subdirectory.  This should have one line per topic that
data() will find, in the format ‘foo’ if data(foo)
provides ‘foo’, or ‘foo: bar bah’ if data(foo) provides
‘bar’ and ‘bah’.  R CMD build will automatically add
a datalist file to data directories of over 1Mb, using the
function tools::add_datalist.
 Tables (.tab, .txt, or .csv files) can be
compressed by gzip, bzip2 or xz,
optionally with additional extension .gz, .bz2 or
.xz.
 If your package is to be distributed, do consider the resource
implications of large datasets for your users: they can make packages
very slow to download and use up unwelcome amounts of storage space, as
well as taking many seconds to load.  It is normally best to distribute
large datasets as .rda images prepared by save(, compress =
TRUE) (the default).  Using bzip2 or xz compression
will usually reduce the size of both the package tarball and the
installed package, in some cases by a factor of two or more.
 Package tools has a couple of functions to help with data images:
checkRdaFiles reports on the way the image was saved, and
resaveRdaFiles will re-save with a different type of compression,
including choosing the best type for that particular image.
 Some packages using ‘LazyData’ will benefit from using a form of
compression other than gzip in the installed lazy-loading
database.  This can be selected by the --data-compress option
to R CMD INSTALL or by using the ‘LazyDataCompression’
field in the DESCRIPTION file.  Useful values are bzip2,
xz and the default, gzip.  The only way to discover which
is best is to try them all and look at the size of the
pkgname/data/Rdata.rdb file.
 Lazy-loading is not supported for very large datasets (those which when
serialized exceed 2GB, the limit for the format on 32-bit platforms and
all platforms prior to R 3.0.0).
 The analogue for sysdata.rda is field ‘SysDataCompression’:
the default (since R 2.12.2) is xz for files bigger than 1MB
otherwise gzip.
 
Next: Specifying URLs, Previous: Data in packages, Up: Package structure   [Contents][Index] Code which needs to be compiled (C, C++, FORTRAN, Fortran 95 …)
is included in the src subdirectory and discussed elsewhere in
this document.
 Subdirectory exec could be used for scripts for interpreters such
as the shell, BUGS, JavaScript, Matlab, Perl, php (amap),
Python or Tcl (Simile), or even R.  However, it seems more
common to use the inst directory, for example
WriteXLS/inst/Perl, NMF/inst/m-files,
RnavGraph/inst/tcl, RProtoBuf/inst/python and
emdbook/inst/BUGS and gridSVG/inst/js.
 Java code is a special case: except for very small programs,
.java files should be byte-compiled (to a .class file) and
distributed as part of a .jar file: the conventional location for
the .jar file(s) is inst/java.  It is desirable (and
required under an Open Source license) to make the Java source files
available: this is best done in a top-level java directory in the
package—the source files should not be installed.
 If your package requires one of these interpreters or an extension then
this should be declared in the ‘SystemRequirements’ field of its
DESCRIPTION file.  (Users of Java most often do so via
rJava, when depending on/importing that suffices.)
 Windows and Mac users should be aware that the Tcl extensions
‘BWidget’ and ‘Tktable’ which are currently included with the
R for Windows and in the OS X installers are extensions and do
need to be declared for users of other platforms (and that
‘Tktable’ is less widely available than it used to be, including
not in the main repositories for major Linux distributions).
 ‘BWidget’ needs to be installed by the user on other OSes.  This is
fairly easy to do: first find the Tcl/Tk search path:
 then download the sources from
http://sourceforge.net/projects/tcllib/files/BWidget/ and
at the command line run something like
 substituting a location on the Tcl/Tk search path for /usr/local/lib if
needed.
 
Previous: Non-R scripts in packages, Up: Package structure   [Contents][Index] URLs in many places in the package documentation will be converted to
clickable hyperlinks in at least some of their renderings.  So care is
needed that their forms are correct and portable.
 The full URL should be given, including the scheme (often ‘http://’
or ‘https://’) and a final ‘/’ for references to directories.
 Spaces in URLs are not portable and how they are handled does vary by
HTTP server and by client.  There should be no space in the host part of
an ‘http://’ URL, and spaces in the remainder should be encoded,
with each space replaced by ‘%20’.
 Other characters may benefit from being encoded: see the help on
URLencode().
 The canonical URL for a CRAN package is
 and not a version starting
‘http://cran.r-project.org/web/packages/pkgname’.
 
Next: Checking and building packages, Previous: Package structure, Up: Creating R packages   [Contents][Index] Note that most of this section is specific to Unix-alikes: see the
comments later on about the Windows port of R.
 If your package needs some system-dependent configuration before
installation you can include an executable (Bourne24) shell script
configure in your package which (if present) is executed by
R CMD INSTALL before any other action is performed.  This can be
a script created by the Autoconf mechanism, but may also be a script
written by yourself.  Use this to detect if any nonstandard libraries
are present such that corresponding code in the package can be disabled
at install time rather than giving error messages when the package is
compiled or used.  To summarize, the full power of Autoconf is available
for your extension package (including variable substitution, searching
for libraries, etc.).
 Under a Unix-alike only, an executable (Bourne shell) script
cleanup is executed as the last thing by R CMD INSTALL if
option --clean was given, and by R CMD build when
preparing the package for building from its source.
 As an example consider we want to use functionality provided by a (C or
FORTRAN) library foo.  Using Autoconf, we can create a configure
script which checks for the library, sets variable HAVE_FOO to
TRUE if it was found and to FALSE otherwise, and then
substitutes this value into output files (by replacing instances of
‘@HAVE_FOO@’ in input files with the value of HAVE_FOO).
For example, if a function named bar is to be made available by
linking against library foo (i.e., using -lfoo), one
could use
 in configure.ac (assuming Autoconf 2.50 or later).
 The definition of the respective R function in foo.R.in could be
 From this file configure creates the actual R source file
foo.R looking like
 if library foo was not found (with the desired functionality).
In this case, the above R code effectively disables the function.
 One could also use different file fragments for available and missing
functionality, respectively.
 You will very likely need to ensure that the same C compiler and
compiler flags are used in the configure tests as when compiling
R or your package.  Under a Unix-alike, you can achieve this by
including the following fragment early in configure.ac
(before calling AC_PROG_CC)
 (Using ‘${R_HOME}/bin/R’ rather than just ‘R’ is necessary
in order to use the correct version of R when running the script as
part of R CMD INSTALL, and the quotes since ‘${R_HOME}’
might contain spaces.)
 If your code does load checks then you may also need
 and packages written with C++ need to pick up the details for the C++
compiler and switch the current language to C++ by something like
 The latter is important, as for example C headers may not be available
to C++ programs or may not be written to avoid C++ name-mangling.
 You can use R CMD config for getting the value of the basic
configuration variables, and also the header and library flags necessary
for linking a front-end executable program against R, see R CMD
config --help for details.
 To check for an external BLAS library using the ACX_BLAS macro
from the official Autoconf Macro Archive, one can simply do
 Note that FLIBS as determined by R must be used to ensure that
FORTRAN 77 code works on all R platforms.  Calls to the Autoconf macro
AC_F77_LIBRARY_LDFLAGS, which would overwrite FLIBS, must
not be used (and hence e.g. removed from ACX_BLAS).  (Recent
versions of Autoconf in fact allow an already set FLIBS to
override the test for the FORTRAN linker flags.)
 N.B.: If the configure script creates files, e.g.
src/Makevars, you do need a cleanup script to remove
them.  Otherwise R CMD build may ship the files that are
created.  For example, package RODBC has
 As this example shows, configure often creates working files
such as config.log.
 If your configure script needs auxiliary files, it is recommended that
you ship them in a tools directory (as R itself does).
 You should bear in mind that the configure script will not be used on
Windows systems.  If your package is to be made publicly available,
please give enough information for a user on a non-Unix-alike platform
to configure it manually, or provide a configure.win script to be
used on that platform.  (Optionally, there can be a cleanup.win
script.  Both should be shell scripts to be executed by ash,
which is a minimal version of Bourne-style sh.)  When
configure.win is run the environment variables R_HOME
(which uses ‘/’ as the file separator), R_ARCH and Use
R_ARCH_BIN will be set.  Use R_ARCH to decide if this is a
64-bit build (its value there is ‘/x64’) and to install DLLs to the
correct place (${R_HOME}/libs${R_ARCH}).  Use
R_ARCH_BIN to find the correct place under the bin
directory, e.g. ${R_HOME}/bin${R_ARCH_BIN}/Rscript.exe.
 In some rare circumstances, the configuration and cleanup scripts need
to know the location into which the package is being installed.  An
example of this is a package that uses C code and creates two shared
object/DLLs.  Usually, the object that is dynamically loaded by R
is linked against the second, dependent, object.  On some systems, we
can add the location of this dependent object to the object that is
dynamically loaded by R.  This means that each user does not have to
set the value of the LD_LIBRARY_PATH (or equivalent) environment
variable, but that the secondary object is automatically resolved.
Another example is when a package installs support files that are
required at run time, and their location is substituted into an R
data structure at installation time.



The names of the top-level library directory (i.e., specifiable
via the ‘-l’ argument) and the directory of the package
itself are made available to the installation scripts via the two
shell/environment variables R_LIBRARY_DIR and R_PACKAGE_DIR.
Additionally, the name of the package (e.g. ‘survival’ or
‘MASS’) being installed is available from the environment variable
R_PACKAGE_NAME.  (Currently the value of R_PACKAGE_DIR is
always ${R_LIBRARY_DIR}/${R_PACKAGE_NAME}, but this used not to
be the case when versioned installs were allowed.  Its main use is in
configure.win scripts for the installation path of external
software’s DLLs.)  Note that the value of R_PACKAGE_DIR may
contain spaces and other shell-unfriendly characters, and so should be
quoted in makefiles and configure scripts.
 One of the more tricky tasks can be to find the headers and libraries of
external software.  One tool which is increasingly available on
Unix-alikes (but not by default on OS X) to do this is
pkg-config.  The configure script will need to test for
the presence of the command itself (see for example package
Cairo), and if present it can be asked if the software is
installed, of a suitable version and for compilation/linking flags by
e.g.
 Note that pkg-config --libs gives the information
required to link against the default version of that library (usually
the dynamic one), and pkg-config --static is needed if the
static library is to be used.
 Sometimes the name by which the software is known to
pkg-config is not what one might expect (e.g.
‘gtk+-2.0’ even for 2.22).  To get a complete list use
 
Next: Configure example, Previous: Configure and cleanup, Up: Configure and cleanup   [Contents][Index] Sometimes writing your own configure script can be avoided by
supplying a file Makevars: also one of the most common uses of a
configure script is to make Makevars from
Makevars.in.
 A Makevars file is a makefile and is used as one of several
makefiles by R CMD SHLIB (which is called by R CMD
INSTALL to compile code in the src directory).  It should be
written if at all possible in a portable style, in particular (except
for Makevars.win) without the use of GNU extensions.
 The most common use of a Makevars file is to set additional
preprocessor options (for example include paths) for C/C++ files
via PKG_CPPFLAGS, and additional compiler flags by setting
PKG_CFLAGS, PKG_CXXFLAGS, PKG_FFLAGS or
PKG_FCFLAGS, for C, C++, FORTRAN or Fortran 9x respectively
(see Creating shared objects).
 N.B.: Include paths are preprocessor options, not compiler
options, and must be set in PKG_CPPFLAGS as otherwise
platform-specific paths (e.g. ‘-I/usr/local/include’) will take
precedence.
 Makevars can also be used to set flags for the linker, for
example ‘-L’ and ‘-l’ options, via PKG_LIBS.
 When writing a Makevars file for a package you intend to
distribute, take care to ensure that it is not specific to your
compiler: flags such as -O2 -Wall -pedantic (and all other
-W flags: for the Solaris compiler these are used to pass
arguments to compiler phases) are all specific to GCC.
 Also, do not set variables such as CPPFLAGS, CFLAGS etc.:
these should be settable by users (sites) through appropriate personal
(site-wide) Makevars files.
See Customizing package compilation in R Installation and Administration,
 There are some macros25  which are set whilst configuring the
building of R itself and are stored in
R_HOME/etcR_ARCH/Makeconf.  That makefile is included
as a Makefile after Makevars[.win], and the macros
it defines can be used in macro assignments and make command lines in
the latter.  These include
 A macro containing the set of libraries need to link FORTRAN code.  This
may need to be included in PKG_LIBS: it will normally be included
automatically if the package contains FORTRAN source files.
 A macro containing the BLAS libraries used when building R.  This may
need to be included in PKG_LIBS.  Beware that if it is empty then
the R executable will contain all the double-precision and
double-complex BLAS routines, but no single-precision nor complex
routines.  If BLAS_LIBS is included, then FLIBS also needs
to be26 included following it, as most BLAS
libraries are written at least partially in FORTRAN.
 A macro containing the LAPACK libraries (and paths where appropriate)
used when building R.  This may need to be included in
PKG_LIBS.  It may point to a dynamic library libRlapack
which contains the main double-precision LAPACK routines as well as
those double-complex LAPACK routines needed to build R, or it may
point to an external LAPACK library, or may be empty if an external BLAS
library also contains LAPACK.
 [libRlapack includes all the double-precision LAPACK routines
which were current in 2003: a list of which routines are included is in
file src/modules/lapack/README. Note that an external LAPACK/BLAS
library need not do so, as some were ‘deprecated’ (and not compiled by
default) in LAPACK 3.6.0 in late 2015.]
 For portability, the macros BLAS_LIBS and FLIBS should
always be included after LAPACK_LIBS (and in that order).
 A macro containing flags which are needed to circumvent
over-optimization of FORTRAN code: it is typically ‘-g -O2
-ffloat-store’ on ‘ix86’ platforms using gfortran.
Note that this is not an additional flag to be used as part of
PKG_FFLAGS, but a replacement for FFLAGS, and that it is
intended for the FORTRAN 77 compiler ‘F77’ and not necessarily for
the Fortran 90/95 compiler ‘FC’.  See the example later in this
section.
 Setting certain macros in Makevars will prevent R CMD
SHLIB setting them: in particular if Makevars sets
‘OBJECTS’ it will not be set on the make command line.
This can be useful in conjunction with implicit rules to allow other
types of source code to be compiled and included in the shared object.
It can also be used to control the set of files which are compiled,
either by excluding some files in src or including some files in
subdirectories.  For example
 Note that Makevars should not normally contain targets, as it is
included before the default makefile and make will call the
first target, intended to be all in the default makefile.  If you
really need to circumvent that, use a suitable (phony) target all
before any actual targets in Makevars.[win]: for example package
fastICA used to have
 needed to ensure that the LAPACK routines find some constants without
infinite looping.  The Windows equivalent was
 (since the other macros are all empty on that platform, and R’s
internal BLAS was not used).  Note that the first target in
Makevars will be called, but for back-compatibility it is best
named all.
 If you want to create and then link to a library, say using code in a
subdirectory, use something like
 Be careful to create all the necessary dependencies, as there is no
guarantee that the dependencies of all will be run in a
particular order (and some of the CRAN build machines use
multiple CPUs and parallel makes).  In particular,
 does not suffice.
 Note that on Windows it is required that Makevars[.win] does
create a DLL: this is needed as it is the only reliable way to ensure
that building a DLL succeeded.  If you want to use the src
directory for some purpose other than building a DLL, use a
Makefile.win file.
 It is sometimes useful to have a target ‘clean’ in Makevars
or Makevars.win: this will be used by R CMD build to
clean up (a copy of) the package sources.  When it is run by
build it will have fewer macros set, in particular not
$(SHLIB), nor $(OBJECTS) unless set in the file itself.
It would also be possible to add tasks to the target ‘shlib-clean’
which is run by R CMD INSTALL and R CMD SHLIB with
options --clean and --preclean.
 If you want to run R code in Makevars, e.g. to find
configuration information, please do ensure that you use the correct
copy of R or Rscript: there might not be one in the path
at all, or it might be the wrong version or architecture.  The correct
way to do this is via
 where $(R_ARCH_BIN) is only needed currently on Windows.
 Environment or make variables can be used to select different macros for
32- and 64-bit code, for example (GNU make syntax, allowed on
Windows)
 On Windows there is normally a choice between linking to an import
library or directly to a DLL.  Where possible, the latter is much more
reliable: import libraries are tied to a specific toolchain, and in
particular on 64-bit Windows two different conventions have been
commonly used.  So for example instead of
 one can use
 since on Windows -lxxx will look in turn for
 where the first and second are conventionally import libraries, the
third and fourth often static libraries (with .lib intended for
Visual C++), but might be import libraries.  See for example
https://sourceware.org/binutils/docs-2.20/ld/WIN32.html#WIN32.
 The fly in the ointment is that the DLL might not be named
libxxx.dll, and in fact on 32-bit Windows there is a
libxml2.dll whereas on one build for 64-bit Windows the DLL is
called libxml2-2.dll.  Using import libraries can cover over
these differences but can cause equal difficulties.
 If static libraries are available they can save a lot of problems with
run-time finding of DLLs, especially when binary packages are to be
distributed and even more when these support both architectures.  Where
using DLLs is unavoidable we normally arrange (via
configure.win) to ship them in the same directory as the package
DLL.
 
Next: Using pthreads, Previous: Using Makevars, Up: Using Makevars   [Contents][Index] There is some support for packages which wish to use
OpenMP27.  The
make macros
 are available for use in src/Makevars or src/Makevars.win.
Include the appropriate macro in PKG_CFLAGS, PKG_CPPFLAGS
and so on, and also in PKG_LIBS.  C/C++ code that needs to be
conditioned on the use of OpenMP can be used inside #ifdef
_OPENMP: note that some toolchains used for R (including many of
those using clang28)  have no OpenMP support at all, not even
omp.h.
 For example, a package with C code written for OpenMP should have in
src/Makevars the lines
 Note that the macro SHLIB_OPENMP_CXXFLAGS applies to the C++98
compiler and not necessarily to the C++11 compiler: users of the latter
should do their own configure checks.
 Some care is needed when compilers are from different families which may
use different OpenMP runtimes (e.g. clang vs GCC
including gfortran, although it is currently possible to use
the clang runtime with GCC but not vice versa).  For a
package with Fortran 77 code using OpenMP the appropriate lines are
 as the C compiler will be used to link the package code (and there is no
guarantee that this will work everywhere).
 There is nothing to say what version of OpenMP is supported: version 3.0
(May 2008) is supported by recent versions of the Linux, Windows and
Solaris platforms, but portable packages cannot assume that end users
have recent versions.  OS X currently uses Apple builds of
clang with no OpenMP support.
 The performance of OpenMP varies substantially between platforms.  Both
the Windows and earlier Apple OS X implementations have substantial
overheads and are only beneficial if quite substantial tasks are run in
parallel.  Also, on Windows new threads are started with the
default29 FPU control
word, so computations done on OpenMP threads will not make use of
extended-precision arithmetic which is the default for the main process.
 Calling any of the R API from threaded code is ‘for experts only’:
they will need to read the source code to determine if it is
thread-safe.  In particular, code which makes use of the stack-checking
mechanism must not be called from threaded code.
 Packages are not standard-alone programs, and an R process could
contain more than one OpenMP-enabled package as well as other components
(for example, an optimized BLAS) making use of OpenMP.  So careful
consideration needs to be given to resource usage.  OpenMP works with
parallel regions, and for most implementations the default is to use as
many threads as ‘CPUs’ for such regions.  Parallel regions can be
nested, although it is common to use only a single thread below the
first level.  The correctness of the detected number of ‘CPUs’ and the
assumption that the R process is entitled to use them all are both
dubious assumptions.  The best way to limit resources is to limit the
overall number of threads available to OpenMP in the R process: this
can be done via environment variable OMP_THREAD_LIMIT, where
implemented.30  Alternatively, the
number of threads per region can be limited by the environment variable
OMP_NUM_THREADS or API call omp_set_num_threads, or,
better, for the regions in your code as part of their
specification. E.g. R uses
 That way you only control your own code and not that of other OpenMP users.
 
Next: Compiling in sub-directories, Previous: OpenMP support, Up: Using Makevars   [Contents][Index] There is no direct support for the POSIX threads (more commonly known as
pthreads): by the time we considered adding it several packages
were using it unconditionally so it seems that nowadays it is
universally available on POSIX operating systems (hence not Windows).
 For reasonably recent versions of gcc and clang the
correct specification is
 (and the plural version is also accepted on some systems/versions).  For
other platforms the specification is
 (and note that the library name is singular).  This is what
-pthread does on all known current platforms (although earlier
versions of OpenBSD used a different library name).
 For a tutorial see
https://computing.llnl.gov/tutorials/pthreads/.
 POSIX threads are not normally used on Windows, which has its own native
concepts of threads.  However, there are two projects implementing
pthreads on top of Windows, pthreads-w32 and
winpthreads (part of the MinGW-w64 project).
 Whether Windows toolchains implement pthreads is up to the
toolchain provider.  A make variable
SHLIB_PTHREAD_FLAGS is available: this should be included in both
PKG_CPPFLAGS (or the Fortran or F9x equivalents) and
PKG_LIBS.
 The presence of a working pthreads implementation cannot be
unambiguously determined without testing for yourself: however, that
‘_REENTRANT’ is defined31 in C/C++ code is a good indication.
 See also the comments on thread-safety and performance under OpenMP: on
all known R platforms OpenMP is implemented via
pthreads and the known performance issues are in the latter.
 
Previous: Using pthreads, Up: Using Makevars   [Contents][Index] Package authors fairly often want to organize code in sub-directories of
src, for example if they are including a separate piece of
external software to which this is an R interface.
 One simple way is simply to set OBJECTS to be all the objects
that need to be compiled, including in sub-directories.  For example,
CRAN package RSiena has
 One problem with that approach is that unless GNU make extensions are
used, the source files need to be listed and kept up-to-date.  As in the
following from CRAN package lossDev:
 Where the subdirectory is self-contained code with a suitable makefile,
the best approach is something like
 Note the quotes: the macros can contain spaces, e.g. CC = "gcc
-m64 -std=gnu99".  Several authors have forgotten about parallel makes:
the static library in the subdirectory must be made before the shared
object ($(SHLIB)) and so the latter must depend on the former.
Others forget the need for position-independent code.
 We really do not recommend using src/Makefile instead of
src/Makevars, and as the example above shows, it is not
necessary.
 
Next: Using F95 code, Previous: Using Makevars, Up: Configure and cleanup   [Contents][Index] It may be helpful to give an extended example of using a
configure script to create a src/Makevars file: this is
based on that in the RODBC package.
 The configure.ac file follows: configure is created from
this by running autoconf in the top-level package directory
(containing configure.ac).
 where src/Makevars.in would be simply
 A user can then be advised to specify the location of the ODBC driver
manager files by options like (lines broken for easier reading)
 or by setting the environment variables ODBC_INCLUDE and
ODBC_LIBS.
 
Next: Using C++11 code, Previous: Configure example, Up: Configure and cleanup   [Contents][Index] R assumes that source files with extension .f are FORTRAN 77,
and passes them to the compiler specified by ‘F77’.  On most but
not all platforms that compiler will accept Fortran 90/95 code: some
platforms have a separate Fortran 90/95 compiler and a few (by now quite
rare32) platforms have no Fortran
90/95 support.
 This means that portable packages need to be written in correct
FORTRAN 77, which will also be valid Fortran 95.  See
https://developer.R-project.org/Portability.html for reference
resources.  In particular, free source form F95 code is not
portable.
 On some systems an alternative F95 compiler is available: from the
gcc family this might be gfortran or g95.
Configuring R will try to find a compiler which (from its name)
appears to be a Fortran 90/95 compiler, and set it in macro ‘FC’.
Note that it does not check that such a compiler is fully (or even
partially) compliant with Fortran 90/95.  Packages making use of Fortran
90/95 features should use file extension .f90 or .f95 for
the source files: the variable PKG_FCFLAGS specifies any special
flags to be used.  There is no guarantee that compiled Fortran 90/95
code can be mixed with any other type of compiled code, nor that a build
of R will have support for such packages.
 Some (but not) all compilers specified by the ‘FC’ macro will
accept Fortran 2003 or 2008 code: such code should still use file
extension .f90 or .f95.  For platforms using
gfortran, you may need to include -std=f2003 or
-std=f2008 in PKG_FCFLAGS: the default is ‘GNU Fortran’,
Fortran 95 with non-standard extensions.  The Solaris f95
compiler ‘accepts some Fortran 2003 features’.
 Modern versions of Fortran support modules, whereby compiling one source
file creates a module file which is then included in others. (Module
files typically have a .mod extension: they do depend on the
compiler used and so should never be included in a package.)  This
creates a dependence which make will not know about and often
causes installation with a parallel make to fail.  Thus it is necessary
to add explicit dependencies to src/Makevars to tell
make the constraints on the order of compilation.  For
example, if file iface.f90 creates a module ‘iface’ used by
files cmi.f90 and dmi.f90 then src/Makevars needs
to contain something like
 
Previous: Using F95 code, Up: Configure and cleanup   [Contents][Index] R can be built without a C++ compiler although one is available (but
not necessarily installed) on all known R platforms.  For full
portability across platforms, all that can be assumed is approximate
support for the C++98 standard (the widely used g++ deviates
considerably from the standard).  Some compilers have a concept of
‘C++03’ (‘essentially a bug fix’) or ‘C++ Technical Report 1’ (TR1), an
optional addition to the ‘C++03’ revision which was published in 2007.
A revised standard was published in 2011 and compilers with pretty much
complete implementations are becoming available.  C++11 added all of the
C99 features which are not otherwise implemented in C++, and C++
compilers commonly accept C99 extensions to C++98.  A minor update to
C++11 (often known as C++14) was approved in August 2014.
 What standard a C++ compiler aims to support can be hard to determine.
As from version 6 (to be released in 2016), g++ will default
to C++14: earlier versions aim to support C++03 with many extensions
(including support for TR1).  clang with its
native33  libcxx headers and library
includes many C++11 features, and does not support TR1.
 Since version 3.1.0, R has provided support for C++11 in packages in
addition to C++98.  This support is not uniform across platforms as it
depends on the capabilities of the compiler (see below).  When R is
configured, it will determine whether the C++ compiler supports C++11
and which compiler flags, if any, are required to enable C++11 support.
For example, recent versions of g++ or clang++
accept the compiler flag -std=c++11, and earlier versions
support a flag -std=c++0x, but the latter only provides partial
support for the C++11 standard.
 In order to use C++11 code in a package, the package’s Makevars
file (or Makevars.win on Windows) should include the line
 Compilation and linking will then be done with the C++11 compiler.  If
any other value is given to the ‘CXX_STD’ macro it will be ignored.
(Further options may become available in the future as the C++ standard
evolves.)
 Packages without a Makevars file may specify that they require
C++11 by including ‘C++11’ in the ‘SystemRequirements’ field
of the DESCRIPTION file, e.g.
 If a package does have a Makevars[.win] file then setting the
make variable ‘CXX_STD’ is preferred, as it allows R CMD
SHLIB to work correctly in the package’s src directory.
 The C++11 compiler will be used systematically by R for all C++ code
if the environment variable USE_CXX1X is defined (with any
value). Hence this environment variable should be defined when invoking
R CMD SHLIB in the absence of a Makevars file (or
Makevars.win on Windows) if a C++11 compiler is required.
 Further control over compilation of C++11 code can be obtained by
specifying the macros ‘CXX1X’ and ‘CXX1XSTD’ when R is
configured34, or in a personal or site Makevars file.
See Customizing package compilation in R Installation and Administration.
If C++11 support is not available then these macros are both
empty. Otherwise, ‘CXX1X’ defaults to the same value as the C++
compiler ‘CXX’ and the flag ‘CXX1XSTD’ defaults to
-std=c++11 or -std=c++0x (the latter on Windows).  It
is possible to specify ‘CXX1X’ to be a distinct compiler just for
C++11–using packages, e.g. g++ on Solaris.  Note however
that different C++ compilers (and even different versions of the same
compiler) often differ in their ABI so their outputs can rarely be
mixed. By setting ‘CXX1XSTD’ it is also possible to choose a
different dialect of the standard, such as -std=gnu++11, or
enable support for the 2014 revision using something like
-std=c++14 or -std=c++1y.
 As noted above, support for C++11 varies across platforms.  The default
compiler on Windows is GCC 4.6.x and supports the -std=c++0x
flag and some C++11 features (see
https://gcc.gnu.org/gcc-4.6/cxx0x_status.html).  On some
platforms, it may be possible or necessary to select a different
compiler for C++11, via personal or site Makevars files.
 There is no guarantee that C++11 can be used in a package in combination
with any other compiled language (even C), as the C++11 compiler may be
incompatible with the native compilers for the platform.  (There are
known problems mixing C++11 with Fortran.)
 If a package using C++11 has a configure script it is
essential that it selects the correct compiler, via something like
 (paying attention to all the quotes required).
 
Next: Writing package vignettes, Previous: Configure and cleanup, Up: Creating R packages   [Contents][Index] Before using these tools, please check that your package can be
installed (which checked it can be loaded).  R CMD check will
inter alia do this, but you may get more detailed error messages
doing the install directly.
 If your package specifies an encoding in its DESCRIPTION file,
you should run these tools in a locale which makes use of that encoding:
they may not work at all or may work incorrectly in other locales
(although UTF-8 locales will most likely work).
 Note: R CMD check and R CMD build run R processes with
--vanilla in which none of the user’s startup files are read.
If you need R_LIBS set (to find packages in a non-standard
library) you can set it in the environment: also you can use the check
and build environment files (as specified by the environment variables
R_CHECK_ENVIRON and R_BUILD_ENVIRON; if unset,
files35  ~/.R/check.Renviron and
~/.R/build.Renviron are used) to set environment variables when
using these utilities.
 Note to Windows users: R CMD build may make use of the Windows toolset (see the “R
Installation and Administration” manual) if present and in your path,
and it is required for packages which need it to install (including
those with configure.win or cleanup.win scripts or a
src directory) and e.g. need vignettes built.
 You may need to set the environment variable TMPDIR to point to a
suitable writable directory with a path not containing spaces – use
forward slashes for the separators.  Also, the directory needs to be on
a case-honouring file system (some network-mounted file systems are
not).
 
Next: Building package tarballs, Previous: Checking and building packages, Up: Checking and building packages   [Contents][Index] Using R CMD check, the R package checker, one can test whether
source R packages work correctly.  It can be run on one or
more directories, or compressed package tar archives with
extension .tar.gz, .tgz, .tar.bz2 or
.tar.xz.
 It is strongly recommended that the final checks are run on a
tar archive prepared by R CMD build.
 This runs a series of checks, including
 To allow a configure script to generate suitable files, files
ending in ‘.in’ will be allowed in the R directory.
 A warning is given for directory names that look like R package check
directories – many packages have been submitted to CRAN
containing these.
 Compiled code is checked for symbols corresponding to functions which
might terminate R or write to stdout/stderr instead of
the console.  Note that the latter might give false positives in that
the symbols might be pulled in with external libraries and could never
be called.  Windows38 users
should note that the Fortran and C++ runtime libraries are examples of
such external libraries.
 Of course, released packages should be able to run at least their own
examples.  Each example is run in a ‘clean’ environment (so earlier
examples cannot be assumed to have been run), and with the variables
T and F redefined to generate an error unless they are set
in the example: See Logical vectors in An
Introduction to R.
 If there is an error39 in executing the R code in vignette foo.ext, a log
file foo.ext.log is created in the check directory.  The
vignette PDFs are re-made in a copy of the package sources in the
vign_test subdirectory of the check directory, so for further
information on errors look in directory
pkgname/vign_test/vignettes.  (It is only retained if there
are errors or if environment variable _R_CHECK_CLEAN_VIGN_TEST_ is
set to a false value.)
 All these tests are run with collation set to the C locale, and
for the examples and tests with environment variable LANGUAGE=en:
this is to minimize differences between platforms.
 Use R CMD check --help to obtain more information about the usage
of the R package checker.  A subset of the checking steps can be
selected by adding command-line options. It also allows customization by
setting environment variables _R_CHECK_*_:, as described in
Tools in R Internals:
a set of these customizations similar to those used by CRAN
can be selected by the option --as-cran (which works best if
Internet access is available).  Some Windows users may
need to set environment variable R_WIN_NO_JUNCTIONS to a non-empty
value.  The test of cyclic declarations40in DESCRIPTION files needs
repositories (including CRAN) set: do this in
~/.Rprofile, by e.g.
 One check customization which can be revealing is
 which reports unused local assignments.  Not only does this point out
computations which are unnecessary because their results are unused, it
also can show errors.  (Two such are to intend to update an object by
assigning a value but mistype its name or assign in the wrong scope,
for example using <- where <<- was intended.)  This can
give false positives, most commonly because of non-standard evaluation
for formulae and because the intention is to return objects in the
environment of a function for later use.
 Complete checking of a package which contains a file README.md
needs pandoc installed: see
http://johnmacfarlane.net/pandoc/installing.html.  This
should be reasonably current: at the time of writing CRAN used
version 1.12.4.2 to process these files.
 You do need to ensure that the package is checked in a suitable locale
if it contains non-ASCII characters.  Such packages are likely
to fail some of the checks in a C locale, and R CMD
check will warn if it spots the problem.  You should be able to check
any package in a UTF-8 locale (if one is available).  Beware that
although a C locale is rarely used at a console, it may be the
default if logging in remotely or for batch jobs.
 Multiple sub-architectures: On systems which support multiple sub-architectures (principally
Windows), R CMD check will install and check a package which
contains compiled code under all available sub-architectures. (Use
option --force-multiarch to force this for packages without
compiled code, which are otherwise only checked under the main
sub-architecture.)  This will run the loading tests, examples and
tests directory under each installed sub-architecture in turn,
and give an error if any fail.  Where environment variables (including
perhaps PATH) need to be set differently for each
sub-architecture, these can be set in architecture-specific files such
as R_HOME/etc/i386/Renviron.site.
 An alternative approach is to use R CMD check --no-multiarch
to check the primary sub-architecture, and then to use something like
R --arch=x86_64 CMD check --extra-arch or (Windows)
/path/to/R/bin/x64/Rcmd check --extra-arch to run for each
additional sub-architecture just the checks41 which differ by sub-architecture.  (This
approach is required for packages which are installed by R CMD
INSTALL --merge-multiarch.)
 Where packages need additional commands to install all the
sub-architectures these can be supplied by e.g.
--install-args=--force-biarch.
 
Next: Building binary packages, Previous: Checking packages, Up: Checking and building packages   [Contents][Index] Packages may be distributed in source form as “tarballs”
(.tar.gz files) or in binary form. The source form can be
installed on all platforms with suitable tools and is the usual form for
Unix-like systems; the binary form is platform-specific, and is the more
common distribution form for the Windows and OS X platforms.
 Using R CMD build, the R package builder, one can build R
package tarballs from their sources (for example, for subsequent release).
 Prior to actually building the package in the standard gzipped tar file
format, a few diagnostic checks and cleanups are performed.  In
particular, it is tested whether object indices exist and can be assumed
to be up-to-date, and C, C++ and FORTRAN source files and relevant
makefiles in a src directory are tested and converted to LF
line-endings if necessary.
 Run-time checks whether the package works correctly should be performed
using R CMD check prior to invoking the final build procedure.
 To exclude files from being put into the package, one can specify a list
of exclude patterns in file .Rbuildignore in the top-level source
directory.  These patterns should be Perl-like regular expressions (see
the help for regexp in R for the precise details), one per
line, to be matched case-insensitively42 against the file and directory names relative to the
top-level package source directory.  In addition, directories from
source control systems43 or from eclipse44, directories with names ending .Rcheck or
Old or old and files GNUMakefile45, Read-and-delete-me
or with base names starting with ‘.#’, or starting and ending with
‘#’, or ending in ‘~’, ‘.bak’ or ‘.swp’, are
excluded by default.  In addition, those files in the R,
demo and man directories which are flagged by R
CMD check as having invalid names will be excluded.
 Use R CMD build --help to obtain more information about the usage
of the R package builder.
 Unless R CMD build is invoked with the
--no-build-vignettes option (or the package’s
DESCRIPTION contains ‘BuildVignettes: no’ or similar), it
will attempt to (re)build the vignettes (see Writing package vignettes) in the package.  To do so it installs the current package
into a temporary library tree, but any dependent packages need to be
installed in an available library tree (see the Note: at the top of this
section).
 Similarly, if the .Rd documentation files contain any
\Sexpr macros (see Dynamic pages), the package will be
temporarily installed to execute them.  Post-execution binary copies of
those pages containing build-time macros will be saved in
build/partial.rdb.  If there are any install-time or render-time
macros, a .pdf version of the package manual will be built and
installed in the build subdirectory.  (This allows
CRAN or other repositories to display the manual even if they
are unable to install the package.)  This can be suppressed by the
option --no-manual or if package’s DESCRIPTION contains
‘BuildManual: no’ or similar.
 One of the checks that R CMD build runs is for empty source
directories.  These are in most (but not all) cases unintentional, if
they are intentional use the option --keep-empty-dirs (or set
the environment variable _R_BUILD_KEEP_EMPTY_DIRS_ to ‘TRUE’,
or have a ‘BuildKeepEmpty’ field with a true value in the
DESCRIPTION file).
 The --resave-data option allows saved images (.rda and
.RData files) in the data directory to be optimized for
size.  It will also compress tabular files and convert .R files
to saved images.  It can take values no, gzip (the default
if this option is not supplied, which can be changed by setting the
environment variable _R_BUILD_RESAVE_DATA_) and best
(equivalent to giving it without a value), which chooses the most
effective compression.  Using best adds a dependence on R
(>= 2.10) to the DESCRIPTION file if bzip2 or
xz compression is selected for any of the files.  If this is
thought undesirable, --resave-data=gzip (which is the default
if that option is not supplied) will do what compression it can with
gzip.  A package can control how its data is resaved by
supplying a ‘BuildResaveData’ field (with one of the values given
earlier in this paragraph) in its DESCRIPTION file.
 The --compact-vignettes option will run
tools::compactPDF over the PDF files in inst/doc (and its
subdirectories) to losslessly compress them.  This is not enabled by
default (it can be selected by environment variable
_R_BUILD_COMPACT_VIGNETTES_) and needs qpdf
(http://qpdf.sourceforge.net/) to be available.
 It can be useful to run R CMD check --check-subdirs=yes on the
built tarball as a final check on the contents.
 Where a non-POSIX file system is in use which does not utilize execute
permissions, some care is needed with permissions.  This applies on
Windows and to e.g. FAT-formatted drives and SMB-mounted file systems
on other OSes.  The ‘mode’ of the file recorded in the tarball will be
whatever file.info() returns.  On Windows this will record only
directories as having execute permission and on other OSes it is likely
that all files have reported ‘mode’ 0777.  A particular issue is
packages being built on Windows which are intended to contain executable
scripts such as configure and cleanup: R CMD
build ensures those two are recorded with execute permission.
 Directory build of the package sources is reserved for use by
R CMD build: it contains information which may not easily be
created when the package is installed, including index information on
the vignettes and, rarely, information on the help pages and perhaps a
copy of the PDF reference manual (see above).
 
Previous: Building package tarballs, Up: Checking and building packages   [Contents][Index] Binary packages are compressed copies of installed versions of
packages.  They contain compiled shared libraries rather than C, C++ or
Fortran source code, and the R functions are included in their installed
form.  The format and filename are platform-specific; for example, a
binary package for Windows is usually supplied as a .zip file,
and for the OS X platform the default binary package file extension is
.tgz.
 The recommended method of building binary packages is to use
 R CMD INSTALL --build pkg
where pkg is either the name of a source tarball (in the usual
.tar.gz format) or the location of the directory of the package
source to be built.  This operates by first installing the package and
then packing the installed binaries into the appropriate binary package
file for the particular platform.
 By default, R CMD INSTALL --build will attempt to install the
package into the default library tree for the local installation of
R. This has two implications:
 To prevent changes to the present working installation or to provide an
install location with write access, create a suitably located directory
with write access and use the -l option to build the package
in the chosen location.  The usage is then
 R CMD INSTALL -l location --build pkg
 where location is the chosen directory with write access. The package
will be installed as a subdirectory of location, and the package binary
will be created in the current directory.
 Other options for R CMD INSTALL can be found using R
CMD INSTALL --help, and platform-specific details for special cases are
discussed in the platform-specific FAQs.
 Finally, at least one web-based service is available for building binary
packages from (checked) source code: WinBuilder (see
http://win-builder.R-project.org/) is able to build Windows
binaries. Note that this is intended for developers on other platforms
who do not have access to Windows but wish to provide binaries for the
Windows platform.
 
Next: Package namespaces, Previous: Checking and building packages, Up: Creating R packages   [Contents][Index] In addition to the help files in Rd format, R packages allow
the inclusion of documents in arbitrary other formats.  The standard
location for these is subdirectory inst/doc of a source package,
the contents will be copied to subdirectory doc when the package
is installed.  Pointers from package help indices to the installed
documents are automatically created.  Documents in inst/doc can
be in arbitrary format, however we strongly recommend providing them in
PDF format, so users on almost all platforms can easily read them.  To
ensure that they can be accessed from a browser (as an HTML index is
provided), the file names should start with an ASCII letter
and be comprised entirely of ASCII letters or digits or hyphen
or underscore.
 A special case is package vignettes.  Vignettes are documents in
PDF or HTML format obtained from plain text literate source files
from which R knows how to extract R code and create output (in
PDF/HTML or intermediate (La)TeX).  Vignette engines do this work,
using “tangle” and “weave” functions respectively.  Sweave, provided
by the R distribution, is the default engine.  Since R version 3.0.0,
other vignette engines besides Sweave are supported; see Non-Sweave vignettes.
 Package vignettes have their sources in subdirectory vignettes of
the package sources.  Note that the location of the vignette sources
only affects R CMD build and R CMD check: the
tarball built by R CMD build includes in inst/doc the
components intended to be installed.
 Sweave vignette sources are normally given the file extension
.Rnw or .Rtex, but for historical reasons
extensions46 .Snw and
.Stex are also recognized.  Sweave allows the integration of
LaTeX documents: see the Sweave help page in R and the
Sweave vignette in package utils for details on the
source document format.
 Package vignettes are tested by R CMD check by executing all R
code chunks they contain (except those marked for non-evaluation, e.g.,
with option eval=FALSE for Sweave).  The R working directory
for all vignette tests in R CMD check is a copy of the
vignette source directory.  Make sure all files needed to run the R
code in the vignette (data sets, …) are accessible by either
placing them in the inst/doc hierarchy of the source package or
by using calls to system.file().  All other files needed to
re-make the vignettes (such as LaTeX style files, BibTeX input
files and files for any figures not created by running the code in the
vignette) must be in the vignette source directory.
 R CMD build will automatically47 create the
(PDF or HTML versions of the) vignettes in inst/doc for
distribution with the package sources.  By including the vignette
outputs in the package sources it is not necessary that these can be
re-built at install time, i.e., the package author can use private R
packages, screen snapshots and LaTeX extensions which are only
available on his machine.48
 By default R CMD build will run Sweave on all Sweave
vignette source files in vignettes.  If Makefile is found
in the vignette source directory, then R CMD build will try to
run make after the Sweave runs, otherwise
texi2pdf is run on each .tex file produced.
 The first target in the Makefile should take care of both
creation of PDF/HTML files and cleaning up afterwards (including
after Sweave), i.e., delete all files that shall not appear in
the final package archive.  Note that if the make step runs R
it needs to be careful to respect the environment values of R_LIBS
and R_HOME49.
Finally, if there is a Makefile and it has a ‘clean:’
target, make clean is run.
 All the usual caveats about including a Makefile apply.
It must be portable (no GNU extensions), use LF line endings
and must work correctly with a parallel make: too many authors
have written things like
 which will start removing the source files whilst pdflatex is
working.
 Metadata lines can be placed in the source file, preferably in LaTeX
comments in the preamble.  One such is a \VignetteIndexEntry of
the form
 Others you may see are \VignettePackage (currently ignored),
\VignetteDepends and \VignetteKeyword (which replaced
\VignetteKeywords).  These are processed at package installation
time to create the saved data frame Meta/vignette.rds, but only
the \VignetteIndexEntry and \VignetteKeyword statements
are currently used.  The \VignetteEngine statement
is described in Non-Sweave vignettes.
 At install time an HTML index for all vignettes in the package is
automatically created from the \VignetteIndexEntry statements
unless a file index.html exists in directory
inst/doc. This index is linked from the HTML help index for
the package.  If you do supply a inst/doc/index.html file it
should contain relative links only to files under the installed
doc directory, or perhaps (not really an index) to HTML help
files or to the DESCRIPTION file, and be valid HTML as
confirmed via the W3C Markup Validation
Service or Validator.nu.
 Sweave/Stangle allows the document to specify the split=TRUE
option to create a single R file for each code chunk: this will not
work for vignettes where it is assumed that each vignette source
generates a single file with the vignette extension replaced by
.R.
 Do watch that PDFs are not too large – one in a CRAN package
was 72MB!  This is usually caused by the inclusion of overly detailed
figures, which will not render well in PDF viewers.  Sometimes it is
much better to generate fairly high resolution bitmap (PNG, JPEG)
figures and include those in the PDF document.
 When R CMD build builds the vignettes, it copies these and
the vignette sources from directory vignettes to inst/doc.
To install any other files from the vignettes directory, include
a file vignettes/.install_extras which specifies these as
Perl-like regular expressions on one or more lines.  (See the
description of the .Rinstignore file for full details.)
 
Next: Non-Sweave vignettes, Previous: Writing package vignettes, Up: Writing package vignettes   [Contents][Index] Vignettes will in general include descriptive text, R input, R
output and figures, LaTeX include files and bibliographic references.
As any of these may contain non-ASCII characters, the handling
of encodings can become very complicated.
 The vignette source file should be written in ASCII or contain
a declaration of the encoding (see below).  This applies even to
comments within the source file, since vignette engines process comments
to look for options and metadata lines.  When an engine’s weave and
tangle functions are called on the vignette source, it will be converted
to the encoding of the current R session.
 Stangle() will produce an R code file in the current locale’s
encoding: for a non-ASCII vignette what that is recorded in a
comment at the top of the file.
 Sweave() will produce a .tex file in the current
encoding, or in UTF-8 if that is declared.  Non-ASCII encodings
need to be declared to LaTeX via a line like
 (It is also possible to use the more recent ‘inputenx’ LaTeX
package.) For files where this line is not needed (e.g. chapters
included within the body of a larger document, or non-Sweave
vignettes), the encoding may be declared using a comment like
 If the encoding is UTF-8, this can also be declared using
the declaration
 If no declaration is given in the vignette, it will be assumed to be
in the encoding declared for the package.  If there is no encoding
declared in either place, then it is an error to use non-ASCII
characters in the vignette.
 In any case, be aware that LaTeX may require the ‘usepackage’
declaration.
 Sweave() will also parse and evaluate the R code in each
chunk.  The R output will also be in the current locale (or UTF-8
if so declared), and should
be covered by the ‘inputenc’ declaration.  One thing people often
forget is that the R output may not be ASCII even for
ASCII R sources, for many possible reasons.  One common one
is the use of ‘fancy’ quotes: see the R help on sQuote: note
carefully that it is not portable to declare UTF-8 or CP1252 to cover
such quotes, as their encoding will depend on the locale used to run
Sweave():  this can be circumvented by setting
options(useFancyQuotes="UTF-8") in the vignette.
 The final issue is the encoding of figures – this applies only to PDF
figures and not PNG etc.  The PDF figures will contain declarations for
their encoding, but the Sweave option pdf.encoding may need to be
set appropriately: see the help for the pdf() graphics device.
 As a real example of the complexities, consider the fortunes
package version ‘1.4-0’.  That package did not have a declared
encoding, and its vignette was in ASCII.  However, the data it
displays are read from a UTF-8 CSV file and will be assumed to be in the
current encoding, so fortunes.tex will be in UTF-8 in any locale.
Had read.table been told the data were UTF-8, fortunes.tex
would have been in the locale’s encoding.
 
Previous: Encodings and vignettes, Up: Writing package vignettes   [Contents][Index] R 3.0.0 and later allow vignettes in formats other than Sweave by
means of “vignette engines”.  For example knitr version 1.1
or later can create .tex files from a variation on Sweave format,
and .html files from a variation on “markdown” format. These
engines replace the Sweave() function with other functions to
convert vignette source files into LaTeX files for processing into
.pdf, or directly into .pdf or .html files. The
Stangle() function is replaced with a function that extracts the
R source from a vignette.
 R recognizes non-Sweave vignettes using filename extensions specified
by the engine.  For example, the knitr package supports
the extension .Rmd (standing for
“R markdown”).  The user indicates the vignette engine
within the vignette source using a \VignetteEngine line, for example
 This specifies the name of a package and an engine to use in place of
Sweave in processing the vignette.  As Sweave is the only engine
supplied with the R distribution, the package providing any other
engine must be specified in the ‘VignetteBuilder’ field of the
package DESCRIPTION file, and also specified in the
‘Suggests’, ‘Imports’ or ‘Depends’ field (since its
namespace must be available to build or check your package).  If more
than one package is specified as a builder, they will be searched in the
order given there.  The utils package is always implicitly
appended to the list of builder packages, but may be included earlier
to change the search order.
 Note that a package with non-Sweave vignettes should always have a
‘VignetteBuilder’ field in the DESCRIPTION file, since this
is how R CMD check recognizes that there are vignettes to be
checked: packages listed there are required when the package is checked.
 The vignette engine can produce .tex, .pdf, or .html
files as output.  If it produces .tex files, R will
call texi2pdf to convert them to .pdf for display
to the user (unless there is a Makefile in the vignettes
directory).
 Package writers who would like to supply vignette engines need
to register those engines in the package .onLoad function.
For example, that function could make the call
 (The actual registration in knitr is more complicated, because
it supports other input formats.)  See the ?tools::vignetteEngine
help topic for details on engine registration.
 
Next: Writing portable packages, Previous: Writing package vignettes, Up: Creating R packages   [Contents][Index] R has a namespace management system for code in packages.  This
system allows the package writer to specify which variables in the
package should be exported to make them available to package
users, and which variables should be imported from other
packages.
 The namespace for a package is specified by the
NAMESPACE file in the top level package directory.  This file
contains namespace directives describing the imports and exports
of the namespace.  Additional directives register any shared objects to
be loaded and any S3-style methods that are provided.  Note that
although the file looks like R code (and often has R-style
comments) it is not processed as R code.  Only very simple
conditional processing of if statements is implemented.
 Packages are loaded and attached to the search path by calling
library or require.  Only the exported variables are
placed in the attached frame.  Loading a package that imports variables
from other packages will cause these other packages to be loaded as well
(unless they have already been loaded), but they will not be
placed on the search path by these implicit loads.  Thus code in the
package can only depend on objects in its own namespace and its imports
(including the base namespace) being visible50.
 Namespaces are sealed once they are loaded.  Sealing means that
imports and exports cannot be changed and that internal variable
bindings cannot be changed.  Sealing allows a simpler implementation
strategy for the namespace mechanism.  Sealing also allows code
analysis and compilation tools to accurately identify the definition
corresponding to a global variable reference in a function body.
 The namespace controls the search strategy for variables used by
functions in the package.  If not found locally, R searches the
package namespace first, then the imports, then the base namespace and
then the normal search path.
 Prior to R 2.14.0, namespaces were optional in packages: a default
namespace was generated on installation in 2.14.x and 2.15.x.  As from
3.0.0 a namespace is mandatory.
 
Next: Registering S3 methods, Previous: Package namespaces, Up: Package namespaces   [Contents][Index] Exports are specified using the export directive in the
NAMESPACE file.  A directive of the form
 specifies that the variables f and g are to be exported.
(Note that variable names may be quoted, and reserved words and
non-standard names such as [<-.fractions must be.)
 For packages with many variables to export it may be more convenient to
specify the names to export with a regular expression using
exportPattern.  The directive
 exports all variables that do not start with a period.  However, such
broad patterns are not recommended for production code: it is better to
list all exports or use narrowly-defined groups.  (This pattern applies
to S4 classes.)  Beware of patterns which include names starting with a
period: some of these are internal-only variables and should never be
exported, e.g. ‘.__S3MethodsTable__.’ (and the code nowadays
excludes known cases).
 Packages implicitly import the base namespace.
Variables exported from other packages with namespaces need to be
imported explicitly using the directives import and
importFrom.  The import directive imports all exported
variables from the specified package(s).  Thus the directives
 specifies that all exported variables in the packages foo and
bar are to be imported.  If only some of the exported variables
from a package are needed, then they can be imported using
importFrom.  The directive
 specifies that the exported variables f and g of the
package foo are to be imported.  Using importFrom
selectively rather than import is good practice and recommended
notably when importing from packages with more than a dozen exports.
 To import every symbol from a package but for a few exceptions,
pass the except argument to import. The directive
 imports every symbol from foo except bar and
baz. The value of except should evaluate to something
coercible to a character vector, after substituting each symbol for
its corresponding string.
 It is possible to export variables from a namespace which it has
imported from other namespaces: this has to be done explicitly and not
via exportPattern.
 If a package only needs a few objects from another package it can use a
fully qualified variable reference in the code instead of a formal
import.  A fully qualified reference to the function f in package
foo is of the form foo::f.  This is slightly less efficient
than a formal import and also loses the advantage of recording all
dependencies in the NAMESPACE file (but they still need to be
recorded in the DESCRIPTION file).  Evaluating foo::f will
cause package foo to be loaded, but not attached, if it was not
loaded already—this can be an advantage in delaying the loading of a
rarely used package.
 Using foo:::f instead of foo::f allows access to
unexported objects.  This is generally not recommended, as the
semantics of unexported objects may be changed by the package author
in routine maintenance.
 
Next: Load hooks, Previous: Specifying imports and exports, Up: Package namespaces   [Contents][Index] The standard method for S3-style UseMethod dispatching might fail
to locate methods defined in a package that is imported but not attached
to the search path.  To ensure that these methods are available the
packages defining the methods should ensure that the generics are
imported and register the methods using S3method directives.  If
a package defines a function print.foo intended to be used as a
print method for class foo, then the directive
 ensures that the method is registered and available for UseMethod
dispatch, and the function print.foo does not need to be exported.
Since the generic print is defined in base it does not need
to be imported explicitly.
 (Note that function and class names may be quoted, and reserved words
and non-standard names such as [<- and function must
be.)
 It is possible to specify a third argument to S3method, the function to
be used as the method, for example
 when print.check_so_symbols is not needed.
 There used to be a limit on the number of S3method directives: it
was 500 prior to R 3.0.2.
 
Next: useDynLib, Previous: Registering S3 methods, Up: Package namespaces   [Contents][Index] There are a number of hooks called as packages are loaded, attached,
detached, and unloaded.  See help(".onLoad") for more details.
 Since loading and attaching are distinct operations, separate hooks are
provided for each.  These hook functions are called .onLoad and
.onAttach.  They both take arguments51 libname and
pkgname; they should be defined in the namespace but not
exported.
 Packages can use a .onDetach (as from R 3.0.0) or .Last.lib
function (provided the latter is exported from the namespace) when
detach is called on the package.  It is called with a single
argument, the full path to the installed package.  There is also a hook
.onUnload which is called when the namespace is unloaded
(via a call to unloadNamespace, perhaps called by
detach(unload = TRUE)) with argument the full path to the installed
package’s directory.  .onUnload and .onDetach should be
defined in the namespace and not exported, but .Last.lib does
need to be exported.
 Packages are not likely to need .onAttach (except perhaps for a
start-up banner); code to set options and load shared objects should be
placed in a .onLoad function, or use made of the useDynLib
directive described next.
 User-level hooks are also available: see the help on function
setHook.
 These hooks are often used incorrectly.  People forget to export
.Last.lib.   Compiled code should be loaded in .onLoad (or
via a useDynLb directive: see below) and unloaded in
.onUnload.  Do remember that a package’s namespace can be loaded
without the namespace being attached (e.g. by pkgname::fun) and
that a package can be detached and re-attached whilst its namespace
remains loaded.
 
Next: An example, Previous: Load hooks, Up: Package namespaces   [Contents][Index] A NAMESPACE file can contain one or more useDynLib
directives which allows shared objects that need to be
loaded.52  The directive
 registers the shared object foo53 for loading with library.dynam.
Loading of registered object(s) occurs after the package code has been
loaded and before running the load hook function.  Packages that would
only need a load hook function to load a shared object can use the
useDynLib directive instead.
 The useDynLib directive also accepts the names of the native
routines that are to be used in R via the .C, .Call,
.Fortran and .External interface functions.  These are given as
additional arguments to the directive, for example,
 By specifying these names in the useDynLib directive, the native
symbols are resolved when the package is loaded and R variables
identifying these symbols are added to the package’s namespace with
these names.  These can be used in the .C, .Call,
.Fortran and .External calls in place of the name of the
routine and the PACKAGE argument.  For instance, we can call the
routine myRoutine from R with the code
 rather than
 There are at least two benefits to this approach.  Firstly, the symbol
lookup is done just once for each symbol rather than each time the
routine is invoked.  Secondly, this removes any ambiguity in resolving
symbols that might be present in several compiled DLLs.
 In some circumstances, there will already be an R variable in the
package with the same name as a native symbol. For example, we may have
an R function in the package named myRoutine.  In this case,
it is necessary to map the native symbol to a different R variable
name. This can be done in the useDynLib directive by using named
arguments. For instance, to map the native symbol name myRoutine
to the R variable myRoutine_sym, we would use
 We could then call that routine from R using the command
 Symbols without explicit names are assigned to the R variable with
that name.
 In some cases, it may be preferable not to create R variables in the
package’s namespace that identify the native routines.  It may be too
costly to compute these for many routines when the package is loaded
if many of these routines are not likely to be used.  In this case,
one can still perform the symbol resolution correctly using the DLL,
but do this each time the routine is called.  Given a reference to the
DLL as an R variable, say dll, we can call the routine
myRoutine using the expression
 The $ operator resolves the routine with the given name in the
DLL using a call to getNativeSymbol.  This is the same
computation as above where we resolve the symbol when the package is
loaded. The only difference is that this is done each time in the case
of dll$myRoutine.
 In order to use this dynamic approach (e.g., dll$myRoutine), one
needs the reference to the DLL as an R variable in the package.  The
DLL can be assigned to a variable by using the variable =
dllName format used above for mapping symbols to R variables.  For
example, if we wanted to assign the DLL reference for the DLL
foo in the example above to the variable myDLL, we would
use the following directive in the NAMESPACE file:
 Then, the R variable myDLL is in the package’s namespace and
available for calls such as myDLL$dynRoutine to access routines
that are not explicitly resolved at load time.
 If the package has registration information (see Registering native routines), then we can use that directly rather than specifying the
list of symbols again in the useDynLib directive in the
NAMESPACE file.  Each routine in the registration information is
specified by giving a name by which the routine is to be specified along
with the address of the routine and any information about the number and
type of the parameters.  Using the .registration argument of
useDynLib, we can instruct the namespace mechanism to create
R variables for these symbols.  For example, suppose we have the
following registration information for a DLL named myDLL:
 Then, the directive in the NAMESPACE file
 causes the DLL to be loaded and also for the R variables foo,
bar_sym, R_call_sym and R_version_sym to be
defined in the package’s namespace.
 Note that the names for the R variables are taken from the entry in
the registration information and do not need to be the same as the name
of the native routine.  This allows the creator of the registration
information to map the native symbols to non-conflicting variable names
in R, e.g. R_version to R_version_sym for use in an
R function such as
 Using argument .fixes allows an automatic prefix to be added to
the registered symbols, which can be useful when working with an
existing package.  For example, package KernSmooth has
 which makes the R variables corresponding to the FORTRAN symbols
F_bkde and so on, and so avoid clashes with R code in the
namespace.
 
Next: Namespaces with S4 classes and methods, Previous: useDynLib, Up: Package namespaces   [Contents][Index] As an example consider two packages named foo and bar.  The
R code for package foo in file foo.R is
 Some C code defines a C function compiled into DLL foo (with an
appropriate extension).  The NAMESPACE file for this package is
 The second package bar has code file bar.R
 and NAMESPACE file
 Calling library(bar) loads bar and attaches its exports to
the search path.  Package foo is also loaded but not attached to
the search path.  A call to g produces
 This is consistent with the definitions of c in the two settings:
in bar the function c is defined to be equivalent to
sum, but in foo the variable c refers to the
standard function c in base.
 
Previous: An example, Up: Package namespaces   [Contents][Index] Some additional steps are needed for packages which make use of formal
(S4-style) classes and methods (unless these are purely used
internally).  The package should have Depends: methods in its
DESCRIPTION file54 and import(methods) or
importFrom(methods, ...) plus any classes and methods which are
to be exported need to be declared in the NAMESPACE file.  For
example, the stats4 package has
 All S4 classes to be used outside the package need to be listed in an
exportClasses directive. Alternatively, they can be specified
using exportClassPattern55 in the same style as
for exportPattern.  To export methods for generics from other
packages an exportMethods directive can be used.
 Note that exporting methods on a generic in the namespace will also
export the generic, and exporting a generic in the namespace will also
export its methods.  If the generic function is not local to this
package, either because it was imported as a generic function or because
the non-generic version has been made generic solely to add S4 methods
to it (as for functions such as plot in the example above), it
can be declared via either or both of export or
exportMethods, but the latter is clearer (and is used in the
stats4 example above).  In particular, for primitive functions
there is no generic function, so export would export the
primitive, which makes no sense.  On the other hand, if the generic is
local to this package, it is more natural to export the function itself
using export(), and this must be done if an implicit
generic is created without setting any methods for it (as is the case
for AIC in stats4).
 A non-local generic function is only exported to ensure that calls to
the function will dispatch the methods from this package (and that is
not done or required when the methods are for primitive functions).  For
this reason, you do not need to document such implicitly created generic
functions, and undoc in package tools will not report them.
 If a package uses S4 classes and methods exported from another package,
but does not import the entire namespace of the other
package56, it needs
to import the classes and methods explicitly, with directives
 listing the classes and functions with methods respectively.  Suppose we
had two small packages A and B with B using A.
Then they could have NAMESPACE files
 and
 respectively.
 Note that importMethodsFrom will also import any generics defined
in the namespace on those methods.
 It is important if you export S4 methods that the corresponding generics
are available.  You may for example need to import plot from
graphics to make visible a function to be converted into its
implicit generic.  But it is better practice to make use of the generics
exported by stats4 as this enables multiple packages to
unambiguously set methods on those generics.
 
Next: Diagnostic messages, Previous: Package namespaces, Up: Creating R packages   [Contents][Index] This section contains advice on writing packages to be used on multiple
platforms or for distribution (for example to be submitted to a package
repository such as CRAN).
 Portable packages should have simple file names: use only alphanumeric
ASCII characters and period (.), and avoid those names
not allowed under Windows which are mentioned above.
 Many of the graphics devices are platform-specific: even X11()
(aka x11()) which although emulated on Windows may not be
available on a Unix-alike (and is not the preferred screen device on OS
X).  It is rarely necessary for package code or examples to open a new
device, but if essential,57 use dev.new().
 Use R CMD build to make the release .tar.gz file.
 R CMD check provides a basic set of checks, but often further
problems emerge when people try to install and use packages submitted to
CRAN – many of these involve compiled code.  Here are some
further checks that you can do to make your package more portable.
 The use of ${shell ...} can be avoided by using backticks, e.g.
 which works in all versions of make known60 to be used with
R.
 If you really must require GNU make, declare it in the DESCRIPTION
file by
 and ensure that you use the value of environment variable MAKE
(and not just make) in your scripts.  (On some platforms GNU
make is available under a name such as gmake, and there
SystemRequirements is used to set MAKE.)
 If you only need GNU make for parts of the package which are rarely
needed (for example to create bibliography files under
vignettes), use a file called GNUmakefile rather than
Makefile as GNU make (only) will use the former.
 Since the only viable make for Windows is GNU make, it is permissible to
use GNU extensions in files Makevars.win or Makefile.win.
 Although there is a 2011 version of the C++ standard, even partial
implementations are not universally available.  Portable C++ code needs
to follow the 1998 standard (and not use features from C99).  See also
Using C++11 code to specify a C++11 compiler.
 If you use FORTRAN 77, ftnchek
(http://www.dsm.fordham.edu/~ftnchek/) provides thorough testing
of conformance to the standard.
 Not all common R platforms conform to the expected standards, e.g.
C99 for C code.  One common area of problems is the *printf
functions where Windows does not support %lld, %Lf and
similar formats (and has its own formats such as %I64d for 64-bit
integers).  It is very rare to need to output such types, and 64-bit
integers can usually be converted to doubles for output.
 It is not safe to assume that long and pointer types are the same
size, and they are not on 64-bit Windows.  If you need to convert
pointers to and from integers use the C99 integer types intptr_t
and uintptr_t (which are defined in the header <stdint.h>
and are not required to be implemented by the C99 standard).
 Note that integer in FORTRAN corresponds to int
in C on all R platforms.
 This applies not only to your own code but to any external software you
compile in or link to.
 Nor should the C++11 random number library be used.
 and checking if any of the symbols marked U is unexpected is a
good way to avoid this.
 Do be careful in what your tests (and examples) actually test.  Bad
practice seen in distributed packages include:
 Worse, packages have tested the exact format of system error messages,
which are platform-dependent and perhaps locale-dependent.
 If you must try to establish a tolerance empirically, configure and
build R with --disable-long-double and use appropriate
compiler flags (such as -ffloat-store and
-fexcess-precision=standard for gcc, depending on the
CPU type70) to
mitigate the effects of extended-precision calculations.
 Tests which involve random inputs or non-deterministic algorithms should
normally set a seed or be tested for many seeds.
 
Next: Check timing, Previous: Writing portable packages, Up: Writing portable packages   [Contents][Index] There are a several tools available to reduce the size of PDF files:
often the size can be reduced substantially with no or minimal loss in
quality. Not only do large files take up space: they can stress the PDF
viewer and take many minutes to print (if they can be printed at all).
 qpdf (http://qpdf.sourceforge.net/) can compress
losslessly.  It is fairly readily available (e.g. it has binaries for
Windows and packages in Debian/Ubuntu/Fedora, and is installed as part
of the CRAN OS X distribution of R).  R CMD build
has an option to run qpdf over PDF files under inst/doc
and replace them if at least 10Kb and 10% is saved.  The full path to
the qpdf command can be supplied as environment variable
R_QPDF (and is on the CRAN binary of R for OS X).  It seems
MiKTeX does not use PDF object compression and so qpdf can
reduce considerably the files it outputs: MiKTeX can be overridden by
code in the preamble of an Sweave or LaTeX file — see how this is
done for the R reference manual at
https://svn.r-project.org/R/trunk/doc/manual/refman.top.
 Other tools can reduce the size of PDFs containing bitmap images at
excessively high resolution.  These are often best re-generated (for
example Sweave defaults to 300 ppi, and 100–150 is more
appropriate for a package manual).  These tools include Adobe Acrobat
(not Reader), Apple’s Preview71  and Ghostscript (which
converts PDF to PDF by
 and suitable options might be
 ; see http://www.ghostscript.com/doc/current/Ps2pdf.htm for
more such and consider all the options for image downsampling).  There
have been examples in CRAN packages for which Ghostscript 9.06
and later produced much better reductions than 9.05 or earlier.
 We come across occasionally large PDF files containing excessively
complicated figures using PDF vector graphics: such figures are often
best redesigned or failing that, output as PNG files.
 Option --compact-vignettes to R CMD build defaults to
value ‘qpdf’: use ‘both’ to try harder to reduce the size,
provided you have Ghostscript available (see the help for
tools::compactPDF).
 
Next: Encoding issues, Previous: PDF size, Up: Writing portable packages   [Contents][Index] There are several ways to find out where time is being spent in the
check process.  Start by setting the environment variable
_R_CHECK_TIMINGS_ to ‘0’.  This will report the total CPU
times (not Windows) and elapsed times for installation and running
examples, tests and vignettes, under each sub-architecture if
appropriate.  For tests and vignettes, it reports the time for each as
well as the total.
 Setting _R_CHECK_TIMINGS_ to a positive value sets a threshold (in
seconds elapsed time) for reporting timings.
 If you need to look in more detail at the timings for examples, use
option --timings to R CMD check (this is set by
--as-cran).  This adds a summary to the check output for all
the examples with CPU or elapsed time of more than 5 seconds.  It
produces a file mypkg.Rcheck/mypkg-Ex.timings
containing timings for each help file: it is a tab-delimited file which
can be read into R for further analysis.
 Timings for the tests and vignette runs are given at the bottom of the
corresponding log file: note that log files for successful vignette runs
are only retained if environment variable
_R_CHECK_ALWAYS_LOG_VIGNETTE_OUTPUT_ is set to a true value.
 
Next: Portable C and C++ code, Previous: Check timing, Up: Writing portable packages   [Contents][Index] Care is needed if your package contains non-ASCII text, and in
particular if it is intended to be used in more than one locale.  It is
possible to mark the encoding used in the DESCRIPTION file and in
.Rd files, as discussed elsewhere in this manual.
 First, consider carefully if you really need non-ASCII text.
Many users of R will only be able to view correctly text in their
native language group (e.g. Western European, Eastern European,
Simplified Chinese) and ASCII.72.  Other characters may not be rendered at all,
rendered incorrectly, or cause your R code to give an error.  For
.Rd documentation, marking the encoding and including
ASCII transliterations is likely to do a reasonable job.  The
set of characters which is commonly supported is wider than it used to
be around 2000, but non-Latin alphabets (Greek, Russian, Georgian,
…) are still often problematic and those with double-width
characters (Chinese, Japanese, Korean) often need specialist fonts to
render correctly.
 Several CRAN packages have messages in their R code in French (and a
few in German).  A better way to tackle this is to use the
internationalization facilities discussed elsewhere in this manual.
 Function showNonASCIIfile in package tools can help in
finding non-ASCII bytes in files.
 There is a portable way to have arbitrary text in character strings
(only) in your R code, which is to supply them in Unicode as
\uxxxx escapes.  If there are any characters not in the current
encoding the parser will encode the character string as UTF-8 and mark
it as such.  This applies also to character strings in datasets: they
can be prepared using \uxxxx escapes or encoded in UTF-8 in a
UTF-8 locale, or even converted to UTF-8 via ‘iconv()’.  If you do
this, make sure you have ‘R (>= 2.10)’ (or later) in the
‘Depends’ field of the DESCRIPTION file.
 R sessions running in non-UTF-8 locales will if possible re-encode
such strings for display (and this is done by RGui on Windows,
for example).  Suitable fonts will need to be selected or made
available73  both for the console/terminal and graphics devices such as
‘X11()’ and ‘windows()’.  Using ‘postscript’ or
‘pdf’ will choose a default 8-bit encoding depending on the
language of the UTF-8 locale, and your users would need to be told how
to select the ‘encoding’ argument.
 If you want to run R CMD check on a Unix-alike over a package
that sets a package encoding in its DESCRIPTION file and do
not use a UTF-8 locale you may need to specify a suitable locale
via environment variable R_ENCODING_LOCALES.  The default
is equivalent to the value
 (which is appropriate for a system based on glibc: OS X requires
latin9=fr_FR.ISO8859-15) except that if the current locale is
UTF-8 then the package code is translated to UTF-8 for syntax checking,
so it is strongly recommended to check in a UTF-8 locale.
 
Next: Binary distribution, Previous: Encoding issues, Up: Writing portable packages   [Contents][Index] Writing portable C and C++ code is mainly a matter of observing the
standards (C99, C++98 or where declared C++11) and testing that
extensions (such as POSIX functions) are supported.  However, some
common errors are worth pointing out here.  It can be helpful to look up
functions at http://www.cplusplus.com/reference/ or
http://en.cppreference.com/w/ and compare what is defined in the
various standards.
 Both the compiler and OS (via system header files, which differ
by architecture even for nominally the same OS) affect the compilability
of C/C++ code.  Compilers from the GCC, clang, Intel and
Solaris Studio suites are routinely used with R, and both
clang and Solaris have more than one implementation of C++
headers and library.  The range of possibilities makes comprehensive
empirical checking impossible, and regrettably compilers are patchy at
best on warning about non-standard code.
 A not-uncommonly-seen problem is to mistakenly call floor(x/y) or
ceil(x/y) for int arguments x and y.  Since
x/y does integer division, the result is an int and
‘overloading ambiguity’ may be reported.  Some people have (pointlessly)
called floor and ceil on integer arguments, which may have
an ‘overloading ambiguity’.
 A surprising common misuse is things like pow(10, -3): this
should be the constant 1e-3.
 If you must use them in C++11, beware that some
compilers74 provide both
std::isnan and ::isnan, so using
 may cause ‘overloading ambiguity’ and you must use std::isnan
etc explicitly.
 It is an error (and make little sense, although has been seen) to call
these functions for integer arguments.
 Some (but not all) extensions are listed at
https://gcc.gnu.org/onlinedocs/gcc/C-Extensions.html and
https://gcc.gnu.org/onlinedocs/gcc/C_002b_002b-Extensions.html.
 Some additional information for C++ is available at
http://journal.r-project.org/archive/2011-2/RJournal_2011-2_Plummer.pdf
by Martyn Plummer.
 
Previous: Portable C and C++ code, Up: Writing portable packages   [Contents][Index] If you want to distribute a binary version of a package on Windows or OS
X, there are further checks you need to do to check it is portable: it
is all too easy to depend on external software on your own machine that
other users will not have.
 For Windows, check what other DLLs your package’s DLL depends on
(‘imports’ from in the DLL tools’ parlance).  A convenient GUI-based
tool to do so is ‘Dependency Walker’
(http://www.dependencywalker.com/) for both 32-bit and 64-bit
DLLs – note that this will report as missing links to R’s own DLLs
such as R.dll and Rblas.dll.  For 32-bit DLLs only, the
command-line tool pedump.exe -i (in Rtools*.exe) can be
used, and for the brave, the objdump tool in the appropriate
toolchain will also reveal what DLLs are imported from.  If you use a
toolchain other than one provided by the R developers or use your own
makefiles, watch out in particular for dependencies on the toolchain’s
runtime DLLs such as libgfortran, libstdc++ and
libgcc_s.
 For OS X, using R CMD otool -L on the package’s shared object(s)
in the libs directory will show what they depend on: watch for
any dependencies in /usr/local/lib, notably
libgfortran.2.dylib, libgfortran.3.dylib or
libquadmath.0.dylib.
 Many people (including the CRAN package repository) will not
accept source packages containing binary files as the latter are a
security risk.  If you want to distribute a source package which needs
external software on Windows or OS X, options include
 Be aware that license requirements will need to be met so you may need
to supply the sources for the additional components (and will if your
package has a GPL-like license).
 
Next: Internationalization, Previous: Writing portable packages, Up: Creating R packages   [Contents][Index] Diagnostic messages can be made available for translation, so it is
important to write them in a consistent style.  Using the tools
described in the next section to extract all the messages can give a
useful overview of your consistency (or lack of it).
Some guidelines follow.
 In R error messages do not construct a message with paste (such
messages will not be translated) but via multiple arguments to
stop or warning, or via gettextf.
 and double quotation marks when referring to an R character string or
a class, such as
 Since ASCII does not contain directional quotation marks, it
is best to use ‘'’ and let the translator (including automatic
translation) use directional quotations where available.  The range of
quotation styles is immense: unfortunately we cannot reproduce them in a
portable texinfo document.  But as a taster, some languages use
‘up’ and ‘down’ (comma) quotes rather than left or right quotes, and
some use guillemets (and some use what Adobe calls ‘guillemotleft’ to
start and others use it to end).
 In R messages it is also possible to use sQuote or dQuote as in
 and was replaced by
 Note that it is much better to have complete clauses as here, since
in another language one might need to say
‘There is no package in library %s’ or
‘There are no packages in libraries %s’.
 
Next: CITATION files, Previous: Diagnostic messages, Up: Creating R packages   [Contents][Index] There are mechanisms to translate the R- and C-level error and warning
messages.  There are only available if R is compiled with NLS support
(which is requested by configure option --enable-nls,
the default).
 The procedures make use of msgfmt and xgettext which are
part of GNU gettext and this will need to be installed:
Windows users can find pre-compiled binaries at
https://www.stats.ox.ac.uk/pub/Rtools/goodies/gettext-tools.zip.
 
Next: R messages, Previous: Internationalization, Up: Internationalization   [Contents][Index] The process of enabling translations is
 If you want to use different messages for singular and plural forms, you
need to add
 and mark strings by
 The file src/pkg.pot is the template file, and
conventionally this is shipped as po/pkg.pot.
 
Next: Preparing translations, Previous: C-level messages, Up: Internationalization   [Contents][Index] Mechanisms are also available to support the automatic translation of
R stop, warning and message messages.  They make
use of message catalogs in the same way as C-level messages, but using
domain R-pkg rather than pkg.  Translation of
character strings inside stop, warning and message
calls is automatically enabled, as well as other messages enclosed in
calls to gettext or gettextf.  (To suppress this, use
argument domain=NA.)
 Tools to prepare the R-pkg.pot file are provided in package
tools: xgettext2pot will prepare a file from all strings
occurring inside gettext/gettextf, stop,
warning and message calls.  Some of these are likely to be
spurious and so the file is likely to need manual editing.
xgettext extracts the actual calls and so is more useful when
tidying up error messages.
 The R function ngettext provides an interface to the C
function of the same name: see example in the previous section.  It is
safest to use domain="R-pkg" explicitly in calls to
ngettext, and necessary for earlier versions of R unless they
are calls directly from a function in the package.
 
Previous: R messages, Up: Internationalization   [Contents][Index] Once the template files have been created, translations can be made.
Conventional translations have file extension .po and are placed
in the po subdirectory of the package with a name that is either
‘ll.po’ or ‘R-ll.po’ for translations of the C and R
messages respectively to language with code ‘ll’.
 See Localization of messages in R Installation and Administration, for details of language codes.
 There is an R function, update_pkg_po in package tools,
to automate much of the maintenance of message translations.  See its
help for what it does in detail.
 If this is called on a package with no existing translations, it creates
the directory pkgdir/po, creates a template file of R
messages, pkgdir/po/R-pkg.pot, within it, creates the
‘en@quot’ translation and installs that.  (The ‘en@quot’
pseudo-language interprets quotes in their directional forms in suitable
(e.g. UTF-8) locales.)
 If the package has C source files in its src directory
that are marked for translation, use
 to create a dummy template file, then call update_pkg_po again
(this can also be done before it is called for the first time).
 When translations to new languages are added in the pkgdir/po
directory, running the same command will check and then
install the translations.
 If the package sources are updated, the same command will update the
template files, merge the changes into the translation .po files
and then installed the updated translations.  You will often see that
merging marks translations as ‘fuzzy’ and this is reported in the
coverage statistics.  As fuzzy translations are not used, this is
an indication that the translation files need human attention.
 The merged translations are run through tools::checkPofile to
check that C-style formats are used correctly: if not the mismatches are
reported and the broken translations are not installed.
 This function needs the GNU gettext-tools installed and on the
path: see its help page.
 
Next: Package types, Previous: Internationalization, Up: Creating R packages   [Contents][Index] An installed file named CITATION will be used by the
citation() function.  (It should be in the inst
subdirectory of the package sources.)
 The CITATION file is parsed as R code (in the package’s
declared encoding, or in ASCII if none is declared).  If no
such file is present, citation auto-generates citation
information from the package DESCRIPTION metadata, and an example
of what that would look like as a CITATION file can be seen in
recommended package nlme (see below): recommended packages
boot, cluster and mgcv have further
examples.
 A CITATION file will contain calls to function bibentry.
 Here is that for nlme:
 Note the way that information that may need to be updated is picked up
from object meta, a parsed version of the DESCRIPTION
file76 – it is
tempting to hardcode such information, but it normally then gets
outdated.  See ?bibentry for further details of the information
which can be provided.
 In case a bibentry contains LaTeX markup (e.g., for accented
characters or mathematical symbols), it may be necessary to provide a
text representation to be used for printing via the textVersion
argument to bibentry.  E.g., earlier versions of
nlme additionally used
 The CITATION file should itself produce no output when
source-d.
 It is desirable (and essential for CRAN) that the
CITATION file does not contain calls to functions such as
packageDescription which assume the package is installed in a
library tree on the package search path.
 
Next: Services, Previous: CITATION files, Up: Creating R packages   [Contents][Index] The DESCRIPTION file has an optional field Type which if
missing is assumed to be ‘Package’, the sort of extension discussed
so far in this chapter.  Currently one other type is recognized; there
used also to be a ‘Translation’ type.
 
Previous: Package types, Up: Package types   [Contents][Index] This is a rather general mechanism, designed for adding new front-ends
such as the former gnomeGUI package (see the Archive area on
CRAN).  If a configure file is found in the top-level
directory of the package it is executed, and then if a Makefile
is found (often generated by configure), make is called.
If R CMD INSTALL --clean is used make clean is called.  No
other action is taken.
 R CMD build can package up this type of extension, but R
CMD check will check the type and skip it.
 Many packages of this type need write permission for the R
installation directory.
 
Previous: Package types, Up: Creating R packages   [Contents][Index] Several members of the R project have set up services to assist those
writing R packages, particularly those intended for public
distribution.
 win-builder.r-project.org
offers the automated preparation of (32/64-bit) Windows binaries from
well-tested source packages.
 R-Forge (R-Forge.r-project.org) and
RForge (www.rforge.net) are similar
services with similar names.  Both provide source-code management
through SVN, daily building and checking, mailing lists and a repository
that can be accessed via install.packages (they can be
selected by setRepositories and the GUI menus that use it).
Package developers have the opportunity to present their work on the
basis of project websites or news announcements.  Mailing lists, forums
or wikis provide useRs with convenient instruments for discussions and
for exchanging information between developers and/or interested useRs.
 
Next: Tidying and profiling R code, Previous: Creating R packages, Up: Top   [Contents][Index] 
Next: Sectioning, Previous: Writing R documentation files, Up: Writing R documentation files   [Contents][Index] R objects are documented in files written in “R documentation”
(Rd) format, a simple markup language much of which closely resembles
(La)TeX, which can be processed into a variety of formats,
including LaTeX, HTML and plain text.  The translation is
carried out by functions in the tools package called by the
script Rdconv in R_HOME/bin and by the
installation scripts for packages.
 The R distribution contains more than 1300 such files which can be
found in the src/library/pkg/man directories of the R
source tree, where pkg stands for one of the standard packages
which are included in the R distribution.
 As an example, let us look at a simplified version of
src/library/base/man/load.Rd which documents the R function
load.
 An Rd file consists of three parts.  The header gives basic
information about the name of the file, the topics documented, a title,
a short textual description and R usage information for the objects
documented.  The body gives further information (for example, on the
function’s arguments and return value, as in the above example).
Finally, there is an optional footer with keyword information.  The
header is mandatory.
 Information is given within a series of sections with standard
names (and user-defined sections are also allowed).  Unless otherwise
specified77 these should occur only once in an Rd
file (in any order), and the processing software will retain only the
first occurrence of a standard section in the file, with a warning.
 See “Guidelines for Rd
files” for guidelines for writing documentation in Rd format
which should be useful for package writers.

The R
generic function prompt is used to construct a bare-bones Rd
file ready for manual editing.  Methods are defined for documenting
functions (which fill in the proper function and argument names) and
data frames.  There are also functions promptData,
promptPackage, promptClass, and promptMethods for
other types of Rd file.
 The general syntax of Rd files is summarized below.  For a detailed
technical discussion of current Rd syntax, see
“Parsing Rd files”.
 Rd files consist of four types of text input.  The most common
is LaTeX-like, with the backslash used as a prefix on markup
(e.g. \alias), and braces used to indicate arguments
(e.g. {load}).  The least common type of text is ‘verbatim’
text, where no markup other than the comment marker (%) is
processed.  There is also a rare variant of ‘verbatim’ text
(used in \eqn, \deqn, \figure,
and \newcommand) where comment markers need not be escaped.
The final type is R-like, intended for R code, but allowing some
embedded macros.  Quoted strings within R-like text are handled
specially: regular character escapes such as \n may be entered
as-is.  Only markup starting with \l (e.g.  \link) or
\v (e.g. \var) will be recognized within quoted strings.
The rarely used vertical tab \v must be entered as \\v.
 Each macro defines the input type for its argument.  For example, the
file initially uses LaTeX-like syntax, and this is also used in the
\description section, but the \usage section uses
R-like syntax, and the \alias macro uses ‘verbatim’ syntax.
Comments run from a percent symbol % to the end of the line in
all types of text except the rare ‘verbatim’ variant
(as on the first line of the load example).
 Because backslashes, braces and percent symbols have special meaning, to
enter them into text sometimes requires escapes using a backslash.  In
general balanced braces do not need to be escaped, but percent symbols
always do, except in the ‘verbatim’ variant.
For the complete list of macros and rules for escapes, see
“Parsing Rd files”.
 
Next: Documenting data sets, Previous: Rd format, Up: Rd format   [Contents][Index] The basic markup commands used for documenting R objects (in
particular, functions) are given in this subsection.
 name typically78 is the basename of
the Rd file containing the documentation.  It is the “name” of
the Rd object represented by the file and has to be unique in a
package.  To avoid problems with indexing the package manual, it may not
contain ‘!’  ‘|’ nor ‘@’, and to avoid possible problems
with the HTML help system it should not contain ‘/’ nor a space.
(LaTeX special characters are allowed, but may not be collated
correctly in the index.)  There can only be one \name entry in a
file, and it must not contain any markup.  Entries in the package manual
will be in alphabetic79 order
of the \name entries.
 The \alias sections specify all “topics” the file documents.
This information is collected into index data bases for lookup by the
on-line (plain text and HTML) help systems.  The topic can
contain spaces, but (for historical reasons) leading and trailing spaces
will be stripped.  Percent and left brace need to be escaped by
a backslash.
 There may be several \alias entries.  Quite often it is
convenient to document several R objects in one file.  For example,
file Normal.Rd documents the density, distribution function,
quantile function and generation of random variates for the normal
distribution, and hence starts with
 Also, it is often convenient to have several different ways to refer to
an R object, and an \alias does not need to be the name of an
object.
 Note that the \name is not necessarily a topic documented, and if
so desired it needs to have an explicit \alias entry (as in this
example).
 Title information for the Rd file.  This should be capitalized
and not end in a period; try to limit its length to at most 65
characters for widest compatibility.
 Markup is supported in the text, but use of characters other than
English text and punctuation (e.g., ‘<’) may limit portability.
 There must be one (and only one) \title section in a help file.
 A short description of what the function(s) do(es) (one paragraph, a few
lines only).  (If a description is too long and cannot easily be
shortened, the file probably tries to document too much at once.)
This is mandatory except for package-overview files.
 One or more lines showing the synopsis of the function(s) and variables
documented in the file.  These are set in typewriter font.  This is an
R-like command.
 The usage information specified should match the function definition
exactly (such that automatic checking for consistency between
code and documentation is possible).
 It is no longer advisable to use \synopsis for the actual
synopsis and show modified synopses in the \usage.  Support for
\synopsis will be removed in \R 3.1.0.  To indicate that a
function can be used in several different ways, depending on the named
arguments specified, use section \details.  E.g.,
abline.Rd contains
 Use \method{generic}{class} to indicate the name
of an S3 method for the generic function generic for objects
inheriting from class "class".  In the printed versions,
this will come out as generic (reflecting the understanding that
methods should not be invoked directly but via method dispatch), but
codoc() and other QC tools always have access to the full name.
 For example, print.ts.Rd contains
 which will print as
 Usage for replacement functions should be given in the style of
dim(x) <- value rather than explicitly indicating the name of the
replacement function ("dim<-" in the above).  Similarly, one
can use \method{generic}{class}(arglist) <-
value to indicate the usage of an S3 replacement method for the generic
replacement function "generic<-" for objects inheriting
from class "class".
 Usage for S3 methods for extracting or replacing parts of an object, S3
methods for members of the Ops group, and S3 methods for user-defined
(binary) infix operators (‘%xxx%’) follows the above rules,
using the appropriate function names.  E.g., Extract.factor.Rd
contains
 which will print as
 \S3method is accepted as an alternative to \method.
 Description of the function’s arguments, using an entry of the form
 for each element of the argument list.  (Note that there is
no whitespace between the three parts of the entry.) There may be
optional text outside the \item entries, for example to give
general information about groups of parameters.
 A detailed if possible precise description of the functionality
provided, extending the basic information in the \description
slot.
 Description of the function’s return value.
 If a list with multiple values is returned, you can use entries of the
form
 for each component of the list returned.  Optional text may
precede80 this
list (see for example the help for rle).  Note that \value
is implicitly a \describe environment, so that environment should
not be used for listing components, just individual \item{}{}
entries.
 A section with references to the literature.  Use \url{} or
\href{}{} for web pointers.
 Use this for a special note you want to have pointed out.  Multiple
\note sections are allowed, but might be confusing to the end users.
 For example, pie.Rd contains
 Information about the author(s) of the Rd file.  Use
\email{} without extra delimiters (such as ‘( )’ or
‘< >’) to specify email addresses, or \url{} or
\href{}{} for web pointers.
 Pointers to related R objects, using \code{\link{...}} to
refer to them (\code is the correct markup for R object names,
and \link produces hyperlinks in output formats which support
this.  See Marking text, and Cross-references).
 Examples of how to use the function.  Code in this section is set
in typewriter font without reformatting and is run by
example() unless marked otherwise (see below).
 Examples are not only useful for documentation purposes, but also
provide test code used for diagnostic checking of R code.  By
default, text inside \examples{} will be displayed in the
output of the help page and run by example() and by R CMD
check.  You can use \dontrun{}

for text that should only be shown, but not run, and
\dontshow{}

for extra commands for testing that should not be shown to users, but
will be run by example().  (Previously this was called
\testonly, and that is still accepted.)
 Text inside \dontrun{} is ‘verbatim’, but the other parts
of the \examples section are R-like text.
 For example,
 Thus, example code not included in \dontrun must be executable!
In addition, it should not use any system-specific features or require
special facilities (such as Internet access or write permission to
specific directories).  Text included in \dontrun is indicated by
comments in the processed help files: it need not be valid R code but
the escapes must still be used for %, \ and unpaired
braces as in other ‘verbatim’ text.
 Example code must be capable of being run by example, which uses
source.  This means that it should not access stdin,
e.g. to scan() data from the example file.
 Data needed for making the examples executable can be obtained by random
number generation (for example, x <- rnorm(100)), or by using
standard data sets listed by data() (see ?data for more
info).
 Finally, there is \donttest, used (at the beginning of a separate
line) to mark code that should be run by example() but not by
R CMD check (by default: the option --run-donttest can
be used).  This should be needed only occasionally but can be used for
code which might fail in circumstances that are hard to test for, for
example in some locales.  (Use e.g. capabilities() or
nzchar(Sys.which("someprogram")) to test for features needed in
the examples wherever possible, and you can also use try() or
tryCatch().  Use interactive() to condition examples which
need someone to interact with.)  Note that code included in
\donttest must be correct R code, and any packages used should
be declared in the DESCRIPTION file.  It is good practice to
include a comment in the \donttest section explaining why it is
needed.
 There can be zero or more \keyword sections per file.
Each \keyword section should specify a single keyword, preferably
one of the standard keywords as listed in file KEYWORDS in the
R documentation directory (default R_HOME/doc).  Use
e.g. RShowDoc("KEYWORDS") to inspect the standard keywords from
within R.  There can be more than one \keyword entry if the R
object being documented falls into more than one category, or none.
 Do strongly consider using \concept (see Indices) instead of
\keyword if you are about to use more than very few non-standard
keywords.
 The special keyword ‘internal’ marks a page of internal objects
that are not part of the package’s API.  If the help page for object
foo has keyword ‘internal’, then help(foo) gives this
help page, but foo is excluded from several object indices,
including the alphabetical list of objects in the HTML help system.
 help.search() can search by keyword, including user-defined
values: however the ‘Search Engine & Keywords’ HTML page accessed
via help.start() provides single-click access only to a
pre-defined list of keywords.
 
Next: Documenting S4 classes and methods, Previous: Documenting functions, Up: Rd format   [Contents][Index] The structure of Rd files which document R data sets is slightly
different.  Sections such as \arguments and \value are not
needed but the format and source of the data should be explained.
 As an example, let us look at src/library/datasets/man/rivers.Rd
which documents the standard R data set rivers.
 This uses the following additional markup commands.
 Indicates the “type” of the documentation object.  Always ‘data’
for data sets, and ‘package’ for pkg-package.Rd
overview files.  Documentation for S4 methods and classes uses
‘methods’ (from promptMethods()) and ‘class’ (from
promptClass()).
 A description of the format of the data set (as a vector, matrix, data
frame, time series, …).  For matrices and data frames this should
give a description of each column, preferably as a list or table.
See Lists and tables, for more information.
 Details of the original source (a reference or URL,
see Specifying URLs).  In addition, section \references could
give secondary sources and usages.
 Note also that when documenting data set bar,
 If bar is a data frame, documenting it as a data set can
be initiated via prompt(bar).  Otherwise, the promptData
function may be used.
 
Next: Documenting packages, Previous: Documenting data sets, Up: Rd format   [Contents][Index] There are special ways to use the ‘?’  operator, namely
‘class?topic’ and ‘methods?topic’, to access
documentation for S4 classes and methods, respectively.  This mechanism
depends on conventions for the topic names used in \alias
entries.  The topic names for S4 classes and methods respectively are of
the form
 where signature_list contains the names of the classes in the
signature of the method (without quotes) separated by ‘,’ (without
whitespace), with ‘ANY’ used for arguments without an explicit
specification.  E.g., ‘genericFunction-class’ is the topic name for
documentation for the S4 class "genericFunction", and
‘coerce,ANY,NULL-method’ is the topic name for documentation for
the S4 method for coerce for signature c("ANY", "NULL").
 Skeletons of documentation for S4 classes and methods can be generated
by using the functions promptClass() and promptMethods()
from package methods.  If it is necessary or desired to provide an
explicit function declaration (in a \usage section) for an S4
method (e.g., if it has “surprising arguments” to be mentioned
explicitly), one can use the special markup
 (e.g., ‘\S4method{coerce}{ANY,NULL}(from, to)’).
 To make full use of the potential of the on-line documentation system,
all user-visible S4 classes and methods in a package should at least
have a suitable \alias entry in one of the package’s Rd files.
If a package has methods for a function defined originally somewhere
else, and does not change the underlying default method for the
function, the package is responsible for documenting the methods it
creates, but not for the function itself or the default method.
 An S4 replacement method is documented in the same way as an S3 one: see
the description of  \method in Documenting functions.
 See help("Documentation", package = "methods") for more
information on using and creating on-line documentation for S4 classes and
methods.
 
Previous: Documenting S4 classes and methods, Up: Rd format   [Contents][Index] Packages may have an overview help page with an \alias
pkgname-package, e.g. ‘utils-package’ for the
utils package, when package?pkgname will open that
help page.  If a topic named pkgname does not exist in
another Rd file, it is helpful to use this as an additional
\alias.
 Skeletons of documentation for a package can be generated using the
function promptPackage().  If the final = LIBS argument
is used, then the Rd file will be generated in final form, containing
the information that would be produced up to
library(help = pkgname).  Otherwise (the default) comments
will be inserted giving suggestions for content.
 Apart from the mandatory \name and \title and the
pkgname-package alias, the only requirement for the package
overview page is that it include a \docType{package} statement.
All other content is optional.  We suggest that it should be a short
overview, to give a reader unfamiliar with the package enough
information to get started.  More extensive documentation is better
placed into a package vignette (see Writing package vignettes) and
referenced from this page, or into individual man pages for the
functions, datasets, or classes.
 
Next: Marking text, Previous: Rd format, Up: Writing R documentation files   [Contents][Index] To begin a new paragraph or leave a blank line in an example, just
insert an empty line (as in (La)TeX).  To break a line, use
\cr.

 In addition to the predefined sections (such as \description{},
\value{}, etc.), you can “define” arbitrary ones by
\section{section_title}{…}.

For example
 For consistency with the pre-assigned sections, the section name (the
first argument to \section) should be capitalized (but not all
upper case).  Whitespace between the first and second braced expressions
is not allowed.  Markup (e.g. \code) within the section title
may cause problems with the latex conversion (depending on the version
of macro packages such as ‘hyperref’) and so should be avoided.
 The \subsection macro takes arguments in the same format as
\section, but is used within a section, so it may be used to
nest subsections within sections or other subsections.  There is no
predefined limit on the nesting level, but formatting is not designed
for more than 3 levels (i.e. subsections within subsections within
sections).
 Note that additional named sections are always inserted at a fixed
position in the output (before \note, \seealso and the
examples), no matter where they appear in the input (but in the same
order amongst themselves as in the input).
 
Next: Lists and tables, Previous: Sectioning, Up: Writing R documentation files   [Contents][Index] The following logical markup commands are available for emphasizing or
quoting text.
 Emphasize text using italic and bold font if
possible; \strong is regarded as stronger (more emphatic).
 Set text in bold font where possible.
 Portably single or double quote text (without hard-wiring the
characters used for quotation marks).
 Each of the above commands takes LaTeX-like input, so other macros
may be used within text.
 The following logical markup commands are available for indicating
specific kinds of text.  Except as noted, these take ‘verbatim’ text
input, and so other macros may not be used within them.  Some characters
will need to be escaped (see Insertions).
 Indicate text that is a literal example of a piece of an R program,
e.g., a fragment of R code or the name of an R object.  Text is
entered in R-like syntax, and displayed using typewriter font
where possible.  Macros \var and \link are interpreted within
text.
 Indicate text that is a literal example of a piece of a program.  Text
is displayed using typewriter font where possible.  Formatting,
e.g. line breaks, is preserved.  (Note that this includes a line break
after the initial {, so typically text should start on the same line as
the command.)
 Due to limitations in LaTeX as of this writing, this macro may not be
nested within other markup macros other than \dQuote and
\sQuote, as errors or bad formatting may result.
 Indicate keyboard input, using slanted typewriter font if
possible, so users can distinguish the characters they are supposed to
type from computer output.  Text is entered ‘verbatim’.
 Indicate text that is a literal example of a sequence of characters,
entered ‘verbatim’.  No wrapping or reformatting will occur.  Displayed
using typewriter font where possible.
 Indicate text that is a literal example of a sequence of characters,
with no interpretation of e.g. \var, but which will be included
within word-wrapped text.  Displayed using typewriter font if
possible.
 Indicate the name of an R package.  LaTeX-like.
 Indicate the name of a file.  Text is LaTeX-like, so backslash needs
to be escaped.  Displayed using a distinct font where possible.
 Indicate an electronic mail address.  LaTeX-like, will be rendered as
a hyperlink in HTML and PDF conversion.  Displayed using
typewriter font where possible.
 Indicate a uniform resource locator (URL) for the World Wide
Web.  The argument is handled as ‘verbatim’ text (with percent and
braces escaped by backslash), and rendered as a hyperlink in HTML and
PDF conversion.  Linefeeds are removed, and as from R 3.2.0 leading
and trailing whitespace81 is removed. See Specifying URLs.
 Displayed using typewriter font where possible.
 Indicate a hyperlink to the World Wide Web. The first argument is
handled as ‘verbatim’ text (with percent and braces escaped by
backslash) and is used as the URL in the hyperlink, with the
second argument of LaTeX-like text displayed to the user.  Linefeeds
are removed from the first argument, and as from R 3.2.0 leading and
trailing whitespace is removed.
 Note that RFC3986-encoded URLs (e.g. using ‘\%28VS.85\%29’ in
place of ‘(VS.85)’) may not work correctly in versions of R
before 3.1.3 and are best avoided—use URLdecode() to decode
them.
 Indicate a metasyntactic variable.  In some cases this will be rendered
distinctly, e.g. in italic, but not in all82. LaTeX-like.
 Indicate an environment variable. ‘Verbatim’.
Displayed using typewriter font where possible
 Indicate a command-line option.  ‘Verbatim’.
Displayed using typewriter font where possible.
 Indicate the name of a command. LaTeX-like, so \var is
interpreted.  Displayed using typewriter font where possible.
 Indicate the introductory or defining use of a term. LaTeX-like.
 Indicate a reference without a direct cross-reference via \link
(see Cross-references), such as the name of a book. LaTeX-like.
 Indicate an acronym (an abbreviation written in all capital letters),
such as GNU. LaTeX-like.
 
Next: Cross-references, Previous: Marking text, Up: Writing R documentation files   [Contents][Index] The \itemize and \enumerate commands take a single
argument, within which there may be one or more \item commands.
The text following each \item is formatted as one or more
paragraphs, suitably indented and with the first paragraph marked with a
bullet point (\itemize) or a number (\enumerate).
 Note that unlike argument lists, \item in these formats is
followed by a space and the text (not enclosed in braces).  For example
 \itemize and \enumerate commands may be nested.
 The \describe command is similar to \itemize but allows
initial labels to be specified.  Each \item takes two arguments,
the label and the body of the item, in exactly the same way as an
argument or value \item.  \describe commands are mapped to
<DL> lists in HTML and \description lists in LaTeX.
 The \tabular command takes two arguments.  The first gives for
each of the columns the required alignment (‘l’ for
left-justification, ‘r’ for right-justification or ‘c’ for
centring.)  The second argument consists of an arbitrary number of
lines separated by \cr, and with fields separated by \tab.
For example:
 There must be the same number of fields on each line as there are
alignments in the first argument, and they must be non-empty (but can
contain only spaces).  (There is no whitespace between \tabular
and the first argument, nor between the two arguments.)
 
Next: Mathematics, Previous: Lists and tables, Up: Writing R documentation files   [Contents][Index] The markup \link{foo} (usually in the combination
\code{\link{foo}}) produces a hyperlink to the help for
foo.  Here foo is a topic, that is the argument of
\alias markup in another Rd file (possibly in another package).
Hyperlinks are supported in some of the formats to which Rd files are
converted, for example HTML and PDF, but ignored in others, e.g.
the text format.
 One main usage of \link is in the \seealso section of the
help page, see Rd format.
 Note that whereas leading and trailing spaces are stripped when
extracting a topic from a \alias, they are not stripped when
looking up the topic of a \link.
 You can specify a link to a different topic than its name by
\link[=dest]{name} which links to topic dest
with name name.  This can be used to refer to the documentation
for S3/4 classes, for example \code{"\link[=abc-class]{abc}"}
would be a way to refer to the documentation of an S4 class "abc"
defined in your package, and
\code{"\link[=terms.object]{terms}"} to the S3 "terms"
class (in package stats).  To make these easy to read in the
source file, \code{"\linkS4class{abc}"} expands to the form
given above.
 There are two other forms of optional argument specified as
\link[pkg]{foo} and
\link[pkg:bar]{foo} to link to the package
pkg, to files foo.html and
bar.html respectively.  These are rarely needed, perhaps to
refer to not-yet-installed packages (but there the HTML help system
will resolve the link at run time) or in the normally undesirable event
that more than one package offers help on a topic83 (in
which case the present package has precedence so this is only needed to
refer to other packages).  They are currently only used in HTML help
(and ignored for hyperlinks in LaTeX conversions of help pages), and
link to the file rather than the topic (since there is no way to know
which topics are in which files in an uninstalled package).  The
only reason to use these forms for base and recommended
packages is to force a reference to a package that might be further down
the search path.  Because they have been frequently misused, the HTML
help system looks for topic foo in package pkg
if it does not find file foo.html.
 
Next: Figures, Previous: Cross-references, Up: Writing R documentation files   [Contents][Index] Mathematical formulae should be set beautifully for printed
documentation yet we still want something useful for text and HTML
online help.  To this end, the two commands
\eqn{latex}{ascii} and
\deqn{latex}{ascii} are used.  Whereas \eqn
is used for “inline” formulae (corresponding to TeX’s
$…$), \deqn gives “displayed equations” (as in
LaTeX’s displaymath environment, or TeX’s
$$…$$).  Both arguments are treated as ‘verbatim’ text.
 Both commands can also be used as \eqn{latexascii} (only
one argument) which then is used for both latex and
ascii.  No whitespace is allowed between command and the first
argument, nor between the first and second arguments.
 The following example is from Poisson.Rd:
 For text on-line help we get
 Greek letters (both cases) will be rendered in HTML if preceded by a
backslash, \dots and \ldots will be rendered as ellipses
and \sqrt, \ge and \le as mathematical symbols.
 Note that only basic LaTeX can be used, there being no provision to
specify LaTeX style files such as the AMS extensions.
 
Next: Insertions, Previous: Mathematics, Up: Writing R documentation files   [Contents][Index] To include figures in help pages, use the \figure markup.  There
are three forms.
 The two commonly used simple forms are \figure{filename}
and \figure{filename}{alternate text}.  This will
include a copy of the figure in either HTML or LaTeX output.  In text
output, the alternate text will be displayed instead.  (When the second
argument is omitted, the filename will be used.)  Both the filename and
the alternate text will be parsed verbatim, and should not include
special characters that are significant in HTML or LaTeX.
 The expert form is \figure{filename}{options:
string}.  (The word ‘options:’ must be typed exactly as
shown and followed by at least one space.)  In this form, the
string is copied into the HTML img tag as attributes
following the src attribute, or into the second argument of the
\Figure macro in LaTeX, which by default is used as options to
an \includegraphics call.  As it is unlikely that any single
string would suffice for both display modes, the expert form would
normally be wrapped in conditionals.  It is up to the author to make
sure that legal HTML/LaTeX is used.  For example, to include a
logo in both HTML (using the simple form) and LaTeX (using the
expert form), the following could be used:
 The files containing the figures should be stored in the directory
man/figures.  Files with extensions .jpg, .jpeg,
.pdf, .png and .svg from that directory will be
copied to the help/figures directory at install time. (Figures in
PDF format will not display in most HTML browsers, but might be the
best choice in reference manuals.)  Specify the filename relative to
man/figures in the \figure directive.
 
Next: Indices, Previous: Figures, Up: Writing R documentation files   [Contents][Index] Use \R for the R system itself.  Use \dots

for the dots in function argument lists ‘…’, and
\ldots

for ellipsis dots in ordinary text.84  These can be followed by
{}, and should be unless followed by whitespace.
 After an unescaped ‘%’, you can put your own comments regarding the
help text.  The rest of the line (but not the newline at the end) will
be completely disregarded.  Therefore, you can also use it to make part
of the “help” invisible.
 You can produce a backslash (‘\’) by escaping it by another
backslash.  (Note that \cr is used for generating line breaks.)
 The “comment” character ‘%’ and unpaired braces85
almost always need to be escaped by ‘\’, and ‘\\’ can
be used for backslash and needs to be when there two or more adjacent
backslashes).  In R-like code quoted strings are handled slightly
differently; see “Parsing Rd files” for details – in particular braces should not be
escaped in quoted strings.
 All of ‘% { } \’ should be escaped in LaTeX-like text.
 Text which might need to be represented differently in different
encodings should be marked by \enc, e.g.
\enc{Jöreskog}{Joreskog} (with no whitespace between the
braces) where the first argument will be used where encodings are
allowed and the second should be ASCII (and is used for e.g.
the text conversion in locales that cannot represent the encoded form).
(This is intended to be used for individual words, not whole sentences
or paragraphs.)
 
Next: Platform-specific sections, Previous: Insertions, Up: Writing R documentation files   [Contents][Index] The \alias command (see Documenting functions) is used to
specify the “topics” documented, which should include all R
objects in a package such as functions and variables, data sets, and S4
classes and methods (see Documenting S4 classes and methods).  The
on-line help system searches the index data base consisting of all
alias topics.
 In addition, it is possible to provide “concept index entries” using
\concept, which can be used for help.search() lookups.
E.g., file cor.test.Rd in the standard package stats
contains
 so that e.g. ??Spearman will succeed in finding the
help page for the test for association between paired samples using
Spearman’s rho.
 (Note that help.search() only uses “sections” of documentation
objects with no additional markup.)
 If you want to cross reference such items from other help files via
\link, you need to use \alias and not \concept.
 
Next: Conditional text, Previous: Indices, Up: Writing R documentation files   [Contents][Index] Sometimes the documentation needs to differ by platform.  Currently two
OS-specific options are available, ‘unix’ and ‘windows’, and
lines in the help source file can be enclosed in
 or
 for OS-specific inclusion or exclusion.  Such blocks should not be
nested, and should be entirely within a block (that, is between the
opening and closing brace of a section or item), or at top-level contain
one or more complete sections.
 If the differences between platforms are extensive or the R objects
documented are only relevant to one platform, platform-specific Rd files
can be put in a unix or windows subdirectory.
 
Next: Dynamic pages, Previous: Platform-specific sections, Up: Writing R documentation files   [Contents][Index] Occasionally the best content for one output format is different from
the best content for another.  For this situation, the
\if{format}{text} or
\ifelse{format}{text}{alternate} markup
is used.  Here format is a comma separated list of formats in
which the text should be rendered.  The alternate will be
rendered if the format does not match.  Both text and
alternate may be any sequence of text and markup.
 Currently the following formats are recognized:  example,
html, latex and text.  These select output for
the corresponding targets. (Note that example refers to
extracted example code rather than the displayed example in some other
format.)  Also accepted are TRUE (matching all formats) and
FALSE (matching no formats).  These could be the output
of the \Sexpr macro (see Dynamic pages).
 The \out{literal} macro would usually be used within
the text part of \if{format}{text}.  It
causes the renderer to output the literal text exactly, with no
attempt to escape special characters.  For example, use
the following to output the markup necessary to display the Greek letter in
LaTeX or HTML, and the text string alpha in other formats:
 
Next: User-defined macros, Previous: Conditional text, Up: Writing R documentation files   [Contents][Index] Two macros supporting dynamically generated man pages are \Sexpr
and \RdOpts.  These are modelled after Sweave, and are intended
to contain executable R expressions in the Rd file.
 The main argument to \Sexpr must be valid R code that can be
executed. It may also take options in square brackets before the main
argument. Depending on the options, the code may be executed at
package build time, package install time, or man page rendering time.
 The options follow the same format as in Sweave, but different options
are supported.  Currently the allowed options and their defaults are:
 Conditionals such as #ifdef
(see Platform-specific sections) are applied after the
build macros but before the install macros.  In some
situations (e.g. installing directly from a source directory without a
tarball, or building a binary package) the above description is not
literally accurate, but authors can rely on the sequence being
build, #ifdef, install, render, with all
stages executed.
 Code is only run once in each stage, so a \Sexpr[results=rd]
macro can output an \Sexpr macro designed for a later stage,
but not for the current one or any earlier stage.
 The \RdOpts macro is used to set new defaults for options to apply
to following uses of \Sexpr.
 For more details, see the online document
“Parsing Rd files”.
 
Next: Encoding, Previous: Dynamic pages, Up: Writing R documentation files   [Contents][Index] The \newcommand and \renewcommand macros allow new macros
to be defined within an Rd file.  These are similar but not identical to
the same-named LaTeX macros.
 They each take two arguments which are parsed verbatim.  The first is
the name of the new macro including the initial backslash, and the second
is the macro definition.  As in LaTeX, \newcommand requires that the
new macro not have been previously defined, whereas \renewcommand
allows existing macros (including all built-in ones) to be replaced.
(As from version 3.2.0, this test is disabled by default, but may
be enabled by setting the environment variable _WARN_DUPLICATE_RD_MACROS_
to a true value.)
 Also as in LaTeX, the new macro may be defined to take arguments,
and numeric placeholders such as #1 are used in the macro
definition. However, unlike LaTeX, the number of arguments is
determined automatically from the highest placeholder number seen in
the macro definition.  For example, a macro definition containing
#1 and #3 (but no other placeholders) will define a
three argument macro (whose second argument will be ignored). As in
LaTeX, at most 9 arguments may be defined. If the #
character is followed by a non-digit it will have no special
significance.  All arguments to user-defined macros will be parsed as
verbatim text, and simple text-substitution will be used to replace
the place-holders, after which the replacement text will be parsed.
 As of R version 3.2.0, a number of macros are defined in the file
share/Rd/macros/system.Rd of the R source or home
directory, and these will normally be available in all .Rd files.
For example, that file contains the definition
 which defines \PR to be a single argument macro; then code
(typically used in the NEWS.Rd file) like
 will expand to
 when parsed.  
 Some macros that might be of general use are:
 A package on CRAN
 A single space (used after a period that does not end a sentence).
 A digital object identifier (DOI).
 See the system.Rd file in share/Rd/macros for more details
and macro definitions, including macros \packageTitle,
\packageDescription, \packageAuthor, \packageMaintainer,
\packageDESCRIPTION and \packageIndices.






 Packages may also define their own common macros; these would be stored
in an .Rd file in man/macros in the package source and
will be installed into help/macros when the package is installed.
A package may also use the macros from a different package by listing
the other package in the ‘RdMacros’ field in the DESCRIPTION
file.
 
Next: Processing documentation files, Previous: User-defined macros, Up: Writing R documentation files   [Contents][Index] Rd files are text files and so it is impossible to deduce the encoding
they are written in unless ASCII: files with 8-bit characters
could be UTF-8, Latin-1, Latin-9, KOI8-R, EUC-JP, etc.  So an
\encoding{} section must be used to specify the encoding if it
is not ASCII.  (The \encoding{} section must be on a
line by itself, and in particular one containing no non-ASCII
characters.  The encoding declared in the DESCRIPTION file will
be used if none is declared in the file.)  The Rd files are
converted to UTF-8 before parsing and so the preferred encoding for the
files themselves is now UTF-8.
 Wherever possible, avoid non-ASCII chars in Rd files, and
even symbols such as ‘<’, ‘>’, ‘$’, ‘^’, ‘&’,
‘|’, ‘@’, ‘~’, and ‘*’ outside ‘verbatim’
environments (since they may disappear in fonts designed to render
text).  (Function showNonASCIIfile in package tools can help
in finding non-ASCII bytes in the files.)
 For convenience, encoding names ‘latin1’ and ‘latin2’ are
always recognized: these and ‘UTF-8’ are likely to work fairly
widely.  However, this does not mean that all characters in UTF-8 will
be recognized, and the coverage of non-Latin characters86 is fairly low.  Using LaTeX
inputenx (see ?Rd2pdf in R) will give greater coverage
of UTF-8.
 The \enc command (see Insertions) can be used to provide
transliterations which will be used in conversions that do not support
the declared encoding.
 The LaTeX conversion converts the file to UTF-8 from the declared
encoding, and includes a
 command, and this needs to be matched by a suitable invocation of the
\usepackage{inputenc} command.  The R utility R
CMD Rd2pdf looks at the converted code and includes the encodings used:
it might for example use
 (Use of utf8 as an encoding requires LaTeX dated 2003/12/01 or
later.  Also, the use of Cyrillic characters in ‘UTF-8’ appears to
also need ‘\usepackage[T2A]{fontenc}’, and R CMD Rd2pdf
includes this conditionally on the file t2aenc.def being present
and environment variable _R_CYRILLIC_TEX_ being set.)
 Note that this mechanism works best with Latin letters: the coverage of
UTF-8 in LaTeX is quite low.
 
Next: Editing Rd files, Previous: Encoding, Up: Writing R documentation files   [Contents][Index] There are several commands to process Rd files from the system command
line.
 Using R CMD Rdconv one can convert R documentation format to
other formats, or extract the executable examples for run-time testing.
The currently supported conversions are to plain text, HTML and
LaTeX as well as extraction of the examples.
 R CMD Rd2pdf generates PDF output from documentation in Rd
files, which can be specified either explicitly or by the path to a
directory with the sources of a package.  In the latter case, a
reference manual for all documented objects in the package is created,
including the information in the DESCRIPTION files.
 R CMD Sweave and R CMD Stangle process vignette-like
documentation files (e.g. Sweave vignettes with extension
‘.Snw’ or ‘.Rnw’, or other non-Sweave vignettes).
R CMD Stangle is used to extract the R code fragments.
 The exact usage and a detailed list of available options for all of
these commands can be obtained by running R CMD command
--help, e.g., R CMD Rdconv --help.  All available commands can be
listed using R --help (or Rcmd --help under Windows).
 All of these work under Windows.  You may need to have installed the
the tools to build packages from source as described in the “R
Installation and Administration” manual, although typically all that is
needed is a LaTeX installation.
 
Previous: Processing documentation files, Up: Writing R documentation files   [Contents][Index] It can be very helpful to prepare .Rd files using a editor which
knows about their syntax and will highlight commands, indent to show the
structure and detect mis-matched braces, and so on.
 The system most commonly used for this is some version of
Emacs (including XEmacs) with the ESS
package (http://ess.r-project.org/: it is often is installed with
Emacs but may need to be loaded, or even installed,
separately).
 Another is the Eclipse IDE with the Stat-ET plugin
(http://www.walware.de/goto/statet), and (on Windows only)
Tinn-R (http://sourceforge.net/projects/tinn-r/).
 People have also used LaTeX mode in a editor, as .Rd files are
rather similar to LaTeX files.
 Some R front-ends provide editing support for  .Rd files, for
example RStudio (https://rstudio.org/).
 
Next: Debugging, Previous: Writing R documentation files, Up: Top   [Contents][Index] R code which is worth preserving in a package and perhaps making
available for others to use is worth documenting, tidying up and perhaps
optimizing. The last two of these activities are the subject of this
chapter.
 
Next: Profiling R code for speed, Previous: Tidying and profiling R code, Up: Tidying and profiling R code   [Contents][Index] R treats function code loaded from packages and code entered by users
differently.  By default code entered by users has the source code stored
internally, and when the function is listed, the original source is
reproduced.  Loading code from a package (by default) discards the
source code, and the function listing is re-created from the parse tree
of the function.
 Normally keeping the source code is a good idea, and in particular it
avoids comments being removed from the source.  However, we can make
use of the ability to re-create a function listing from its parse tree
to produce a tidy version of the function, for example with consistent
indentation and spaces around operators.  If the original source
does not follow the standard format this tidied version can be much
easier to read.
 We can subvert the keeping of source in two ways.
 In each case if we then list the function we will get the standard
layout.
 Suppose we have a file of functions myfuns.R that we want to
tidy up.  Create a file tidy.R containing
 and run R with this as the source file, for example by R
--vanilla < tidy.R or by pasting into an R session.  Then the file
new.myfuns.R will contain the functions in alphabetical order in
the standard layout.  Warning:  comments in your functions will be lost.
 The standard format provides a good starting point for further tidying.
Although the deparsing cannot do so, we recommend the consistent use of
the preferred assignment operator ‘<-’ (rather than ‘=’) for
assignment.  Many package authors use a version of Emacs (on a
Unix-alike or Windows) to edit R code, using the ESS[S] mode of the
ESS Emacs package.  See R coding
standards in R Internals for style options within the ESS[S] mode
recommended for the source code of R itself.
 
Next: Profiling R code for memory use, Previous: Tidying R code, Up: Tidying and profiling R code   [Contents][Index] It is possible to profile R code on Windows and most87 Unix-alike versions of
R.
 The command Rprof is used to control profiling, and its help
page can be consulted for full details.  Profiling works by recording
at fixed intervals88 (by default every 20 msecs)
which line in which R function is being used, and recording the
results in a file (default Rprof.out in the working directory).
Then the function summaryRprof or the command-line utility
R CMD Rprof Rprof.out can be used to summarize the
activity.
 As an example, consider the following code (from Venables & Ripley,
2002, pp. 225–6).
 Having run this we can summarize the results by
 This often produces
surprising results and can be used to identify bottlenecks or pieces of
R code that could benefit from being replaced by compiled code.
 Two warnings: profiling does impose a small performance penalty, and the
output files can be very large if long runs are profiled at the default
sampling interval.
 Profiling short runs can sometimes give misleading results.  R from
time to time performs garbage collection to reclaim unused
memory, and this takes an appreciable amount of time which profiling
will charge to whichever function happens to provoke it.  It may be
useful to compare profiling code immediately after a call to gc()
with a profiling run without a preceding call to gc.
 More detailed analysis of the output can be achieved by the tools in the
CRAN packages proftools and profr: in
particular these allow call graphs to be studied.
 
Next: Profiling compiled code, Previous: Profiling R code for speed, Up: Tidying and profiling R code   [Contents][Index] Measuring memory use in R code is useful either when the code takes
more memory than is conveniently available or when memory allocation and
copying of objects is responsible for slow code. There are three ways to
profile memory use over time in R code. All three require R to
have been compiled with --enable-memory-profiling, which is not
the default, but is currently used for the OS X and Windows binary
distributions. All can be misleading, for different reasons.
 In understanding the memory profiles it is useful to know a little more
about R’s memory allocation. Looking at the results of gc()
shows a division of memory into Vcells used to store the contents
of vectors and Ncells used to store everything else, including
all the administrative overhead for vectors such as type and length
information.  In fact the vector contents are divided into two
pools. Memory for small vectors (by default 128 bytes or less) is
obtained in large chunks and then parcelled out by R; memory for
larger vectors is obtained directly from the operating system.
 Some memory allocation is obvious in interpreted code, for example,
 allocates memory for a new vector y. Other memory allocation is
less obvious and occurs because R is forced to make good on its
promise of ‘call-by-value’ argument passing.  When an argument is
passed to a function it is not immediately copied. Copying occurs (if
necessary) only when the argument is modified.  This can lead to
surprising memory use. For example, in the ‘survey’ package we have
 It may not be obvious that the assignment to x$call will cause
the entire object x to be copied.  This copying to preserve the
call-by-value illusion is usually done by the internal C function
duplicate.
 The main reason that memory-use profiling is difficult is garbage
collection. Memory is allocated at well-defined times in an R
program, but is freed whenever the garbage collector happens to run.
 
Next: Tracking memory allocations, Previous: Profiling R code for memory use, Up: Profiling R code for memory use   [Contents][Index] The sampling profiler Rprof described in the previous section can
be given the option memory.profiling=TRUE. It then writes out the
total R memory allocation in small vectors, large vectors, and cons
cells or nodes at each sampling interval. It also writes out the number
of calls to the internal function duplicate, which is called to
copy R objects. summaryRprof provides summaries of this
information.  The main reason that this can be misleading is that the
memory use is attributed to the function running at the end of the
sampling interval. A second reason is that garbage collection can make
the amount of memory in use decrease, so a function appears to use
little memory.  Running under gctorture helps with both problems:
it slows down the code to effectively increase the sampling frequency
and it makes each garbage collection release a smaller amount of memory.
Changing the memory limits with mem.limits() may also be useful,
to see how the code would run under different memory conditions.
 
Next: Tracing copies of an object, Previous: Memory statistics from Rprof, Up: Profiling R code for memory use   [Contents][Index] The second method of memory profiling uses a memory-allocation
profiler, Rprofmem(), which writes out a stack trace to an
output file every time a large vector is allocated (with a
user-specified threshold for ‘large’) or a new page of memory is
allocated for the R heap. Summary functions for this output are still
being designed.
 Running the example from the previous section with
 shows that apart from some initial and final work in boot there
are no vector allocations over 1000 bytes.
 
Previous: Tracking memory allocations, Up: Profiling R code for memory use   [Contents][Index] The third method of memory profiling involves tracing copies made of a
specific (presumably large) R object. Calling tracemem on an
object marks it so that a message is printed to standard output when
the object is copied via duplicate or coercion to another type,
or when a new object of the same size is created in arithmetic
operations. The main reason that this can be misleading is that
copying of subsets or components of an object is not tracked. It may
be helpful to use tracemem on these components.
 In the example above we can run tracemem on the data frame
st
 The object is duplicated fifteen times, three times for each of the
R+1 calls to storm.bf.  This is surprising, since none of the duplications happen inside nls. Stepping through storm.bf in the debugger shows that all three happen in the line
 Data frames are slower than matrices and this is an example of why.
Using tracemem(st$Viscosity) does not reveal any additional
copying.
 
Previous: Profiling R code for memory use, Up: Tidying and profiling R code   [Contents][Index] Profiling compiled code is highly system-specific, but this section
contains some hints gleaned from various R users.  Some methods need
to be different for a compiled executable and for dynamic/shared
libraries/objects as used by R packages.  We know of no good way to
profile DLLs on Windows.
 
Next: Solaris, Previous: Profiling compiled code, Up: Profiling compiled code   [Contents][Index] Options include using sprof for a shared object, and
oprofile (see http://oprofile.sourceforge.net/) and
perf (see
https://perf.wiki.kernel.org/index.php/Tutorial) for any
executable or shared object.
 You can select shared objects to be profiled with sprof by
setting the environment variable LD_PROFILE.  For example
 It is possible that root access is needed to create the directories used
for the profile data.
 The oprofile project has two modes of operation.  In what is
now called ‘legacy’ mode, it is uses a daemon to collect information on
a process (see below).  Since version 0.9.8 (August 2012), the preferred
mode is to use operf, so we discuss that first.  The modes
differ in how the profiling data is collected: it is analysed by tools
such as opreport and oppannote in both.
 Here is an example on x86_64 Linux using R 3.0.2.  File
pvec.R contains the part of the examples from pvec in
package parallel:
 with timings from the final step
 R-level profiling by Rprof shows
 so the conversion from character to POSIXlt takes most of the
time.
 This can be run under operf and analysed by
 The first report shows where (which library etc) the time was spent:
 The rest of the output is voluminous, and only extracts are shown below.
 Most of the time within R is spent in
 opannotate shows that 31% of the time in R is spent in
memory.c, 21% in datetime.c and 7% in Rstrptime.h.
The analysis for libc showed that calls to wcsftime
dominated, so those calls were cached for R 3.0.3: the time spent in
no-vmlinux (the kernel) was reduced dramatically.
 On platforms which support it, call graphs can be produced by
opcontrol --callgraph if collected via operf
--callgraph.
 The profiling data is by default stored in sub-directory
oprofile_data of the current directory, which can be removed at
the end of the session.
 Another example, from sm version 2.2-5.4.  The example for
sm.variogram took a long time:
 including a lot of system time.  Profiling just the slow part, the
second plot, showed
 so the system time was almost all in the Linux kernel.  It is possible
to dig deeper if you have a matching uncompressed kernel with debug
symbols to specify via --vmlinux: we did not.
 In ‘legacy’ mode oprofile works by running a daemon which
collects information.  The daemon must be started as root, e.g.
 Then as a user
 Shutting down the profiler and clearing the records needs to be done as
root.
 
Next: OS X, Previous: Linux, Up: Profiling compiled code   [Contents][Index] On 64-bit (only) Solaris, the standard profiling tool gprof
collects information from shared objects compiled with -pg.
 
Previous: Solaris, Up: Profiling compiled code   [Contents][Index] Developers have recommended sample (or Sampler.app,
which is a GUI version), Shark (in version of Xcode
up to those for Snow Leopard), and Instruments (part of
Xcode, see
https://developer.apple.com/library/mac/#documentation/DeveloperTools/Conceptual/InstrumentsUserGuide/Introduction/Introduction.html).
 
Next: System and foreign language interfaces, Previous: Tidying and profiling R code, Up: Top   [Contents][Index] This chapter covers the debugging of R extensions, starting with the
ways to get useful error information and moving on to how to deal with
errors that crash R.  For those who prefer other styles there are
contributed packages such as debug on CRAN
(described in an article in
R-News
3/3).  (There are notes from 2002 provided by Roger Peng at
http://www.biostat.jhsph.edu/~rpeng/docs/R-debug-tools.pdf
which provide complementary examples to those given here.)
 
Next: Debugging R code, Previous: Debugging, Up: Debugging   [Contents][Index] Most of the R-level debugging facilities are based around the
built-in browser.  This can be used directly by inserting a call to
browser() into the code of a function (for example, using
fix(my_function) ).  When code execution reaches that point in
the function, control returns to the R console with a special prompt.
For example
 At the browser prompt one can enter any R expression, so for example
ls() lists the objects in the current frame, and entering the
name of an object will89 print it.  The following commands are
also accepted
 Enter ‘step-through’ mode.  In this mode, hitting return executes the
next line of code (more precisely one line and any continuation lines).
Typing c will continue to the end of the current context, e.g.
to the end of the current loop or function.
 In normal mode, this quits the browser and continues execution, and just
return works in the same way.  cont is a synonym.
 This prints the call stack.  For example
 Quit both the browser and the current expression, and return to the
top-level prompt.
 Errors in code executed at the browser prompt will normally return
control to the browser prompt.  Objects can be altered by assignment,
and will keep their changed values when the browser is exited.  If
really necessary, objects can be assigned to the workspace from the
browser prompt (by using <<- if the name is not already in
scope).
 
Next: Checking memory access, Previous: Browsing, Up: Debugging   [Contents][Index] Suppose your R program gives an error message.  The first thing to
find out is what R was doing at the time of the error, and the most
useful tool is traceback().  We suggest that this is run whenever
the cause of the error is not immediately obvious.  Daily, errors are
reported to the R mailing lists as being in some package when
traceback() would show that the error was being reported by some
other package or base R.  Here is an example from the regression
suite.
 The calls to the active frames are given in reverse order (starting with
the innermost).  So we see the error message comes from an explicit
check in glm.fit.  (traceback() shows you all the lines of
the function calls, which can be limited by setting option
"deparse.max.lines".)
 Sometimes the traceback will indicate that the error was detected inside
compiled code, for example (from ?nls)
 This will be the case if the innermost call is to .C,
.Fortran, .Call, .External or .Internal, but
as it is also possible for such code to evaluate R expressions, this
need not be the innermost call, as in
 Occasionally traceback() does not help, and this can be the case
if S4 method dispatch is involved.  Consider the following example
 which does not help much, as there is no call to as.environment
in initialize (and the note “called from internal dispatch”
tells us so).  In this case we searched the R sources for the quoted
call, which occurred in only one place,
methods:::.asEnvironmentPackage.  So now we knew where the
error was occurring.  (This was an unusually opaque example.)
 The error message
 can be hard to handle with the default value (5000).  Unless you know
that there actually is deep recursion going on, it can help to set
something like
 and re-run the example showing the error.
 Sometimes there is warning that clearly is the precursor to some later
error, but it is not obvious where it is coming from.  Setting
options(warn = 2) (which turns warnings into errors) can help here.
 Once we have located the error, we have some choices.  One way to proceed
is to find out more about what was happening at the time of the crash by
looking a post-mortem dump.  To do so, set

options(error=dump.frames) and run the code again.  Then invoke
debugger() and explore the dump.  Continuing our example:
 which is the same as before, but an object called last.dump has
appeared in the workspace.  (Such objects can be large, so remove it
when it is no longer needed.)  We can examine this at a later time by
calling the function debugger.

 which gives the same sequence of calls as traceback, but in
outer-first order and with only the first line of the call, truncated to
the current width.  However, we can now examine in more detail what was
happening at the time of the error.  Selecting an environment opens the
browser in that frame.  So we select the function call which spawned the
error message, and explore some of the variables (and execute two
function calls).
 Because last.dump can be looked at later or even in another R
session, post-mortem debugging is possible even for batch usage of R.
We do need to arrange for the dump to be saved: this can be done either
using the command-line flag --save to save the workspace at the
end of the run, or via a setting such as
 See the help on dump.frames for further options and a worked
example.
 An alternative error action is to use the function recover():
 which is very similar to dump.frames.  However, we can examine
the state of the program directly, without dumping and re-loading the
dump.  As its help page says, recover can be routinely used as
the error action in place of dump.calls and dump.frames,
since it behaves like dump.frames in non-interactive use.
 Post-mortem debugging is good for finding out exactly what went wrong,
but not necessarily why.  An alternative approach is to take a closer
look at what was happening just before the error, and a good way to do
that is to use debug.  This inserts a call to the browser
at the beginning of the function, starting in step-through mode.  So in
our example we could use
 (The prompt Browse[1]> indicates that this is the first level of
browsing: it is possible to step into another function that is itself
being debugged or contains a call to browser().)
 debug can be used for hidden functions and S3 methods by
e.g. debug(stats:::predict.Arima).  (It cannot be used for S4
methods, but an alternative is given on the help page for debug.)
Sometimes you want to debug a function defined inside another function,
e.g. the function arimafn defined inside arima.  To do so,
set debug on the outer function (here arima) and
step through it until the inner function has been defined.  Then
call debug on the inner function (and use c to get out of
step-through mode in the outer function).
 To remove debugging of a function, call undebug with the argument
previously given to debug; debugging otherwise lasts for the rest
of the R session (or until the function is edited or otherwise
replaced).
 trace can be used to temporarily insert debugging code into a
function, for example to insert a call to browser() just before
the point of the error.  To return to our running example
 For your own functions, it may be as easy to use fix to insert
temporary code, but trace can help with functions in a namespace
(as can fixInNamespace).  Alternatively, use
trace(,edit=TRUE) to insert code visually.
 
Next: Debugging compiled code, Previous: Debugging R code, Up: Debugging   [Contents][Index] Errors in memory allocation and reading/writing outside arrays are very
common causes of crashes (e.g., segfaults) on some machines.  Often
the crash appears long after the invalid memory access: in particular
damage to the structures which R itself has allocated may only become
apparent at the next garbage collection (or even at later garbage
collections after objects have been deleted).
 Note that memory access errors may be seen with LAPACK, BLAS, OpenMP and
Java-using packages: some at least of these seem to be intentional, and
some are related to passing characters to Fortran.
 Some of these tools can detect mismatched allocation and deallocation.
C++ programmers should note that memory allocated by new [] must
be freed by delete [], other uses of new by delete,
and memory allocated by malloc, calloc and realloc
by free.  Some platforms will tolerate mismatches (perhaps with
memory leaks) but others will segfault.
 
Next: Using valgrind, Previous: Checking memory access, Up: Checking memory access   [Contents][Index] We can help to detect memory problems in R objects earlier by running
garbage collection as often as possible.  This is achieved by
gctorture(TRUE), which as described on its help page
 Provokes garbage collection on (nearly) every memory allocation.
Intended to ferret out memory protection bugs.  Also makes R run
very slowly, unfortunately.
 The reference to ‘memory protection’ is to missing C-level calls to
PROTECT/UNPROTECT (see Garbage Collection) which if
missing allow R objects to be garbage-collected when they are still
in use.  But it can also help with other memory-related errors.
 Normally running under gctorture(TRUE) will just produce a crash
earlier in the R program, hopefully close to the actual cause. See
the next section for how to decipher such crashes.
 It is possible to run all the examples, tests and vignettes covered by
R CMD check under gctorture(TRUE) by using the option
--use-gct.
 The function gctorture2 provides more refined control over the GC
torture process.  Its arguments step, wait and
inhibit_release are documented on its help page.  Environment
variables can also be used at the start of the R session to turn on
GC torture: R_GCTORTURE corresponds to the step argument to
gctorture2, R_GCTORTURE_WAIT to wait, and
R_GCTORTURE_INHIBIT_RELEASE to inhibit_release.
 If R is configured with --enable-strict-barrier then a
variety of tests for the integrity of the write barrier are enabled.  In
addition tests to help detect protect issues are enabled:
 R CMD check --use-gct can be set to use
gctorture2(n) rather than gctorture(TRUE) by setting
environment variable _R_CHECK_GCT_N_ to a positive integer value
to be used as n.
 Used with a debugger and with gctorture or gctorture2 this
mechanism can be helpful in isolating memory protect problems.
 
Next: Using Address Sanitizer, Previous: Using gctorture, Up: Checking memory access   [Contents][Index] If you have access to Linux on a common CPU type or supported versions
of OS X90 you can use
valgrind (http://www.valgrind.org/, pronounced to rhyme
with ‘tinned’) to check for possible problems.  To run some examples
under valgrind use something like
 where mypkg-Ex.R is a set of examples, e.g. the file created in
mypkg.Rcheck by R CMD check.  Occasionally this reports
memory reads of ‘uninitialised values’ that are the result of compiler
optimization, so can be worth checking under an unoptimized compile: for
maximal information use a build with debugging symbols.  We know there
will be some small memory leaks from readline and R itself —
these are memory areas that are in use right up to the end of the R
session.  Expect this to run around 20x slower than without
valgrind, and in some cases much slower than that.  Several
versions of valgrind were not happy with some optimized BLASes
that use CPU-specific instructions so you may need to build a
version of R specifically to use with valgrind.
 On platforms where valgrind is installed you can build a version
of R with extra instrumentation to help valgrind detect errors
in the use of memory allocated from the R heap.  The
configure option is
--with-valgrind-instrumentation=level, where level
is 0, 1 or 2.  Level 0 is the default and does not add any anything.
Level 1 will detect some uses91 of uninitialised memory and has little impact on speed
(compared to level 0). Level 2 will detect many other memory-use
bugs92  but make R much slower when running under
valgrind.  Using this in conjunction with gctorture can be
even more effective (and even slower).
 An example of valgrind output is
 This example is from an instrumented version of R, while tracking
down a bug in the Matrix package in 2006.  The first line
indicates that R has tried to read 4 bytes from a memory address that
it does not have access to. This is followed by a C stack trace showing
where the error occurred. Next is a description of the memory that was
accessed. It is inside a block allocated by malloc, called from
GetNewPage, that is, in the internal R heap.  Since this
memory all belongs to R, valgrind would not (and did not)
detect the problem in an uninstrumented build of R.  In this example
the stack trace was enough to isolate and fix the bug, which was in
tsc_transpose, and in this example running under
gctorture() did not provide any additional information.  When the
stack trace is not sufficiently informative the option
--db-attach=yes to valgrind may be helpful.  This starts
a post-mortem debugger (by default gdb) so that variables in the
C code can be inspected (see Inspecting R objects).
 valgrind is good at spotting the use of uninitialized values:
use option --track-origins=yes to show where these originated
from.  What it cannot detect is the misuse of arrays allocated on the
stack: this includes C automatic variables and some93
Fortran arrays.
 It is possible to run all the examples, tests and vignettes covered by
R CMD check under valgrind by using the option
--use-valgrind.  If you do this you will need to select the
valgrind options some other way, for example by having a
~/.valgrindrc file containing
 or setting the environment variable VALGRIND_OPTS.
 On OS X you may need to ensure that debugging symbols are made available
(so valgrind reports line numbers in files).  This can usually
be done with the valgrind option --dsymutil=yes to
ask for the symbols to be dumped when the .so file is loaded.
This will not work where packages are installed into a system area (such
as the R.framework) and can be slow.  Installing packages with
R CMD INSTALL --dsym installs the dumped symbols.  (This can
also be done by setting environment variable PKG_MAKE_DSYM to a
non-empty value before the INSTALL.)
 This section has described the use of memtest, the default
(and most useful) of valgrind’s tools.  There are others
described in its documentation: helgrind can be useful for
threaded programs.
 
Next: Using Undefined Behaviour Sanitizer, Previous: Using valgrind, Up: Checking memory access   [Contents][Index] AddressSanitizer (‘ASan’) is a tool with similar aims to the
memory checker in valgrind.  It is available with suitable
builds94 of gcc and clang on common Linux
and OS X platforms.  See
http://clang.llvm.org/docs/UsersManual.html#controlling-code-generation,
http://clang.llvm.org/docs/AddressSanitizer.html and
https://code.google.com/p/address-sanitizer/.
 More thorough checks of C++ code are done if the C++ library has been
‘annotated’: at the time of writing this applied to std::vector
in libc++ for use with clang and gives rise to
‘container-overflow’ reports.
 It requires code to have been compiled and linked with
-fsanitize=address and compiling with -fno-omit-frame-pointer
will give more legible reports.  It has a runtime penalty of 2–3x,
extended compilation times and uses substantially more memory, often
1–2GB, at run time.  On 64-bit platforms it reserves (but does not
allocate) 16–20TB of virtual memory: restrictive shell settings can
cause problems.
 By comparison with valgrind, ASan can
detect misuse of stack and global variables but not the use of
uninitialized memory.
 Recent versions return symbolic addresses for the location of the error
provided llvm-symbolizer95 is on the path: if it is available but not
on the path or has been renamed96, one can use an
environment variable, e.g.
 An alternative is to pipe the output through
asan_symbolize.py97 and perhaps
then (for compiled C++ code) c++filt.  (On OS X, you may need
to run dsymutil to get line-number reports.)
 The simplest way to make use of this is to build a version of R with
something like
 which will ensure that the libasan run-time library is compiled
into the R executable.  However this check can be enabled on a
per-package basis by using a ~/.R/Makevars file like
 (Note that -fsanitize=address has to be part of the compiler
specification to ensure it is used for linking.  These settings will not
be honoured by packages which ignore ~/.R/Makevars.)  It will
be necessary to build R with
 to link the runtime libraries into the R executable if it was not
specified as part of ‘CC’ when R was built.
 For options available via the environment variable
ASAN_OPTIONS see
https://code.google.com/p/address-sanitizer/wiki/Flags#Run-time_flags.
With gcc additional control is available via the
--params flag: see its man page.
 For more detailed information on an error, R can be run under a
debugger with a breakpoint set before the address sanitizer report is
produced: for gdb or lldb you could use
 (See
https://code.google.com/p/address-sanitizer/wiki/AddressSanitizer#gdb.)
 
Previous: Using Address Sanitizer, Up: Using Address Sanitizer   [Contents][Index] For x86_64 Linux there is a leak sanitizer, ‘LSan’: see
https://code.google.com/p/address-sanitizer/wiki/LeakSanitizer.
This is available on recent versions of gcc and clang, and
where available is compiled in as part of ASan.
 One way to invoke this from an ASan-enabled build is by the environment
variable
 However, this was made the default for clang 3.5 and
gcc 5.1.0.
 When LSan is enabled, leaks give the process a failure error status (by
default 23).  For an R package this means the R process,
and as the parser retains some memory to the end of the process, if R
itself was built against ASan, all runs will have a failure error status
(which may include running R as part of building R itself).
 To disable both this and some strict checking use
 LSan also has a ‘stand-alone’ mode where it is compiled in using
-fsanitize=leak and avoids the run-time overhead of ASan.
 
Next: Other analyses with ‘clang’, Previous: Using Address Sanitizer, Up: Checking memory access   [Contents][Index] ‘Undefined behaviour’ is where the language standard does not require
particular behaviour from the compiler.  Examples include division by
zero (where for doubles R requires the
ISO/IEC 60559 behaviour but C/C++ do not), use
of zero-length arrays, shifts too far for signed types (e.g. int
x, y; y = x << 31;), out-of-range coercion, invalid C++ casts and
mis-alignment.  Not uncommon examples of out-of-range coercion in R
packages are attempts to coerce a NaN or infinity to type
int or NA_INTEGER to an unsigned type such as
size_t.  Also common is y[x - 1] forgetting that x
might be NA_INTEGER.
 ‘UBSanitizer’ is a tool for C/C++ source code selected by
-fsanitize=undefined in suitable builds98 of clang and GCC.  Its (main) runtime library is
linked into each package’s DLL, so it is less often needed to be
included in MAIN_LDFLAGS.
 Some versions have greatly increased compilation times on a few
files99.
 This sanitizer can be combined with the Address Sanitizer by
-fsanitize=undefined,address (where both are supported).
 Finer control of what is checked can be achieved by other options: for
clang see
http://clang.llvm.org/docs/UsersManual.html#controlling-code-generation.100
The current set for clang is (on a single line):
 a subset of which could be combined with address, or use something
like
 (function, return and vptr apply only to C++).  In
addition,
 is available as a separate option in some versions of clang
(not enabled by -fsanitize=undefined).
 clang 3.5 and later may need
 for C++ code (in CXX and CXX1X) as the run-time library
for vptr needs to be linked into the main R executable (and
that would need to be linked by clang++, not clang: you
could try building R with something like
 and perhaps for clang 3.7.x
 or add -lclang_rt.asan_cxx-x86_64101 or similar to LD_FLAGS).
 See https://gcc.gnu.org/onlinedocs/gcc/Debugging-Options.html (or
the manual for your version of GCC, installed or via
https://gcc.gnu.org/onlinedocs/) for the options supported by
GCC: 5.3 supports
 with
 as a separate option not enabled by -fsanitize=undefined (and not
desirable for R uses).  At the time of writing the object-size
and vptr checks produced many warnings on GCC’s own C++ headers,
so should be disabled.
 GCC 6 will add
 an extension of bounds to ‘flexible array member-like arrays’.
 Other useful flags include
 which causes the first report to be fatal (it always is for the
unreachable and return suboptions).  For more detailed
information on where the runtime error occurs, R can be run under a
debugger with a breakpoint set before the sanitizer report is produced:
for gdb or lldb you could use
 or similar (there are handlers for each type of undefined behaviour).
 There are also the compiler flags -fcatch-undefined-behavior
and -ftrapv, said to be more reliable in clang than
gcc.
 For more details on the topic see
http://blog.regehr.org/archives/213 and
http://blog.llvm.org/2011/05/what-every-c-programmer-should-know.html
(which has 3 parts).
 
Next: Using ‘Dr. Memory’, Previous: Using Undefined Behaviour Sanitizer, Up: Checking memory access   [Contents][Index] Recent versions of clang on ‘x86_64’ Linux have
‘ThreadSanitizer’ (https://code.google.com/p/thread-sanitizer/),
a ‘data race detector for C/C++ programs’, and ‘MemorySanitizer’
(http://clang.llvm.org/docs/MemorySanitizer.html,
https://code.google.com/p/memory-sanitizer/wiki/MemorySanitizer)
for the detection of uninitialized memory.  Both are based on and
provide similar functionality to tools in valgrind.
 clang has a ‘Static Analyser’ which can be run on the source
files during compilation: see http://clang-analyzer.llvm.org/.
 
Next: Fortran array bounds checking, Previous: Other analyses with ‘clang’, Up: Checking memory access   [Contents][Index] ‘Dr. Memory’ from http://www.drmemory.org/ is a memory checker
for (currently) 32-bit Windows, Linux and OS X with similar aims to
valgrind.  It works with unmodified executables102
and detects memory access errors, uninitialized reads and memory leaks.
 
Previous: Using ‘Dr. Memory’, Up: Checking memory access   [Contents][Index] Most of the Fortran compilers used with R allow code to be compiled
with checking of array bounds: for example gfortran has option
-fbounds-check and Solaris Studio has -C.  This will
give an error when the upper or lower bound is exceeded, e.g.
 One does need to be aware that lazy programmers often specify Fortran
dimensions as 1 rather than * or a real bound and these
will be reported.
 It is easy to arrange to use this check on just the code in your
package: add to ~/.R/Makevars something like (for
gfortran)
 when you run R CMD check.
 This may report incorrectly errors with the way that Fortran character
variables are passed, particularly when Fortran subroutines are called
from C code.  This may include the use of BLAS and LAPACK subroutines in
R, so it is not advisable to build R itself with bounds checking
(and may not even be possible as these subroutines are called during the
R build).
 
Previous: Checking memory access, Up: Debugging   [Contents][Index] Sooner or later programmers will be faced with the need to debug
compiled code loaded into R.  This section is geared to platforms
using gdb with code compiled by gcc, but similar things
are possible with other debuggers such as lldb
(http://lldb.llvm.org/, used on OS X) and Sun’s dbx:
some debuggers have graphical front-ends available.
 Consider first ‘crashes’, that is when R terminated unexpectedly with
an illegal memory access (a ‘segfault’ or ‘bus error’), illegal
instruction or similar.  Unix-alike versions of R use a signal
handler which aims to give some basic information.  For example
 Since the R process may be damaged, the only really safe options are
the first or third.  (Note that a core dump is only produced where
enabled: a common default in a shell is to limit its size to 0, thereby
disabling it.)
 A fairly common cause of such crashes is a package which uses .C
or .Fortran and writes beyond (at either end) one of the
arguments it is passed.  As from R 3.0.0 there is a good way to
detect this: using options(CBoundsCheck = TRUE) (which can be
selected via the environment variable R_C_BOUNDS_CHECK=yes)
changes the way .C and .Fortran work to check if the
compiled code writes in the 64 bytes at either end of an argument.
 Another cause of a ‘crash’ is to overrun the C stack.  R tries to
track that in its own code, but it may happen in third-party compiled
code.  For modern POSIX-compliant OSes R can safely catch that and
return to the top-level prompt, so one gets something like
 However, C stack overflows are fatal under Windows and normally defeat
attempts at debugging on that platform.  Further, the size of the stack
is set when R is compiled, whereas on POSIX OSes it can be set in the
shell from which R is launched.
 If you have a crash which gives a core dump you can use something like
 to examine the core dump.  If core dumps are disabled or to catch errors
that do not generate a dump one can run R directly under a debugger
by for example
 at which point R will run normally, and hopefully the debugger will
catch the error and return to its prompt.  This can also be used to
catch infinite loops or interrupt very long-running code.  For a simple
example
 In many cases it is possible to attach a debugger to a running process:
this is helpful if an alternative front-end is in use or to investigate
a task that seems to be taking far too long.  This is done by something
like
 where pid is the id of the R executable or front-end.
This stops the process so its state can be examined: use continue
to resume execution.
 Some “tricks” worth knowing follow:
 
Next: Inspecting R objects, Previous: Debugging compiled code, Up: Debugging compiled code   [Contents][Index] Under most compilation environments, compiled code dynamically loaded
into R cannot have breakpoints set within it until it is loaded.  To
use a symbolic debugger on such dynamically loaded code under
Unix-alikes use
 Under Windows signals may not be able to be used, and if so the procedure is
more complicated.  See the rw-FAQ and
www.stats.uwo.ca/faculty/murdoch/software/debuggingR/gdb.shtml.
 
Previous: Finding entry points, Up: Debugging compiled code   [Contents][Index] The key to inspecting R objects from compiled code is the function
PrintValue(SEXP s) which uses the normal R printing
mechanisms to print the R object pointed to by s, or the safer
version R_PV(SEXP s) which will only print ‘objects’.
 One way to make use of PrintValue is to insert suitable calls
into the code to be debugged.
 Another way is to call R_PV from the symbolic debugger.
(PrintValue is hidden as Rf_PrintValue.)  For example,
from gdb we can use
 using the object ab from the convolution example, if we have
placed a suitable breakpoint in the convolution C code.
 To examine an arbitrary R object we need to work a little harder.
For example, let
 By setting a breakpoint at do_get and typing get("DF") at
the R prompt, one can find out the address in memory of DF, for
example
 (Debugger output reformatted for better legibility).
 Using R_PV() one can “inspect” the values of the various
elements of the SEXP, for example,
 To find out where exactly the corresponding information is stored, one
needs to go “deeper”:
 Another alternative is the R_inspect function which shows the
low-level structure of the objects recursively (addresses differ from
the above as this example is created on another machine):
 In general the representation of each object follows the format:
 For a more fine-grained control over the depth of the recursion
and the output of vectors R_inspect3 takes additional two character()
parameters: maximum depth and the maximal number of elements that will
be printed for scalar vectors. The defaults in R_inspect are
currently -1 (no limit) and 5 respectively.
 
Next: The R API, Previous: Debugging, Up: Top   [Contents][Index] 
Next: Interface functions .C and .Fortran, Previous: System and foreign language interfaces, Up: System and foreign language interfaces   [Contents][Index] Access to operating system functions is via the R functions
system and system2.


The details will differ by platform (see the on-line help), and about
all that can safely be assumed is that the first argument will be a
string command that will be passed for execution (not necessarily
by a shell) and the second argument to system will be
internal which if true will collect the output of the command
into an R character vector.
 On POSIX-compliant OSes these commands pass a command-line to a shell:
Windows is not POSIX-compliant and there is a separate function
shell to do so.
 The function system.time

is available for timing.  Timing on child processes is only available on
Unix-alikes, and may not be reliable there.
 
Next: dyn.load and dyn.unload, Previous: Operating system access, Up: System and foreign language interfaces   [Contents][Index] These two functions provide an interface to compiled code that has been
linked into R, either at build time or via dyn.load
(see dyn.load and dyn.unload).  They are primarily intended for
compiled C and FORTRAN 77 code respectively, but the .C function
can be used with other languages which can generate C interfaces, for
example C++ (see Interfacing C++ code).
 The first argument to each function is a character string specifying the
symbol name as known103 to C or
FORTRAN, that is the function or subroutine name.  (That the symbol is
loaded can be tested by, for example, is.loaded("cg").  Use the
name you pass to .C or .Fortran rather than the translated
symbol name.)
 There can be up to 65 further arguments giving R objects to be passed
to compiled code.  Normally these are copied before being passed in, and
copied again to an R list object when the compiled code returns.  If
the arguments are given names, these are used as names for the
components in the returned list object (but not passed to the compiled
code).
 The following table gives the mapping between the modes of R atomic
vectors and the types of arguments to a C function or FORTRAN
subroutine.
 Do please note the first two.  On the 64-bit Unix/Linux/OS X platforms,
long is 64-bit whereas int and INTEGER are 32-bit.
Code ported from S-PLUS (which uses long * for logical and
integer) will not work on all 64-bit platforms (although it may
appear to work on some, including Windows).  Note also that if your
compiled code is a mixture of C functions and FORTRAN subprograms the
argument types must match as given in the table above.
 C type Rcomplex is a structure with double members
r and i defined in the header file R_ext/Complex.h
included by R.h.  (On most platforms this is stored in a way
compatible with the C99 double complex type: however, it may not
be possible to pass Rcomplex to a C99 function expecting a
double complex argument.  Nor need it be compatible with a C++
complex type.  Moreover, the compatibility can depends on the
optimization level set for the compiler.)
 Only a single character string can be passed to or from FORTRAN, and the
success of this is compiler-dependent.  Other R objects can be passed
to .C, but it is much better to use one of the other interfaces.
 It is possible to pass numeric vectors of storage mode double to
C as float * or to FORTRAN as REAL by setting the
attribute Csingle, most conveniently by using the R functions
as.single, single or mode.  This is intended only
to be used to aid interfacing existing C or FORTRAN code.
 Logical values are sent as 0 (FALSE), 1
(TRUE) or INT_MIN = -2147483648 (NA, but only if
NAOK is true), and the compiled code should return one of these
three values.  (Non-zero values other than INT_MIN are mapped to
TRUE.)
 Unless formal argument NAOK is true, all the other arguments are
checked for missing values NA and for the IEEE special
values NaN, Inf and -Inf, and the presence of any
of these generates an error.  If it is true, these values are passed
unchecked.
 Argument PACKAGE confines the search for the symbol name to a
specific shared object (or use "base" for code compiled into
R).  Its use is highly desirable, as there is no way to avoid two
package writers using the same symbol name, and such name clashes are
normally sufficient to cause R to crash.  (If it is not present and
the call is from the body of a function defined in a package namespace,
the shared object loaded by the first (if any) useDynLib
directive will be used.  However, prior to R 2.15.2 the detection of
the correct namespace is unreliable and you are strongly recommended to
use the PACKAGE argument for packages to be used with earlier
versions of R.
 Note that the compiled code should not return anything except through
its arguments: C functions should be of type void and FORTRAN
subprograms should be subroutines.
 To fix ideas, let us consider a very simple example which convolves two
finite sequences. (This is hard to do fast in interpreted R code, but
easy in C code.)  We could do this using .C by
 called from R by
 Note that we take care to coerce all the arguments to the correct R
storage mode before calling .C; mistakes in matching the types
can lead to wrong results or hard-to-catch errors.
 Special care is needed in handling character vector arguments in
C (or C++).  On entry the contents of the elements are duplicated and
assigned to the elements of a char ** array, and on exit the
elements of the C array are copied to create new elements of a character
vector.  This means that the contents of the character strings of the
char ** array can be changed, including to \0 to shorten
the string, but the strings cannot be lengthened.  It is
possible104 to allocate a new string via
R_alloc and replace an entry in the char ** array by the
new string.  However, when character vectors are used other than in a
read-only way, the .Call interface is much to be preferred.
 Passing character strings to FORTRAN code needs even more care, and
should be avoided where possible.  Only the first element of the
character vector is passed in, as a fixed-length (255) character array.
Up to 255 characters are passed back to a length-one character vector.
How well this works (or even if it works at all) depends on the C and
FORTRAN compilers on each platform (including on their options).  Often
what is being passed to FORTRAN is one of a small set of possible values
(a factor in R terms) which could alternatively be passed as an
integer code: similarly FORTRAN code that wants to generate diagnostic
messages can pass an integer code to a C or R wrapper which will
convert it to a character string.
 It is possible to pass some R objects other than atomic vectors via
.C, but this is only supported for historical compatibility: use
the .Call or .External interfaces for such objects.  Any
C/C++ code that includes Rinternals.h should be called via
.Call or .External.
 
Next: Registering native routines, Previous: Interface functions .C and .Fortran, Up: System and foreign language interfaces   [Contents][Index] Compiled code to be used with R is loaded as a shared object
(Unix-alikes including OS X, see Creating shared objects for more
information) or DLL (Windows).
 The shared object/DLL is loaded by dyn.load and unloaded by
dyn.unload.  Unloading is not normally necessary, but it is
needed to allow the DLL to be re-built on some platforms, including
Windows.
 The first argument to both functions is a character string giving the
path to the object.  Programmers should not assume a specific file
extension for the object/DLL (such as .so) but use a construction
like
 for platform independence.  On Unix-alike systems the path supplied to
dyn.load can be an absolute path, one relative to the current
directory or, if it starts with ‘~’, relative to the user’s home
directory.
 Loading is most often done automatically based on the useDynLib()
declaration in the NAMESPACE file, but may be done
explicitly via a call to library.dynam.

This has the form
 where libname is the object/DLL name with the extension
omitted.  Note that the first argument, chname, should
not be package since this will not work if the package
is installed under another name.
 Under some Unix-alike systems there is a choice of how the symbols are
resolved when the object is loaded, governed by the arguments
local and now.  Only use these if really necessary: in
particular using now=FALSE and then calling an unresolved symbol
will terminate R unceremoniously.
 R provides a way of executing some code automatically when a object/DLL
is either loaded or unloaded.  This can be used, for example, to
register native routines with R’s dynamic symbol mechanism, initialize
some data in the native code, or initialize a third party library.  On
loading a DLL, R will look for a routine within that DLL named
R_init_lib where lib is the name of the DLL file with
the extension removed.  For example, in the command
 R looks for the symbol named R_init_mylib.  Similarly, when
unloading the object, R looks for a routine named
R_unload_lib, e.g., R_unload_mylib.  In either case,
if the routine is present, R will invoke it and pass it a single
argument describing the DLL.  This is a value of type DllInfo
which is defined in the Rdynload.h file in the R_ext
directory.
 Note that there are some implicit restrictions on this mechanism as the
basename of the DLL needs to be both a valid file name and valid as part
of a C entry point (e.g. it cannot contain ‘.’): for portable
code it is best to confine DLL names to be ASCII alphanumeric
plus underscore.  If entry point R_init_lib is not found it
is also looked for with ‘.’ replaced by ‘_’.
 The following example shows templates for the initialization and
unload routines for the mylib DLL.
 If a shared object/DLL is loaded more than once the most recent version
is used.  More generally, if the same symbol name appears in several
shared objects, the most recently loaded occurrence is used.  The
PACKAGE argument and registration (see the next section) provide
good ways to avoid any ambiguity in which occurrence is meant.
 On Unix-alikes the paths used to resolve dynamically linked dependent
libraries are fixed (for security reasons) when the process is launched,
so dyn.load will only look for such libraries in the locations
set by the R shell script (via etc/ldpaths) and in
the OS-specific defaults.
 Windows allows more control (and less security) over where dependent
DLLs are looked for.  On all versions this includes the PATH
environment variable, but with lowest priority: note that it does not
include the directory from which the DLL was loaded.  It is possible to
add a single path with quite high priority via the DLLpath
argument to dyn.load.  This is (by default) used by
library.dynam to include the package’s libs/i386 or
libs/x64 directory in the DLL search path.
 
Next: Creating shared objects, Previous: dyn.load and dyn.unload, Up: System and foreign language interfaces   [Contents][Index] By ‘native’ routine, we mean an entry point in compiled code.
 In calls to .C, .Call, .Fortran and
.External, R must locate the specified native routine by
looking in the appropriate shared object/DLL.  By default, R uses the
operating system-specific dynamic loader to lookup the symbol in all
loaded DLLs and elsewhere.  Alternatively, the author of the DLL
can explicitly register routines with R and use a single,
platform-independent mechanism for finding the routines in the DLL.  One
can use this registration mechanism to provide additional information
about a routine, including the number and type of the arguments, and
also make it available to R programmers under a different name.  In
the future, registration may be used to implement a form of “secure”
or limited native access.
 To register routines with R, one calls the C routine
R_registerRoutines.  This is typically done when the DLL is first
loaded within the initialization routine R_init_dll name
described in dyn.load and dyn.unload.  R_registerRoutines
takes 5 arguments.  The first is the DllInfo object passed by
R to the initialization routine. This is where R stores the
information about the methods.  The remaining 4 arguments are arrays
describing the routines for each of the 4 different interfaces:
.C, .Call, .Fortran and .External.  Each
argument is a FIND-terminated array of the element types given in
the following table:
 Currently, the R_ExternalMethodDef is the same as
R_CallMethodDef type and contains fields for the name of the
routine by which it can be accessed in R, a pointer to the actual
native symbol (i.e., the routine itself), and the number of arguments
the routine expects to be passed from R. For example, if we had a
routine named myCall defined as
 we would describe this as
 along with any other routines for the .Call interface. For
routines with a variable number of arguments invoked via the
.External interface, one specifies -1 for the number of
arguments which tells R not to check the actual number passed.  Note
that the number of arguments passed to .External were not
checked prior to R 3.0.0.
 Routines for use with the .C and .Fortran interfaces are
described with similar data structures, but which have two additional
fields for describing the type and “style” of each argument.  Each of
these can be omitted. However, if specified, each should be an array
with the same number of elements as the number of parameters for the
routine.  The types array should contain the SEXP types
describing the expected type of the argument. (Technically, the elements
of the types array are of type R_NativePrimitiveArgType which is
just an unsigned integer.)  The R types and corresponding type
identifiers are provided in the following table:
 Consider a C routine, myC, declared as
 We would register it as
 One can also specify whether each argument is used simply as input, or
as output, or as both input and output.  The style field in the
description of a method is used for this.  The purpose is to
allow105 R to transfer values
more efficiently across the R-C/FORTRAN interface by avoiding copying
values when it is not necessary. Typically, one omits this information
in the registration data.
 Having created the arrays describing each routine, the last step is to
actually register them with R.  We do this by calling
R_registerRoutines.  For example, if we have the descriptions
above for the routines accessed by the .C and .Call
we would use the following code:
 This routine will be invoked when R loads the shared object/DLL named
myLib.  The last two arguments in the call to
R_registerRoutines are for the routines accessed by
.Fortran and .External interfaces.  In our example, these
are given as NULL since we have no routines of these types.
 When R unloads a shared object/DLL, its registrations are
automatically removed.  There is no other facility for unregistering a
symbol.
 Examples of registering routines can be found in the different packages
in the R source tree (e.g., stats).  Also, there is a
brief, high-level introduction in R News (volume 1/3, September
2001, pages 20–23, https://www.r-project.org/doc/Rnews/Rnews_2001-3.pdf).
 Once routines are registered, they can be referred to as R objects if
they this is arranged in the useDynLib call in the package’s
NAMESPACE file (see useDynLib).  This avoids the overhead
of looking up an entry point each time it is used, and ensure that the
entry point in the package is the one used (without a PACKAGE =
"pkg" argument).  So for example the stats package has
 in its NAMESPACE file, and then ansari.test’s default
methods can contain
 
Next: Linking to native routines in other packages, Previous: Registering native routines, Up: Registering native routines   [Contents][Index] Sometimes registering native routines or using a PACKAGE argument
can make a large difference.  The results can depend quite markedly on
the OS (and even if it is 32- or 64-bit), on the version of R and
what else is loaded into R at the time.
 To fix ideas, first consider x84_64 OS 10.7 and R 2.15.2.  A
simple .Call function might be
 with C code
 If we compile with by R CMD SHLIB foo.c, load the code by
dyn.load("foo.so") and run foo(pi) it took around 22
microseconds (us). Specifying the DLL by
 reduced the time to 1.7 us.
 Now consider making these functions part of a package whose
NAMESPACE file uses useDynlib(foo).  This immediately
reduces the running time as "foo" will be preferentially looked
for foo.dll.  Without specifying PACKAGE it took about 5
us (it needs to fathom out the appropriate DLL each time it is invoked
but it does not need to search all DLLs), and with the PACKAGE
argument it is again about 1.7 us.
 Next suppose the package has registered the native routine foo.
Then foo() still has to find the appropriate DLL but can get to
the entry point in the DLL faster, in about 4.2 us.  And foo2()
now takes about 1 us.  If we register the symbols in the
NAMESPACE file and use
 then the address for the native routine is looked up just once when the
package is loaded, and foo3(pi) takes about 0.8 us.
 Versions using .C() rather than .Call() take about 0.2 us
longer.
 These are all quite small differences, but C routines are not uncommonly
invoked millions of times for run times of a few microseconds, and those
doing such things may wish to be aware of the differences.
 On Linux and Solaris there is a much smaller overhead in looking up
symbols so foo(pi) takes around 5 times as long as
foo3(pi).
 Symbol lookup on Windows used to be far slower, so R maintains a
small cache.  If the cache is currently empty enough that the symbol can
be stored in the cache then the performance is similar to Linux and
Solaris: if not it may be slower.  R’s own code always uses
registered symbols and so these never contribute to the cache: however
many other packages do rely on symbol lookup.
 
Previous: Speed considerations, Up: Registering native routines   [Contents][Index] In addition to registering C routines to be called by R, it can at
times be useful for one package to make some of its C routines available
to be called by C code in another package.  The interface consists of
two routines declared in header R_ext/Rdynload.h as
 A package packA that wants to make a C routine myCfun
available to C code in other packages would include the call
 in its initialization function R_init_packA.  A package
packB that wants to use this routine would retrieve the function
pointer with a call of the form
 The author of packB is responsible for ensuring that
p_myCfun has an appropriate declaration. In the future R may
provide some automated tools to simplify exporting larger numbers of
routines.
 A package that wishes to make use of header files in other packages
needs to declare them as a comma-separated list in the field
‘LinkingTo’ in the DESCRIPTION file.  This then arranges
that the include directories in the installed linked-to packages
are added to the include paths for C and C++ code.
 It must specify106
‘Imports’ or ‘Depends’ of those packages, for they have to be
loaded107 prior to this one
(so the path to their compiled code has been registered).
 A CRAN example of the use of this mechanism is package
lme4, which links to Matrix.
 
Next: Interfacing C++ code, Previous: Registering native routines, Up: System and foreign language interfaces   [Contents][Index] Shared objects for loading into R can be created using R CMD
SHLIB.  This accepts as arguments a list of files which must be object
files (with extension .o) or sources for C, C++, FORTRAN 77,
Fortran 9x, Objective C or Objective C++ (with extensions .c,
.cc or .cpp, .f, .f90 or .f95,
.m, and .mm or .M, respectively), or commands to be
passed to the linker.  See R CMD SHLIB --help (or the R help
for SHLIB) for usage information.
 If compiling the source files does not work “out of the box”, you can
specify additional flags by setting some of the variables

PKG_CPPFLAGS (for the C preprocessor, typically ‘-I’ flags),






PKG_CFLAGS, PKG_CXXFLAGS, PKG_FFLAGS,
PKG_FCFLAGS, PKG_OBJCFLAGS, and PKG_OBJCXXFLAGS
(for the C, C++, FORTRAN 77, Fortran 9x, Objective C, and Objective C++
compilers, respectively) in the file Makevars in the compilation
directory (or, of course, create the object files directly from the
command line).

Similarly, variable PKG_LIBS in Makevars can be used for
additional ‘-l’ and ‘-L’ flags to be passed to the linker when
building the shared object. (Supplying linker commands as arguments to
R CMD SHLIB will take precedence over PKG_LIBS in
Makevars.)
 It is possible to arrange to include compiled code from other languages
by setting the macro ‘OBJECTS’ in file Makevars, together
with suitable rules to make the objects.
 Flags which are already set (for example in file
etcR_ARCH/Makeconf) can be overridden by the environment
variable MAKEFLAGS (at least for systems using a POSIX-compliant
make), as in (Bourne shell syntax)
 It is also possible to set such variables in personal Makevars
files, which are read after the local Makevars and the system
makefiles or in a site-wide Makevars.site file.
See Customizing package compilation in R Installation and Administration,
 Note that as R CMD SHLIB uses Make, it will not remake a shared
object just because the flags have changed, and if test.c and
test.f both exist in the current directory
 will compile test.c!
 If the src subdirectory of an add-on package contains source code
with one of the extensions listed above or a file Makevars but
not a file Makefile, R CMD INSTALL creates a
shared object (for loading into R through useDynlib in the
NAMESPACE, or in the .onLoad function of the package)
using the R CMD SHLIB mechanism.  If file Makevars
exists it is read first, then the system makefile and then any personal
Makevars files.
 If the src subdirectory of package contains a file
Makefile, this is used by R CMD INSTALL in place of the
R CMD SHLIB mechanism.  make is called with makefiles
R_HOME/etcR_ARCH/Makeconf, src/Makefile and
any personal Makevars files (in that order).  The first target
found in src/Makefile is used.
 It is better to make use of a Makevars file rather than a
Makefile: the latter should be needed only exceptionally.
 Under Windows the same commands work, but Makevars.win will be
used in preference to Makevars, and only src/Makefile.win
will be used by R CMD INSTALL with src/Makefile being
ignored.  For past experiences of building DLLs with a variety of
compilers, see file ‘README.packages’ and
http://www.stats.uwo.ca/faculty/murdoch/software/compilingDLLs/
.  Under Windows you can supply an exports definitions file called
dllname-win.def: otherwise all entry points in objects (but
not libraries) supplied to R CMD SHLIB will be exported from the
DLL.  An example is stats-win.def for the stats package: a
CRAN example in package fastICA.
 If you feel tempted to read the source code and subvert these
mechanisms, please resist.  Far too much developer time has been wasted
in chasing down errors caused by failures to follow this documentation,
and even more by package authors demanding explanations as to why their
packages no longer work.
In particular, undocumented environment or make variables are
not for use by package writers and are subject to change without notice.
 
Next: Fortran I/O, Previous: Creating shared objects, Up: System and foreign language interfaces   [Contents][Index] Suppose we have the following hypothetical C++ library, consisting of
the two files X.h and X.cpp, and implementing the two
classes X and Y which we want to use in R.
 To use with R, the only thing we have to do is writing a wrapper
function and ensuring that the function is enclosed in
 For example,
 Compiling and linking should be done with the C++ compiler-linker
(rather than the C compiler-linker or the linker itself); otherwise, the
C++ initialization code (and hence the constructor of the static
variable Y) are not called.  On a properly configured system, one
can simply use
 to create the shared object, typically X.so (the file name
extension may be different on your platform).  Now starting R yields
 The R for Windows FAQ (rw-FAQ) contains details of how
to compile this example under Windows.
 Earlier version of this example used C++ iostreams: this is best
avoided.  There is no guarantee that the output will appear in the R
console, and indeed it will not on the R for Windows console.  Use
R code or the C entry points (see Printing) for all I/O if at all
possible.  Examples have been seen where merely loading a DLL that
contained calls to C++ I/O upset R’s own C I/O (for example by
resetting buffers on open files).
 Most R header files can be included within C++ programs but they
should not be included within an extern "C" block (as
they include system headers108).  It may not be possible
to include some R headers as they in turn include system header files
that may cause conflicts—if this happens, try defining
‘NO_C_HEADERS’ before including the R headers, and include C++
versions (such as ‘cmath’ and ‘cstdlib’) of the appropriate
headers yourself before the R headers. (Headers R.h and
Rmath.h support ‘NO_C_HEADERS’: the legacy header S.h
does not.  Header Rinternals.h does as from R 3.3.0.)
 By default header Rmath.h includes math.h or cmath.
Header R.h includes
 or their C++ equivalents directly or indirectly, and either
stddef.h or cstddef needs to be included before R.h
if ‘NO_C_HEADERS’ is defined.
 
Next: Linking to other packages, Previous: Interfacing C++ code, Up: System and foreign language interfaces   [Contents][Index] We have already warned against the use of C++ iostreams not least
because output is not guaranteed to appear on the R console, and this
warning applies equally to Fortran (77 or 9x) output to units *
and 6. See Printing from FORTRAN, which describes workarounds.
 In the past most Fortran compilers implemented I/O on top of the C I/O
system and so the two interworked successfully.  This was true of
g77, but it is less true of gfortran as used in
gcc 4.y.z.  In particular, any package that makes use of Fortran
I/O will when compiled on Windows interfere with C I/O: when the Fortran
I/O is initialized (typically when the package is loaded) the C
stdout and stderr are switched to LF line endings.
(Function init in file src/modules/lapack/init_win.c shows how to
mitigate this.)
 
Next: Handling R objects in C, Previous: Fortran I/O, Up: System and foreign language interfaces   [Contents][Index] It is not in general possible to link a DLL in package packA to a
DLL provided by package packB (for the security reasons mentioned
in dyn.load and dyn.unload, and also because some platforms
distinguish between shared objects and dynamic libraries), but it is on
Windows.
 Note that there can be tricky versioning issues here, as package
packB could be re-installed after package packA — it is
desirable that the API provided by package packB remains
backwards-compatible.
 Shipping a static library in package packB for other packages to
link to avoids most of the difficulties.
 
Next: Windows, Previous: Linking to other packages, Up: Linking to other packages   [Contents][Index] It is possible to link a shared object in package packA to a
library provided by package packB under limited circumstances
on a Unix-alike OS.  There are severe portability issues, so this is not
recommended for a distributed package.
 This is easiest if packB provides a static library
packB/lib/libpackB.a.  (Note using directory lib rather
than libs is conventional, and architecture-specific
sub-directories may be needed and are assumed in the sample code
below. The code in the static library will need to be compiled with
PIC flags on platforms where it matters.)  Then as the code from
package packB is incorporated when package packA is
installed, we only need to find the static library at install time for
package packA.  The only issue is to find package packB, and
for that we can ask R by something like (long lines broken for
display here)
 For a dynamic library packB/lib/libpackB.so
(packB/lib/libpackB.dylib on OS X: note that you cannot link to
a shared object, .so, on that platform) we could use
 This will work for installation, but very likely not when package
packB is loaded, as the path to package packB’s lib
directory is not in the ld.so109  search path.  You can arrange to
put it there before R is launched by setting (on some
platforms) LD_RUN_PATH or LD_LIBRARY_PATH or adding to the
ld.so cache (see man ldconfig).  On platforms that
support it, the path to the directory containing the dynamic library can
be hardcoded at install time (which assumes that the location of package
packB will not be changed nor the package updated to a changed
API).  On systems with the gcc or clang and the
GNU linker (e.g. Linux) and some others this can be done by
e.g.
 Some other systems (e.g. Solaris with its native linker) use
-Rdir rather than -rpath,dir (and this is accepted by
the compiler as well as the linker).
 It may be possible to figure out what is required semi-automatically
from the result of R CMD libtool --config (look for
‘hardcode’).
 Making headers provided by package packB available to the code to
be compiled in package packA can be done by the LinkingTo
mechanism (see Registering native routines).
 
Previous: Unix-alikes, Up: Linking to other packages   [Contents][Index] Suppose package packA wants to make use of compiled code provided
by packB in DLL packB/libs/exB.dll, possibly the package’s
DLL packB/libs/packB.dll.  (This can be extended to linking to
more than one package in a similar way.)  There are three issues to be
addressed:
 This is done by the LinkingTo mechanism (see Registering native routines).
 This needs an entry in Makevars.win of the form
 and one possibility is that <something> is the path to the
installed pkgB/libs directory.  To find that we need to ask R
where it is by something like
 Another possibility is to use an import library, shipping with package
packA an exports file exB.def.  Then Makevars.win
could contain
 and then installing package packA will make and use the import
library for exB.dll.  (One way to prepare the exports file is to
use pexports.exe.)
 If exB.dll was used by package packB (because it is in fact
packB.dll or packB.dll depends on it) and packB has
been loaded before packA, then nothing more needs to be done as
exB.dll will already be loaded into the R executable.  (This
is the most common scenario.)
 More generally, we can use the DLLpath argument to
library.dynam to ensure that exB.dll is found, for example
by setting
 Note that DLLpath can only set one path, and so for linking to
two or more packages you would need to resort to setting environment
variable PATH.
 
Next: Interface functions .Call and .External, Previous: Linking to other packages, Up: System and foreign language interfaces   [Contents][Index] Using C code to speed up the execution of an R function is often very
fruitful.  Traditionally this has been done via the .C
function in R.  However, if a user wants to write C code using
internal R data structures, then that can be done using the
.Call and .External functions.  The syntax for the calling
function in R in each case is similar to that of .C, but the
two functions have different C interfaces.  Generally the .Call
interface is simpler to use, but .External is a little more
general.


 A call to .Call is very similar to .C, for example
 The first argument should be a character string giving a C symbol name
of code that has already been loaded into R.  Up to 65 R objects
can passed as arguments.  The C side of the interface is
 A call to .External is almost identical
 but the C side of the interface is different, having only one argument
 Here args is a LISTSXP, a Lisp-style pairlist from which
the arguments can be extracted.
 In each case the R objects are available for manipulation via
a set of functions and macros defined in the header file
Rinternals.h or some S-compatibility macros110 defined
in Rdefines.h.  See Interface functions .Call and .External
for details on .Call and .External.
 Before you decide to use .Call or .External, you should
look at other alternatives.  First, consider working in interpreted R
code; if this is fast enough, this is normally the best option.  You
should also see if using .C is enough.  If the task to be
performed in C is simple enough involving only atomic vectors and
requiring no call to R, .C suffices. A great deal of useful
code was written using just .C before .Call and
.External were available.  These interfaces allow much more
control, but they also impose much greater responsibilities so need to
be used with care.  Neither .Call nor .External copy their
arguments: you should treat arguments you receive through these
interfaces as read-only.
 To handle R objects from within C code we use the macros and functions
that have been used to implement the core parts of R.  A
public111  subset of these is defined in the header file
Rinternals.h in the directory R_INCLUDE_DIR (default
R_HOME/include) that should be available on any R
installation.
 A substantial amount of R, including the standard packages, is
implemented using the functions and macros described here, so the R
source code provides a rich source of examples and “how to do it”: do
make use of the source code for inspirational examples.
 It is necessary to know something about how R objects are handled in
C code.  All the R objects you will deal with will be handled with
the type SEXP112, which is a
pointer to a structure with typedef SEXPREC.  Think of this
structure as a variant type that can handle all the usual types
of R objects, that is vectors of various modes, functions,
environments, language objects and so on.  The details are given later
in this section and in R Internal
Structures in R Internals, but for most
purposes the programmer does not need to know them.  Think rather of a
model such as that used by Visual Basic, in which R objects are
handed around in C code (as they are in interpreted R code) as the
variant type, and the appropriate part is extracted for, for example,
numerical calculations, only when it is needed.  As in interpreted R
code, much use is made of coercion to force the variant object to the
right type.
 
Next: Allocating storage, Previous: Handling R objects in C, Up: Handling R objects in C   [Contents][Index] We need to know a little about the way R handles memory allocation.
The memory allocated for R objects is not freed by the user; instead,
the memory is from time to time garbage collected.  That is, some
or all of the allocated memory not being used is freed or marked as
re-usable.
 The R object types are represented by a C structure defined by a
typedef SEXPREC in Rinternals.h.  It contains several
things among which are pointers to data blocks and to other
SEXPRECs.  A SEXP is simply a pointer to a SEXPREC.
 If you create an R object in your C code, you must tell R that you
are using the object by using the PROTECT macro on a pointer to
the object. This tells R that the object is in use so it is not
destroyed during garbage collection.  Notice that it is the object which
is protected, not the pointer variable.  It is a common mistake to
believe that if you invoked PROTECT(p) at some point then
p is protected from then on, but that is not true once a new
object is assigned to p.
 Protecting an R object automatically protects all the R objects
pointed to in the corresponding SEXPREC, for example all elements
of a protected list are automatically protected.
 The programmer is solely responsible for housekeeping the calls to
PROTECT.  There is a corresponding macro UNPROTECT that
takes as argument an int giving the number of objects to
unprotect when they are no longer needed.  The protection mechanism is
stack-based, so UNPROTECT(n) unprotects the last n
objects which were protected.  The calls to PROTECT and
UNPROTECT must balance when the user’s code returns.  R will
warn about "stack imbalance in .Call" (or .External) if
the housekeeping is wrong.
 Here is a small example of creating an R numeric vector in C code:
 Now, the reader may ask how the R object could possibly get removed
during those manipulations, as it is just our C code that is running.
As it happens, we can do without the protection in this example, but in
general we do not know (nor want to know) what is hiding behind the R
macros and functions we use, and any of them might cause memory to be
allocated, hence garbage collection and hence our object ab to be
removed. It is usually wise to err on the side of caution and assume
that any of the R macros and functions might remove the object.
 In some cases it is necessary to keep better track of whether protection
is really needed.  Be particularly aware of situations where a large
number of objects are generated.  The pointer protection stack has a
fixed size (default 10,000) and can become full.  It is not a good idea
then to just PROTECT everything in sight and UNPROTECT
several thousand objects at the end. It will almost invariably be
possible to either assign the objects as part of another object (which
automatically protects them) or unprotect them immediately after use.
 Protection is not needed for objects which R already knows are in
use.  In particular, this applies to function arguments.
 There is a less-used macro UNPROTECT_PTR(s) that unprotects
the object pointed to by the SEXP s, even if it is not the
top item on the pointer protection stack. This is rarely needed outside
the parser (the R sources currently have three examples, one in
src/main/plot3d.c).

 Sometimes an object is changed (for example duplicated, coerced or
grown) yet the current value needs to be protected.  For these cases
PROTECT_WITH_INDEX saves an index of the protection location that
can be used to replace the protected value using REPROTECT.


For example (from the internal code for optim)
 Note that it is dangerous to mix UNPROTECT_PTR with
PROTECT_WITH_INDEX, as the former changes the protection
locations of objects that were protected after the one being
unprotected.
 There is another way to avoid the affects of garbage collection: a call
to R_PreserveObject adds an object to an internal list of objects
not to be collects, and a subsequent call to R_ReleaseObject
removes it from that list.  This provides a way for objects which are
not returned as part of R objects to be protected across calls to
compiled code: on the other hand it becomes the user’s responsibility to
release them when they are no longer needed (and this often requires the
use of a finalizer).  It is less efficient that the normal protection
mechanism, and should be used sparingly.
 
Next: Details of R types, Previous: Garbage Collection, Up: Handling R objects in C   [Contents][Index] For many purposes it is sufficient to allocate R objects and
manipulate those.  There are quite a few allocXxx functions
defined in Rinternals.h—you may want to explore them.
 One that is commonly used is allocVector, the C-level equivalent
of R-level vector() and its wrappers such as integer()
and character().  One distinction is that whereas the R
functions always initialize the elements of the vector,
allocVector only does so for lists, expressions and character
vectors (the cases where the elements are themselves R objects).
 If storage is required for C objects during the calculations this is
best allocating by calling R_alloc; see Memory allocation.
All of these memory allocation routines do their own error-checking, so
the programmer may assume that they will raise an error and not return
if the memory cannot be allocated.
 
Next: Attributes, Previous: Allocating storage, Up: Handling R objects in C   [Contents][Index] Users of the Rinternals.h macros will need to know how the R
types are known internally.  The different R data types are
represented in C by SEXPTYPE.  Some of these are familiar from
R and some are internal data types.  The usual R object modes are
given in the table.
 Among the important internal SEXPTYPEs are LANGSXP,
CHARSXP, PROMSXP, etc.  (N.B.: although it is
possible to return objects of internal types, it is unsafe to do so as
assumptions are made about how they are handled which may be violated at
user-level evaluation.)  More details are given in R Internal Structures in R Internals.
 Unless you are very sure about the type of the arguments, the code
should check the data types.  Sometimes it may also be necessary to
check data types of objects created by evaluating an R expression in
the C code.  You can use functions like isReal, isInteger
and isString to do type checking.  See the header file
Rinternals.h for definitions of other such functions.  All of
these take a SEXP as argument and return 1 or 0 to indicate
TRUE or FALSE.
 What happens if the SEXP is not of the correct type?  Sometimes
you have no other option except to generate an error.  You can use the
function error for this.  It is usually better to coerce the
object to the correct type.  For example, if you find that an
SEXP is of the type INTEGER, but you need a REAL
object, you can change the type by using
 Protection is needed as a new object is created; the object
formerly pointed to by the SEXP is still protected but now
unused.113
 All the coercion functions do their own error-checking, and generate
NAs with a warning or stop with an error as appropriate.
 Note that these coercion functions are not the same as calling
as.numeric (and so on) in R code, as they do not dispatch on
the class of the object.  Thus it is normally preferable to do the
coercion in the calling R code.
 So far we have only seen how to create and coerce R objects from C
code, and how to extract the numeric data from numeric R vectors.
These can suffice to take us a long way in interfacing R objects to
numerical algorithms, but we may need to know a little more to create
useful return objects.
 
Next: Classes, Previous: Details of R types, Up: Handling R objects in C   [Contents][Index] Many R objects have attributes: some of the most useful are classes
and the dim and dimnames that mark objects as matrices or
arrays.  It can also be helpful to work with the names attribute
of vectors.
 To illustrate this, let us write code to take the outer product of two
vectors (which outer and %o% already do).  As usual the
R code is simple
 where we expect x and y to be numeric vectors (possibly
integer), possibly with names.  This time we do the coercion in the
calling R code.
 C code to do the computations is
 Note the way REAL is used: as it is a function call it can be
considerably faster to store the result and index that.
 However, we would like to set the dimnames of the result.  We can use
 This example introduces several new features.  The getAttrib and
setAttrib


functions get and set individual attributes.  Their second argument is a
SEXP defining the name in the symbol table of the attribute we
want; these and many such symbols are defined in the header file
Rinternals.h.
 There are shortcuts here too: the functions namesgets,
dimgets and dimnamesgets are the internal versions of the
default methods of names<-, dim<- and dimnames<-
(for vectors and arrays), and there are functions such as
GetMatrixDimnames and GetArrayDimnames.
 What happens if we want to add an attribute that is not pre-defined? We
need to add a symbol for it via a call to

install.  Suppose for illustration we wanted to add an attribute
"version" with value 3.0.  We could use
 Using install when it is not needed is harmless and provides a
simple way to retrieve the symbol from the symbol table if it is already
installed. However, the lookup takes a non-trivial amount of time, so
consider code such as
 if it is to be done frequently.
 This example can be simplified by another convenience function:
 
Next: Handling lists, Previous: Attributes, Up: Handling R objects in C   [Contents][Index] In R the class is just the attribute named "class" so it can
be handled as such, but there is a shortcut classgets.  Suppose
we want to give the return value in our example the class "mat".
We can use
 As the value is a character vector, we have to know how to create that
from a C character array, which we do using the function
mkChar.
 
Next: Handling character data, Previous: Classes, Up: Handling R objects in C   [Contents][Index] Some care is needed with lists, as R moved early on from using
LISP-like lists (now called “pairlists”) to S-like generic vectors.
As a result, the appropriate test for an object of mode list is
isNewList, and we need allocVector(VECSXP, n) and
not allocList(n).
 List elements can be retrieved or set by direct access to the elements
of the generic vector.  Suppose we have a list object
 Then we can access a$g as a[[2]] by
 This can rapidly become tedious, and the following function (based on
one in package stats) is very useful:
 and enables us to say
 
Next: Finding and setting variables, Previous: Handling lists, Up: Handling R objects in C   [Contents][Index] R character vectors are stored as STRSXPs, a vector type like
VECSXP where every element is of type CHARSXP.  The
CHARSXP elements of STRSXPs are accessed using
STRING_ELT and SET_STRING_ELT.
 CHARSXPs are read-only objects and must never be modified.  In
particular, the C-style string contained in a CHARSXP should be
treated as read-only and for this reason the CHAR function used
to access the character data of a CHARSXP returns (const
char *) (this also allows compilers to issue warnings about improper
use).  Since CHARSXPs are immutable, the same CHARSXP can
be shared by any STRSXP needing an element representing the same
string. R maintains a global cache of CHARSXPs so that there
is only ever one CHARSXP representing a given string in memory.
 You can obtain a CHARSXP by calling mkChar and providing a
nul-terminated C-style string.  This function will return a pre-existing
CHARSXP if one with a matching string already exists, otherwise
it will create a new one and add it to the cache before returning it to
you.   The variant mkCharLen can be used to create a
CHARSXP from part of a buffer and will ensure null-termination.
 Note that R character strings are restricted to 2^31 - 1
bytes, and hence so should the input to mkChar be (C allows
longer strings on 64-bit platforms).
 
Next: Some convenience functions, Previous: Handling character data, Up: Handling R objects in C   [Contents][Index] It will be usual that all the R objects needed in our C computations
are passed as arguments to .Call or .External, but it is
possible to find the values of R objects from within the C given
their names.  The following code is the equivalent of get(name,
envir = rho).
 The main work is done by

findVar, but to use it we need to install name as a name
in the symbol table.  As we wanted the value for internal use, we return
NULL.
 Similar functions with syntax
 can be used to assign values to R variables.  defineVar
creates a new binding or changes the value of an existing binding in the
specified environment frame; it is the analogue of assign(symbol,
value, envir = rho, inherits = FALSE), but unlike assign,
defineVar does not make a copy of the object
value.114  setVar searches for an existing
binding for symbol in rho or its enclosing environments.
If a binding is found, its value is changed to value.  Otherwise,
a new binding with the specified value is created in the global
environment.  This corresponds to assign(symbol, value, envir =
rho, inherits = TRUE).
 
Next: Named objects and copying, Previous: Finding and setting variables, Up: Handling R objects in C   [Contents][Index] Some operations are done so frequently that there are convenience
functions to handle them. (All these are provided via the header file
Rinternals.h.)
 Suppose we wanted to pass a single logical argument
ignore_quotes: we could use
 which will do any coercion needed (at least from a vector argument), and
return NA_LOGICAL if the value passed was NA or coercion
failed.  There are also asInteger, asReal and
asComplex.  The function asChar returns a CHARSXP.
All of these functions ignore any elements of an input vector after the
first.
 To return a length-one real vector we can use
 and there are versions of this for all the atomic vector types (those for
a length-one character vector being ScalarString with argument a
CHARSXP and mkString with argument const char *).
 Some of the isXXXX functions differ from their apparent
R-level counterparts: for example isVector is true for any
atomic vector type (isVectorAtomic) and for lists and expressions
(isVectorList) (with no check on attributes).  isMatrix is
a test of a length-2 "dim" attribute.
 There are a series of small macros/functions to help construct pairlists
and language objects (whose internal structures just differ by
SEXPTYPE).  Function CONS(u, v) is the basic building
block: it constructs a pairlist from u followed by v
(which is a pairlist or R_NilValue).  LCONS is a variant
that constructs a language object.  Functions list1 to
list5 construct a pairlist from one to five items, and
lang1 to lang6 do the same for a language object (a
function to call plus zero to five arguments).  Functions elt and
lastElt find the ith element and the last element of a
pairlist, and nthcdr returns a pointer to the nth position
in the pairlist (whose CAR is the nth item).
 Functions str2type and type2str map R
length-one character strings to and from SEXPTYPE numbers, and
type2char maps numbers to C character strings.
 
Previous: Some convenience functions, Up: Some convenience functions   [Contents][Index] There is quite a collection of functions that may be used in your C code
if you are willing to adapt to rare “API” changes.
These typically contain “workhorses” of their R counterparts.
 Functions any_duplicated and any_duplicated3 are fast
versions of R’s any(duplicated(.)).
 Function R_compute_identical corresponds to R’s identical function.
 
Previous: Some convenience functions, Up: Handling R objects in C   [Contents][Index] When assignments are done in R such as
 the named object is not necessarily copied, so after those two
assignments y and x are bound to the same SEXPREC
(the structure a SEXP points to).  This means that any code which
alters one of them has to make a copy before modifying the copy if the
usual R semantics are to apply.  Note that whereas .C and
.Fortran do copy their arguments (unless the dangerous dup
= FALSE is used), .Call and .External do not.  So
duplicate is commonly called on arguments to .Call before
modifying them.
 However, at least some of this copying is unneeded.  In the first
assignment shown, x <- 1:10, R first creates an object with
value 1:10 and then assigns it to x but if x is
modified no copy is necessary as the temporary object with value
1:10 cannot be referred to again.  R distinguishes between
named and unnamed objects via a field in a SEXPREC that
can be accessed via the macros NAMED and SET_NAMED.  This
can take values
 The object is not bound to any symbol
 The object has been bound to exactly one symbol
 The object has potentially been bound to two or more symbols, and one
should act as if another variable is currently bound to this value.
 Note the past tenses: R does not do full reference counting and there
may currently be fewer bindings.
 It is safe to modify the value of any SEXP for which
NAMED(foo) is zero, and if NAMED(foo) is two, the value
should be duplicated (via a call to duplicate) before any
modification.  Note that it is the responsibility of the author of the
code making the modification to do the duplication, even if it is
x whose value is being modified after y <- x.
 The case NAMED(foo) == 1 allows some optimization, but it can be
ignored (and duplication done whenever NAMED(foo) > 0).  (This
optimization is not currently usable in user code.)  It is intended
for use within replacement functions.  Suppose we used
 which is computed as
 Then inside "foo<-" the object pointing to the current value of
x will have NAMED(foo) as one, and it would be safe to
modify it as the only symbol bound to it is x and that will be
rebound immediately.  (Provided the remaining code in "foo<-"
make no reference to x, and no one is going to attempt a direct
call such as y <- "foo<-"(x).)
 This mechanism is likely to be replaced in future versions of R.
 
Next: Evaluating R expressions from C, Previous: Handling R objects in C, Up: System and foreign language interfaces   [Contents][Index] In this section we consider the details of the R/C interfaces.
 These two interfaces have almost the same functionality. .Call is
based on the interface of the same name in S version 4, and
.External is based on R’s .Internal.  .External
is more complex but allows a variable number of arguments.
 
Next: Calling .External, Previous: Interface functions .Call and .External, Up: Interface functions .Call and .External   [Contents][Index] Let us convert our finite convolution example to use .Call.  The
calling function in R is
 which could hardly be simpler, but as we shall see all the type
coercion is transferred to the C code, which is
 
Next: Missing and special values, Previous: Calling .Call, Up: Interface functions .Call and .External   [Contents][Index] We can use the same example to illustrate .External.  The R
code changes only by replacing .Call by .External
 but the main change is how the arguments are passed to the C code, this
time as a single SEXP.  The only change to the C code is how we handle
the arguments.
 Once again we do not need to protect the arguments, as in the R side
of the interface they are objects that are already in use.  The macros
 provide convenient ways to access the first four arguments.  More
generally we can use the


CDR and CAR macros as in
 which clearly allows us to extract an unlimited number of arguments
(whereas .Call has a limit, albeit at 65 not a small one).
 More usefully, the .External interface provides an easy way to
handle calls with a variable number of arguments, as length(args)
will give the number of arguments supplied (of which the first is
ignored).  We may need to know the names (‘tags’) given to the actual
arguments, which we can by using the TAG macro and using
something like the following example, that prints the names and the first
value of its arguments if they are vector types.
 This can be called by the wrapper function
 Note that this style of programming is convenient but not necessary, as
an alternative style is
 The (very similar) C code is in the scripts.
 
Previous: Calling .External, Up: Interface functions .Call and .External   [Contents][Index] One piece of error-checking the .C call does (unless NAOK
is true) is to check for missing (NA) and IEEE special
values (Inf, -Inf and NaN) and give an error if any
are found.  With the .Call interface these will be passed to our
code.  In this example the special values are no problem, as
IEC60559 arithmetic will handle them correctly.  In the current
implementation this is also true of NA as it is a type of
NaN, but it is unwise to rely on such details.  Thus we will
re-write the code to handle NAs using macros defined in
R_ext/Arith.h included by R.h.
 The code changes are the same in any of the versions of convolve2
or convolveE:
 Note that the ISNA macro, and the similar macros ISNAN
(which checks for NaN or NA) and R_FINITE (which is
false for NA and all the special values), only apply to numeric
values of type double.  Missingness of integers, logicals and
character strings can be tested by equality to the constants
NA_INTEGER, NA_LOGICAL and NA_STRING.  These and
NA_REAL can be used to set elements of R vectors to NA.
 The constants R_NaN, R_PosInf and R_NegInf can be
used to set doubles to the special values.
 
Next: Parsing R code from C, Previous: Interface functions .Call and .External, Up: System and foreign language interfaces   [Contents][Index] The main function we will use is
 the equivalent of the interpreted R code eval(expr, envir =
rho) (so rho must be an environment), although we can also make
use of findVar, defineVar and findFun (which
restricts the search to functions).
 To see how this might be applied, here is a simplified internal version
of lapply for expressions, used as
 with C code
 It would be closer to lapply if we could pass in a function
rather than an expression.  One way to do this is via interpreted
R code as in the next example, but it is possible (if somewhat
obscure) to do this in C code.  The following is based on the code in
src/main/optimize.c.
 used by
 Function lang2 creates an executable pairlist of two elements, but
this will only be clear to those with a knowledge of a LISP-like
language.
 As a more comprehensive example of constructing an R call in C code
and evaluating, consider the following fragment of
printAttributes in src/main/print.c.
 At this point CAR(a) is the R object to be printed, the
current attribute.  There are three steps: the call is constructed as
a pairlist of length 3, the list is filled in, and the expression
represented by the pairlist is evaluated.
 A pairlist is quite distinct from a generic vector list, the only
user-visible form of list in R.  A pairlist is a linked list (with
CDR(t) computing the next entry), with items (accessed by
CAR(t)) and names or tags (set by SET_TAG).  In this call
there are to be three items, a symbol (pointing to the function to be
called) and two argument values, the first unnamed and the second named.
Setting the type to LANGSXP makes this a call which can be evaluated.
 
Next: Calculating numerical derivatives, Previous: Evaluating R expressions from C, Up: Evaluating R expressions from C   [Contents][Index] In this section we re-work the example of Becker, Chambers & Wilks (1988,
pp.~205–10) on finding a zero of a univariate function.  The R code
and an example are
 where this time we do the coercion and error-checking in the R code.
The C code is
 
Previous: Zero-finding, Up: Evaluating R expressions from C   [Contents][Index] We will use a longer example (by Saikat DebRoy) to illustrate the use of
evaluation and .External.  This calculates numerical derivatives,
something that could be done as effectively in interpreted R code but
may be needed as part of a larger C calculation.
 An interpreted R version and an example are
 where expr is an expression, theta a character vector of
variable names and rho the environment to be used.
 For the compiled version the call from R will be
 with example usage
 Note the need to quote the expression to stop it being evaluated in the
caller.
 Here is the complete C code which we will explain section by section.
 The code to handle the arguments is
 Note that we check for correct types of theta and rho but
do not check the type of expr.  That is because eval can
handle many types of R objects other than EXPRSXP.  There is
no useful coercion we can do, so we stop with an error message if the
arguments are not of the correct mode.
 The first step in the code is to evaluate the expression in the
environment rho, by
 We then allocate space for the calculated derivative by
 The first argument to allocMatrix gives the SEXPTYPE of
the matrix: here we want it to be REALSXP.  The other two
arguments are the numbers of rows and columns.  (Note that LENGTH
is intended to be used for vectors: length is more generally
applicable.)
 Here, we are entering a for loop.  We loop through each of the
variables.  In the for loop, we first create a symbol
corresponding to the i’th element of the STRSXP
theta.  Here, STRING_ELT(theta, i) accesses the
i’th element of the STRSXP theta.  Macro
CHAR() extracts the actual character
representation115 of it: it returns a pointer.  We then
install the name and use findVar to find its value.
 We first extract the real value of the parameter, then calculate
delta, the increment to be used for approximating the numerical
derivative.  Then we change the value stored in par (in
environment rho) by delta and evaluate expr in
environment rho again.  Because we are directly dealing with
original R memory locations here, R does the evaluation for the
changed parameter value.
 Now, we compute the i’th column of the gradient matrix.  Note how
it is accessed: R stores matrices by column (like FORTRAN).
 First we add column names to the gradient matrix.  This is done by
allocating a list (a VECSXP) whose first element, the row names,
is NULL (the default) and the second element, the column names,
is set as theta.  This list is then assigned as the attribute
having the symbol R_DimNamesSymbol.  Finally we set the gradient
matrix as the gradient attribute of ans, unprotect the remaining
protected locations and return the answer ans.
 
Next: External pointers and weak references, Previous: Evaluating R expressions from C, Up: System and foreign language interfaces   [Contents][Index] Suppose an R extension want to accept an R expression from the
user and evaluate it.  The previous section covered evaluation, but the
expression will be entered as text and needs to be parsed first.  A
small part of R’s parse interface is declared in header file
R_ext/Parse.h116.
 An example of the usage can be found in the (example) Windows package
windlgs included in the R source tree.  The essential part is
 Note that a single line of text may give rise to more than one R
expression.
 R_ParseVector is essentially the code used to implement
parse(text=) at R level.  The first argument is a character
vector (corresponding to text) and the second the maximal
number of expressions to parse (corresponding to n).  The third
argument is a pointer to a variable of an enumeration type, and it is
normal (as parse does) to regard all values other than
PARSE_OK as an error.  Other values which might be returned are
PARSE_INCOMPLETE (an incomplete expression was found) and
PARSE_ERROR (a syntax error), in both cases the value returned
being R_NilValue. The fourth argument is a length one character
vector to be used as a filename in error messages, a srcfile
object or the R NULL object (as in the example above). If a
srcfile object was used, a srcref attribute would be
attached to the result, containing a list of srcref objects of
the same length as the expression, to allow it to be echoed with its
original formatting.
 
Previous: Parsing R code from C, Up: Parsing R code from C   [Contents][Index] The source references added by the parser are recorded by R’s evaluator
as it evaluates code. Two functions
make these available to debuggers running C code:



 This function checks R_Srcref and the current evaluation stack
for entries that contain source reference information.  The
skip argument tells how many source references to skip before
returning the SEXP of the srcref object, counting from
the top of the stack.  If skip < 0, abs(skip) locations
are counted up from the bottom of the stack. If too few or no source
references are found, NULL is returned.
 This function extracts the filename from the source reference for
display, returning a length 1 character vector containing the
filename.  If no name is found, "" is returned.
 
Next: Vector accessor functions, Previous: Parsing R code from C, Up: System and foreign language interfaces   [Contents][Index] The SEXPTYPEs EXTPTRSXP and WEAKREFSXP can be
encountered at R level, but are created in C code.
 External pointer SEXPs are intended to handle references to C
structures such as ‘handles’, and are used for this purpose in package
RODBC for example.  They are unusual in their copying semantics in
that when an R object is copied, the external pointer object is not
duplicated.  (For this reason external pointers should only be used as
part of an object with normal semantics, for example an attribute or an
element of a list.)
 An external pointer is created by
 where p is the pointer (and hence this cannot portably be a
function pointer), and tag and prot are references to
ordinary R objects which will remain in existence (be protected from
garbage collection) for the lifetime of the external pointer object.  A
useful convention is to use the tag field for some form of type
identification and the prot field for protecting the memory that
the external pointer represents, if that memory is allocated from the
R heap.  Both tag and prot can be R_NilValue,
and often are.
 The elements of an external pointer can be accessed and set via
 Clearing a pointer sets its value to the C NULL pointer.
 An external pointer object can have a finalizer, a piece of code
to be run when the object is garbage collected.  This can be R code
or C code, and the various interfaces are, respectively.
 The R function indicated by fun should be a function of a
single argument, the object to be finalized.  R does not perform a
garbage collection when shutting down, and the onexit argument of
the extended forms can be used to ask that the finalizer be run during a
normal shutdown of the R session.  It is suggested that it is good
practice to clear the pointer on finalization.
 The only R level function for interacting with external pointers is
reg.finalizer which can be used to set a finalizer.
 It is probably not a good idea to allow an external pointer to be
saved and then reloaded, but if this happens the pointer will be
set to the C NULL pointer.
 Finalizers can be run at many places in the code base and much of it,
including the R interpreter, is not re-entrant.  So great care is
needed in choosing the code to be run in a finalizer. As from R 3.0.3
finalizers are marked to be run at garbage collection but only run at a
somewhat safe point thereafter.
 Weak references are used to allow the programmer to maintain information
on entities without preventing the garbage collection of the entities
once they become unreachable.
 A weak reference contains a key and a value.  The value is reachable is
if it either reachable directly or via weak references with reachable
keys.  Once a value is determined to be unreachable during garbage
collection, the key and value are set to R_NilValue and the
finalizer will be run later in the garbage collection.
 Weak reference objects are created by one of
 where the R or C finalizer are specified in exactly the same way as
for an external pointer object (whose finalization interface is
implemented via weak references).
 The parts can be accessed via
 A toy example of the use of weak references can be found at
homepage.stat.uiowa.edu/~luke/R/references/weakfinex.html,
but that is used to add finalizers to external pointers which can now be
done more directly.  At the time of writing no CRAN or
Bioconductor package uses weak references.
 
Previous: External pointers and weak references, Up: External pointers and weak references   [Contents][Index] Package RODBC uses external pointers to maintain its
channels, connections to databases.  There can be several
connections open at once, and the status information for each is stored
in a C structure (pointed to by this_handle) in the code extract
below) that is returned via an external pointer as part of the RODBC
‘channel’ (as the "handle_ptr" attribute).  The external pointer
is created by
 Note the symbol given to identify the usage of the external pointer, and
the use of the finalizer.  Since the final argument when registering the
finalizer is TRUE, the finalizer will be run at the of the
R session (unless it crashes).  This is used to close and clean up
the connection to the database.  The finalizer code is simply
 Clearing the pointer and checking for a NULL pointer avoids any
possibility of attempting to close an already-closed channel.
 R’s connections provide another example of using external pointers,
in that case purely to be able to use a finalizer to close and destroy the
connection if it is no longer is use.
 
Next: Character encoding issues, Previous: External pointers and weak references, Up: System and foreign language interfaces   [Contents][Index] The vector accessors like REAL and INTEGER and
VECTOR_ELT are functions when used in R extensions.
(For efficiency they are macros when used in the R source code, apart
from SET_STRING_ELT and SET_VECTOR_ELT which are always
functions.)
 The accessor functions check that they are being used on an appropriate
type of SEXP.
 If efficiency is essential, the macro versions of the accessors can be
obtained by defining ‘USE_RINTERNALS’ before including
Rinternals.h.  If you find it necessary to do so, please do test
that your code compiles without ‘USE_RINTERNALS’ defined, as this
provides a stricter test that the accessors have been used correctly.
Note too that the use of ‘USE_RINTERNALS’ when the header is
included in C++ code is not supported: doing so may use C99 features
which are not necessarily supported by the C++ compiler.  Nor is use
with Rdefines.h supported.
 
Previous: Vector accessor functions, Up: System and foreign language interfaces   [Contents][Index] CHARSXPs can be marked as coming from a known encoding (Latin-1
or UTF-8).  This is mainly intended for human-readable output, and most
packages can just treat such CHARSXPs as a whole.  However, if
they need to be interpreted as characters or output at C level then it
would normally be correct to ensure that they are converted to the
encoding of the current locale: this can be done by accessing the data
in the CHARSXP by translateChar rather than by
CHAR.  If re-encoding is needed this allocates memory with
R_alloc which thus persists to the end of the
.Call/.External call unless vmaxset is used
(see Transient storage allocation).
 There is a similar function translateCharUTF8 which converts to
UTF-8: this has the advantage that a faithful translation is almost
always possible (whereas only a few languages can be represented in the
encoding of the current locale unless that is UTF-8).
 There is a public interface to the encoding marked on CHARXSXPs
via
 Only CE_UTF8 and CE_LATIN1 are marked on CHARSXPs
(and so Rf_getCharCE will only return one of the first three),
and these should only be used on non-ASCII strings.  Value
CE_SYMBOL is used internally to indicate Adobe Symbol encoding.
Value CE_ANY is used to indicate a character string that will not
need re-encoding – this is used for character strings known to be in
ASCII, and can also be used as an input parameter where the
intention is that the string is treated as a series of bytes.  (See the
comments under mkChar about the length of input allowed.)
 Function
 can be used to re-encode character strings: like translateChar it
returns a string allocated by R_alloc.  This can translate from
CE_SYMBOL to CE_UTF8, but not conversely.  Argument
subst controls what to do with untranslatable characters or
invalid input: this is done byte-by-byte with 1 indicates to
output hex of the form <a0>, and 2 to replace by .,
with any other value causing the byte to produce no output.
 There is also
 to create marked character strings of a given length.
 
Next: Generic functions and methods, Previous: System and foreign language interfaces, Up: Top   [Contents][Index] There are a large number of entry points in the R executable/DLL that
can be called from C code (and some that can be called from FORTRAN
code).  Only those documented here are stable enough that they will only
be changed with considerable notice.
 The recommended procedure to use these is to include the header file
R.h in your C code by
 This will include several other header files from the directory
R_INCLUDE_DIR/R_ext, and there are other header files
there that can be included too, but many of the features they contain
should be regarded as undocumented and unstable.
 Most of these header files, including all those included by R.h,
can be used from C++ code.
 Note: Because R re-maps many of its external names to avoid clashes with
user code, it is essential to include the appropriate header
files when using these entry points.
 This remapping can cause problems117, and can be eliminated by defining R_NO_REMAP and
prepending ‘Rf_’ to all the function names used from
Rinternals.h and R_ext/Error.h.  These problems can
usually be avoided by including other headers (such as system headers
and those for external software used by the package) before R.h.
 We can classify the entry points as
 Entry points which are documented in this manual and declared in an
installed header file.  These can be used in distributed packages and
will only be changed after deprecation.
 Entry points declared in an installed header file that are exported
on all R platforms but are not documented and subject to change
without notice.
 Entry points that are used when building R and exported on all R
platforms but are not declared in the installed header files.
Do not use these in distributed code.
 Entry points that are where possible (Windows and some modern Unix-alike
compilers/loaders when using R as a shared library) not exported.
 
Next: Error handling, Previous: The R API, Up: The R API   [Contents][Index] There are two types of memory allocation available to the C programmer,
one in which R manages the clean-up and the other in which user
has full control (and responsibility).
 
Next: User-controlled memory, Previous: Memory allocation, Up: Memory allocation   [Contents][Index] Here R will reclaim the memory at the end of the call to .C,
.Call or .External.  Use
 which allocates n units of size bytes each.  A typical usage
(from package stats) is
 (size_t is defined in stddef.h which the header defining
R_alloc includes.)
 There is a similar call, S_alloc (for compatibility with older
versions of S) which zeroes the memory allocated,
 and
 which changes the allocation size from old to new units, and
zeroes the additional units.
 For compatibility with current versions of S, header S.h
(only) defines wrapper macros equivalent to
 This memory is taken from the heap, and released at the end of the
.C, .Call or .External call.  Users can also manage
it, by noting the current position with a call to vmaxget and
subsequently clearing memory allocated by a call to vmaxset. An
example might be
 This is only recommended for experts.
 Note that this memory will be freed on error or user interrupt
(if allowed: see Allowing interrupts).
 Note that although n is size_t, there may be limits imposed
by R’s internal allocation mechanism.  These will only come into play
on 64-bit systems, where the limit for n prior to R 3.0.0 was
just under 16Gb.
 The memory returned is only guaranteed to be aligned as required for
double pointers: take precautions if casting to a pointer which
needs more.  As from R 3.2.0 there is also
 which is guaranteed to have the 16-byte alignment needed for long
double pointers on some platforms.
 These functions should only be used in code called by .C etc,
never from front-ends.  They are not thread-safe.
 
Previous: Transient storage allocation, Up: Memory allocation   [Contents][Index] The other form of memory allocation is an interface to malloc,
the interface providing R error handling.  This memory lasts until
freed by the user and is additional to the memory allocated for the R
workspace.
 The interface functions are
 providing analogues of calloc, realloc and free.
If there is an error during allocation it is handled by R, so if
these routines return the memory has been successfully allocated or
freed.  Free will set the pointer p to NULL.  (Some
but not all versions of S do so.)
 Users should arrange to Free this memory when no longer needed,
including on error or user interrupt.  This can often be done most
conveniently from an on.exit action in the calling R function
– see pwilcox for an example.
 Do not assume that memory allocated by Calloc/Realloc
comes from the same pool as used by malloc: in particular do not
use free or strdup with it.
 Memory obtained by these functions should be aligned in the same way as
malloc, that is ‘suitably aligned for any kind of variable’.
 These entry points need to be prefixed by R_ if
STRICT_R_HEADERS has been defined.
 
Next: Random numbers, Previous: Memory allocation, Up: The R API   [Contents][Index] The basic error handling routines are the equivalents of stop and
warning in R code, and use the same interface.
 These have the same call sequences as calls to printf, but in the
simplest case can be called with a single character string argument
giving the error message. (Don’t do this if the string contains ‘%’
or might otherwise be interpreted as a format.)
 If STRICT_R_HEADERS is not defined there is also an
S-compatibility interface which uses calls of the form
 the last two being the forms available in all S versions.  Here
‘......’ is a set of arguments to printf, so can be a string
or a format string followed by arguments separated by commas.
 
Previous: Error handling, Up: Error handling   [Contents][Index] There are two interface function provided to call error and
warning from FORTRAN code, in each case with a simple character
string argument.  They are defined as
 Messages of more than 255 characters are truncated, with a warning.
 
Next: Missing and IEEE values, Previous: Error handling, Up: The R API   [Contents][Index] The interface to R’s internal random number generation routines is
 giving one uniform, normal or exponential pseudo-random variate.
However, before these are used, the user must call
 and after all the required variates have been generated, call
 These essentially read in (or create) .Random.seed and write it
out after use.
 File S.h defines seed_in and seed_out for
S-compatibility rather than GetRNGstate and
PutRNGstate.  These take a long * argument which is
ignored.
 The random number generator is private to R; there is no way to
select the kind of RNG or set the seed except by evaluating calls to the
R functions.
 The C code behind R’s rxxx functions can be accessed by
including the header file Rmath.h; See Distribution functions.  Those calls generate a single variate and should also be
enclosed in calls to GetRNGstate and PutRNGstate.
 
Next: Printing, Previous: Random numbers, Up: The R API   [Contents][Index] A set of functions is provided to test for NA, Inf,
-Inf and NaN.  These functions are accessed via macros:
 and via function R_IsNaN which is true for NaN but not
NA.
 Do use R_FINITE rather than isfinite or finite; the
latter is often mendacious and isfinite is only available on a
some platforms, on which R_FINITE is a macro expanding to
isfinite.
 Currently in C code ISNAN is a macro calling isnan.
(Since this gives problems on some C++ systems, if the R headers is
called from C++ code a function call is used.)
 You can check for Inf or -Inf by testing equality to
R_PosInf or R_NegInf, and set (but not test) an NA
as NA_REAL.
 All of the above apply to double variables only.  For integer
variables there is a variable accessed by the macro NA_INTEGER
which can used to set or test for missingness.
 
Next: Calling C from FORTRAN and vice versa, Previous: Missing and IEEE values, Up: The R API   [Contents][Index] The most useful function for printing from a C routine compiled into
R is Rprintf.  This is used in exactly the same way as
printf, but is guaranteed to write to R’s output (which might
be a GUI console rather than a file, and can be re-directed by
sink).  It is wise to write complete lines (including the
"\n") before returning to R.  It is defined in
R_ext/Print.h.
 The function REprintf is similar but writes on the error stream
(stderr) which may or may not be different from the standard
output stream.
 Functions Rvprintf and REvprintf are analogues using the
vprintf interface.  Because that is a C99 interface, they are
only defined by R_ext/Print.h in C++ code if the macro
R_USE_C99_IN_CXX is defined when it is included.
 Another circumstance when it may be important to use these functions is
when using parallel computation on a cluster of computational nodes, as
their output will be re-directed/logged appropriately.
 
Previous: Printing, Up: Printing   [Contents][Index] On many systems FORTRAN write and print statements can be
used, but the output may not interleave well with that of C, and will be
invisible on GUI interfaces.  They are not portable and best
avoided.
 Three subroutines are provided to ease the output of information from
FORTRAN code.
 Here label is a character label of up to 255 characters,
nchar is its length (which can be -1 if the whole label is
to be used), and data is an array of length at least ndata
of the appropriate type (double precision, real and
integer respectively).  These routines print the label on one
line and then print data as if it were an R vector on
subsequent line(s).  They work with zero ndata, and so can be used
to print a label alone.
 
Next: Numerical analysis subroutines, Previous: Printing, Up: The R API   [Contents][Index] Naming conventions for symbols generated by FORTRAN differ by platform:
it is not safe to assume that FORTRAN names appear to C with a trailing
underscore.  To help cover up the platform-specific differences there is
a set of macros that should be used.
 to define a function in C to be called from FORTRAN
 to declare a FORTRAN routine in C before use
 to call a FORTRAN routine from C
 to declare a FORTRAN common block in C
 to access a FORTRAN common block from C
 On most current platforms these are all the same, but it is unwise to
rely on this.  Note that names with underscores are not legal in FORTRAN
77, and are not portably handled by the above macros.  (Also, all
FORTRAN names for use by R are lower case, but this is not enforced
by the macros.)
 For example, suppose we want to call R’s normal random numbers from
FORTRAN.  We need a C wrapper along the lines of
 to be called from FORTRAN as in
 Note that this is not guaranteed to be portable, for the return
conventions might not be compatible between the C and FORTRAN compilers
used.  (Passing values via arguments is safer.)
 The standard packages, for example stats, are a rich source of
further examples.
 Passing character strings from C to FORTRAN 77 or vice versa is
not portable (and to Fortran 90 or later is even less so).  We have
found that it helps to ensure that a C string to be passed is followed
by several nuls (and not just the one needed as a C terminator).
But for maximal portability character strings in FORTRAN should be
avoided.
 
Next: Optimization, Previous: Calling C from FORTRAN and vice versa, Up: The R API   [Contents][Index] R contains a large number of mathematical functions for its own use,
for example numerical linear algebra computations and special functions.
 The header files R_ext/BLAS.h, R_ext/Lapack.h and
R_ext/Linpack.h contains declarations of the BLAS, LAPACK and
LINPACK linear algebra functions included in R.  These are expressed
as calls to FORTRAN subroutines, and they will also be usable from
users’ FORTRAN code.  Although not part of the official API,
this set of subroutines is unlikely to change (but might be
supplemented).
 The header file Rmath.h lists many other functions that are
available and documented in the following subsections. Many of these are
C interfaces to the code behind R functions, so the R function
documentation may give further details.
 
Next: Mathematical functions, Previous: Numerical analysis subroutines, Up: Numerical analysis subroutines   [Contents][Index] The routines used to calculate densities, cumulative distribution
functions and quantile functions for the standard statistical
distributions are available as entry points.
 The arguments for the entry points follow the pattern of those for the
normal distribution:
 That is, the first argument gives the position for the density and CDF
and probability for the quantile function, followed by the
distribution’s parameters.  Argument lower_tail should be
TRUE (or 1) for normal use, but can be FALSE (or
0) if the probability of the upper tail is desired or specified.
 Finally, give_log should be non-zero if the result is required on
log scale, and log_p should be non-zero if p has been
specified on log scale.
 Note that you directly get the cumulative (or “integrated”)
hazard function, H(t) = - log(1 -
F(t)), by using
 or shorter (and more cryptic) - pdist(t, ..., 0, 1).

 The random-variate generation routine rnorm returns one normal
variate. See Random numbers, for the protocol in using the
random-variate routines.

 Note that these argument sequences are (apart from the names and that
rnorm has no n) mainly the same as the corresponding R
functions of the same name, so the documentation of the R functions
can be used.  Note that the exponential and gamma distributions are
parametrized by scale rather than rate.
 For reference, the following table gives the basic name (to be prefixed
by ‘d’, ‘p’, ‘q’ or ‘r’ apart from the exceptions
noted) and distribution-specific arguments for the complete set of
distributions.
 Entries marked with an asterisk only have ‘p’ and ‘q’
functions available, and none of the non-central distributions have
‘r’ functions.  After a call to dwilcox, pwilcox or
qwilcox the function wilcox_free() should be called, and
similarly for the signed rank functions.
 (If remapping is suppressed, the Normal distribution names are
Rf_dnorm4, Rf_pnorm5 and Rf_qnorm5.)
 For the negative binomial distribution (‘nbinom’), in addition to the
(size, prob) parametrization, the alternative (size, mu)
parametrization is provided as well by functions ‘[dpqr]nbinom_mu()’,
see ?NegBinomial in R.
 Functions dpois_raw(x, *) and dbinom_raw(x, *) are versions of the
Poisson and binomial probability mass functions which work continuously in
x, whereas dbinom(x,*) and dpois(x,*) only return non
zero values for integer x.
 Note that dbinom_raw() gets both p and q = 1-p which
may be advantageous when one of them is close to 1.
 
Next: Numerical Utilities, Previous: Distribution functions, Up: Numerical analysis subroutines   [Contents][Index] The Gamma function, the natural logarithm of its absolute value and
first four derivatives and the n-th derivative of Psi, the digamma
function, which is the derivative of lgammafn. In other words,
digamma(x) is the same as (psigamma(x,0),
trigamma(x) == psigamma(x,1), etc.
 The (complete) Beta function and its natural logarithm.
 The number of combinations of k items chosen from from n and
the natural logarithm of its absolute value, generalized to arbitrary real
n.  k is rounded to the nearest integer (with a warning if
needed).
 Bessel functions of types I, J, K and Y with index nu.  For
bessel_i and bessel_k there is the option to return
exp(-x) I(x; nu) or exp(x) K(x; nu) if expo is 2. (Use expo == 1 for unscaled
values.)
 
Next: Mathematical constants, Previous: Mathematical functions, Up: Numerical analysis subroutines   [Contents][Index] There are a few other numerical utility functions available as entry points.
 R_pow(x, y) and R_pow_di(x, i)
compute x^y and x^i, respectively
using R_FINITE checks and returning the proper result (the same
as R) for the cases where x, y or i are 0 or
missing or infinite or NaN.
 Computes log(1 + x) (log 1 plus x), accurately
even for small x, i.e., |x| << 1.
 This should be provided by your platform, in which case it is not included
in Rmath.h, but is (probably) in math.h which
Rmath.h includes.
 Computes log(1 + x) - x (log 1 plus x minus x),
accurately even for small x, i.e., |x| << 1.
 Computes log(1 + exp(x)) (log 1 plus exp),
accurately, notably for large x, e.g., x > 720.
 Computes exp(x) - 1 (exp x minus 1), accurately
even for small x, i.e., |x| << 1.
 This should be provided by your platform, in which case it is not included
in Rmath.h, but is (probably) in math.h which
Rmath.h includes.
 Computes log(gamma(x + 1)) (log(gamma(1 plus x))),
accurately even for small x, i.e., 0 < x < 0.5.
 Computes cos(pi * x) (where pi is 3.14159...),
accurately, notably for half integer x.
 This might be provided by your platform118, in which case it is not included in Rmath.h, but is
in math.h which Rmath.h includes.  (Ensure that
neither math.h nor cmath is included before
Rmath.h or define
 before the first inclusion.)
 Computes sin(pi * x) accurately, notably for (half) integer x.
 This might be provided by your platform, in which case it is not
included in Rmath.h, but is in math.h which Rmath.h
includes (but see the comments for cospi).
 Computes tan(pi * x) accurately, notably for (half) integer x.
 This might be provided by your platform, in which case it is not included
in Rmath.h, but is in math.h which Rmath.h includes
(but see the comments for cospi).
 Compute the log of a sum or difference from logs of terms, i.e., “x +
y” as log (exp(logx) + exp(logy)) and “x - y” as
log (exp(logx) - exp(logy)),
and “sum_i x[i]” as log (sum[i = 1:n exp(logx[i])] )
without causing unnecessary overflows or throwing away too much accuracy.
 Return the larger (max) or smaller (min) of two integer or
double numbers, respectively.  Note that fmax2 and fmin2
differ from C99’s fmax and fmin when one of the arguments
is a NaN: these versions return NaN.
 Compute the signum function, where sign(x) is 1, 0, or
-1, when x is positive, 0, or negative, respectively, and
NaN if x is a NaN.
 Performs “transfer of sign” and is defined as |x| * sign(y).
 Returns the value of x rounded to digits decimal digits
(after the decimal point).
 This is the function used by R’s signif().
 Returns the value of x rounded to digits significant
decimal digits.
 This is the function used by R’s round().
 Returns the value of x truncated (to an integer value) towards
zero.
 Note that this is no longer needed in C code, as C99 provide a
trunc function.  It is needed for portable C++98 code.
 
Previous: Numerical Utilities, Up: Numerical analysis subroutines   [Contents][Index] R has a set of commonly used mathematical constants encompassing
constants usually found math.h and contains further ones that are
used in statistical computations.  All these are defined to (at least)
30 digits accuracy in Rmath.h.  The following definitions
use ln(x) for the natural logarithm (log(x) in R).
 There are a set of constants (PI, DOUBLE_EPS) (and so on)
defined (unless STRICT_R_HEADERS is defined) in the included
header R_ext/Constants.h, mainly for compatibility with S.
 Further, the included header R_ext/Boolean.h has enumeration
constants TRUE and FALSE of type Rboolean in
order to provide a way of using “logical” variables in C consistently.
This can conflict with other software: for example it conflicts with the
headers in IJG’s jpeg-9 (but not earlier versions).
 
Next: Integration, Previous: Numerical analysis subroutines, Up: The R API   [Contents][Index] The C code underlying optim can be accessed directly.  The user
needs to supply a function to compute the function to be minimized, of
the type
 where the first argument is the number of parameters in the second
argument.  The third argument is a pointer passed down from the calling
routine, normally used to carry auxiliary information.
 Some of the methods also require a gradient function
 which passes back the gradient in the gr argument.  No function
is provided for finite-differencing, nor for approximating the Hessian
at the result.
 The interfaces (defined in header R_ext/Applic.h) are
 Many of the arguments are common to the various methods.  n is
the number of parameters, x or xin is the starting
parameters on entry and x the final parameters on exit, with
final value returned in Fmin.  Most of the other parameters can
be found from the help page for optim: see the source code
src/appl/lbfgsb.c for the values of nbd, which
specifies which bounds are to be used.
 
Next: Utility functions, Previous: Optimization, Up: The R API   [Contents][Index] The C code underlying integrate can be accessed directly.  The
user needs to supply a vectorizing C function to compute the
function to be integrated, of the type
 where x[] is both input and output and has length n, i.e.,
a C function, say fn, of type integr_fn must basically do
for(i in 1:n) x[i] := f(x[i], ex).  The vectorization requirement
can be used to speed up the integrand instead of calling it n
times.  Note that in the current implementation built on QUADPACK,
n will be either 15 or 21.  The ex argument is a pointer
passed down from the calling routine, normally used to carry auxiliary
information.
 There are interfaces (defined in header R_ext/Applic.h) for
integrals over finite and infinite intervals (or “ranges” or
“integration boundaries”).
 Only the 3rd and 4th argument differ for the two integrators; for the
finite range integral using Rdqags, a and b are the
integration interval bounds, whereas for an infinite range integral using
Rdqagi, bound is the finite bound of the integration (if
the integral is not doubly-infinite) and inf is a code indicating
the kind of integration range,
 corresponds to (bound, +Inf),
 corresponds to (-Inf, bound),
 corresponds to (-Inf, +Inf),
 f and ex define the integrand function, see above;
epsabs and epsrel specify the absolute and relative
accuracy requested, result, abserr and last are the
output components value, abs.err and subdivisions
of the R function integrate, where neval gives the number of
integrand function evaluations, and the error code ier is
translated to R’s integrate() $ message, look at that function
definition.  limit corresponds to integrate(...,
subdivisions = *).  It seems you should always define the two work
arrays and the length of the second one as
 The comments in the source code in src/appl/integrate.c give
more details, particularly about reasons for failure (ier >= 1).
 
Next: Re-encoding, Previous: Integration, Up: The R API   [Contents][Index] R has a fairly comprehensive set of sort routines which are made
available to users’ C code.
The following is declared in header file Rinternals.h.
 R_orderVector() corresponds to R’s order(..., na.last, decreasing).
More specifically, indx <- order(x, y, na.last, decreasing) corresponds to
R_orderVector(indx, n, Rf_lang2(x, y), nalast, decreasing) and for
three vectors, Rf_lang3(x,y,z) is used as arglist.
 Both R_orderVector and R_orderVector1 assume the vector
indx to be allocated to length >= n.  On return,
indx[] contains a permutation of 0:(n-1), i.e., 0-based C
indices (and not 1-based R indices, as R’s order()).
 When ordering only one vector, R_orderVector1 is faster and
corresponds (but is 0-based) to R’s indx <- order(x, na.last,
decreasing).  It was added in R 3.3.0.
 All other sort routines are declared in header file
R_ext/Utils.h (included by R.h) and include the following.
 The first three sort integer, real (double) and complex data
respectively.  (Complex numbers are sorted by the real part first then
the imaginary part.)  NAs are sorted last.
 rsort_with_index sorts on x, and applies the same
permutation to index.  NAs are sorted last.
 Is similar to rsort_with_index but sorts into decreasing order,
and NAs are not handled.
 These all provide (very) partial sorting: they permute x so that
x[k] is in the correct place with smaller values to
the left, larger ones to the right.
 These routines sort v[i:j] or
iv[i:j] (using 1-indexing, i.e.,
v[1] is the first element) calling the quicksort algorithm
as used by R’s sort(v, method = "quick") and documented on the
help page for the R function sort.  The ..._I()
versions also return the sort.index() vector in I.  Note
that the ordering is not stable, so tied values may be permuted.
 Note that NAs are not handled (explicitly) and you should
use different sorting functions if NAs can be present.
 The FORTRAN interface routines for sorting double precision vectors are
qsort3 and qsort4, equivalent to R_qsort and
R_qsort_I, respectively.
 Given the nr by nc matrix matrix in column-major
(“FORTRAN”)
order, R_max_col() returns in maxes[i-1] the
column number of the maximal element in the i-th row (the same as
R’s max.col() function).  In the case of ties (multiple maxima),
*ties_meth is an integer code in 1:3 determining the method:
1 = “random”, 2 = “first” and 3 = “last”.
See R’s help page ?max.col.
 Given the ordered vector xt of length n, return the interval
or index of x in xt[], typically max(i; 1 <= i <= n & xt[i] <=
x) where we use 1-indexing as in R and FORTRAN (but not C).  If
rightmost_closed is true, also returns n-1 if x
equals xt[n].  If all_inside is not 0, the
result is coerced to lie in 1:(n-1) even when x is
outside the xt[] range.  On return, *mflag equals
-1 if x < xt[1], +1 if x >=
xt[n], and 0 otherwise.
 The algorithm is particularly fast when ilo is set to the last
result of findInterval() and x is a value of a sequence which
is increasing or decreasing for subsequent calls.
 findInterval2() is a generalization of findInterval(),
with an extra Rboolean argument left_open.  Setting
left_open = TRUE basically replaces all left-closed right-open
intervals t) by left-open ones t], see the help page
of R function findInterval for details.
 There is also an F77_CALL(interv)() version of
findInterval() with the same arguments, but all pointers.
 A system-independent interface to produce the name of a temporary
file is provided as
 Return a pathname for a temporary file with name beginning with
prefix and ending with fileext in directory tmpdir.
A NULL prefix or extension is replaced by "".  Note that
the return value is malloced and should be freed when no
longer needed (unlike the system call tmpnam).
 There is also the internal function used to expand file names in several
R functions, and called directly by path.expand.
 Expand a path name fn by replacing a leading tilde by the user’s
home directory (if defined).  The precise meaning is platform-specific;
it will usually be taken from the environment variable HOME if
this is defined.
 For historical reasons there are FORTRAN interfaces to functions
D1MACH and I1MACH.  These can be called from C code as
e.g. F77_CALL(d1mach)(4).  Note that these are emulations of
the original functions by Fox, Hall and Schryer on NetLib at
http://www.netlib.org/slatec/src/ for IEC 60559 arithmetic
(required by R).
 
Next: Allowing interrupts, Previous: Utility functions, Up: The R API   [Contents][Index] R has its own C-level interface to the encoding conversion
capabilities provided by iconv because there are
incompatibilities between the declarations in different implementations
of iconv.
 These are declared in header file R_ext/Riconv.h.
 Set up a pointer to an encoding object to be used to convert between two
encodings: "" indicates the current locale.
 Convert as much as possible of inbuf to outbuf.  Initially
the int variables indicate the number of bytes available in the
buffers, and they are updated (and the char pointers are updated
to point to the next free byte in the buffer).  The return value is the
number of characters converted, or (size_t)-1 (beware:
size_t is usually an unsigned type).  It should be safe to assume
that an error condition sets errno to one of E2BIG (the
output buffer is full), EILSEQ (the input cannot be converted,
and might be invalid in the encoding specified) or EINVAL (the
input does not end with a complete multi-byte character).
 Free the resources of an encoding object.
 
Next: Platform and version information, Previous: Re-encoding, Up: The R API   [Contents][Index] No port of R can be interrupted whilst running long computations in
compiled code, so programmers should make provision for the code to be
interrupted at suitable points by calling from C
 and from FORTRAN
 These check if the user has requested an interrupt, and if so branch to
R’s error handling functions.
 Note that it is possible that the code behind one of the entry points
defined here if called from your C or FORTRAN code could be interruptible
or generate an error and so not return to your code.
 
Next: Inlining C functions, Previous: Allowing interrupts, Up: The R API   [Contents][Index] The header files define USING_R, which can be used to test if
the code is indeed being used with R.
 Header file Rconfig.h (included by R.h) is used to define
platform-specific macros that are mainly for use in other header files.
The macro WORDS_BIGENDIAN is defined on
big-endian119
systems (e.g. most OSes on Sparc and PowerPC hardware) and not on
little-endian systems (nowadays all the commoner R platforms).  It
can be useful when manipulating binary files.
 Header file Rversion.h (not included by R.h)
defines a macro R_VERSION giving the version number encoded as an
integer, plus a macro R_Version to do the encoding.  This can be
used to test if the version of R is late enough, or to include
back-compatibility features.  For protection against very old versions
of R which did not have this macro, use a construction such as
 More detailed information is available in the macros R_MAJOR,
R_MINOR, R_YEAR, R_MONTH and R_DAY: see the
header file Rversion.h for their format.  Note that the minor
version includes the patchlevel (as in ‘2.2’).
 Packages which use alloca need to ensure it is defined: as it is
part of neither C nor POSIX there is no standard way to do so.  As
from R 3.2.2 one can use
 (and this should be included before standard C headers such as
stdlib.h, since on some platforms these include malloc.h
which may have a conflicting definition), which suffices for known R
platforms.
 
Next: Controlling visibility, Previous: Platform and version information, Up: The R API   [Contents][Index] The C99 keyword inline should be recognized by all compilers
nowadays used to build R.  Portable code which might be used with
earlier versions of R can be written using the macro R_INLINE
(defined in file Rconfig.h included by R.h), as for
example from package cluster
 Be aware that using inlining with functions in more than one compilation
unit is almost impossible to do portably, see
http://www.greenend.org.uk/rjk/2003/03/inline.html, so this usage
is for static functions as in the example.  All the R
configure code has checked is that R_INLINE can be used in a
single C file with the compiler used to build R.  We recommend that
packages making extensive use of inlining include their own configure
code.
 
Next: Standalone Mathlib, Previous: Inlining C functions, Up: The R API   [Contents][Index] Header R_ext/Visibility has some definitions for controlling the
visibility of entry points.  These are only effective when
‘HAVE_VISIBILITY_ATTRIBUTE’ is defined – this is checked when R
is configured and recorded in header Rconfig.h (included by
R_ext/Visibility.h).  It is generally defined on modern
Unix-alikes with a recent compiler, but not supported on OS X nor
Windows.  Minimizing the visibility of symbols in a shared library will
both speed up its loading (unlikely to be significant) and reduce the
possibility of linking to other entry points of the same name.
 C/C++ entry points prefixed by attribute_hidden will not be
visible in the shared object.  There is no comparable mechanism for
FORTRAN entry points, but there is a more comprehensive scheme used by,
for example package stats.  Most compilers which allow control of
visibility will allow control of visibility for all symbols via a flag,
and where known the flag is encapsulated in the macros
‘C_VISIBILITY’ and F77_VISIBILITY for C and FORTRAN
compilers.  These are defined in etc/Makeconf and so available
for normal compilation of package code.  For example,
src/Makevars could include
 This would end up with no visible entry points, which would be
pointless.  However, the effect of the flags can be overridden by using
the attribute_visible prefix.  A shared object which registers
its entry points needs only for have one visible entry point, its
initializer, so for example package stats has
 The visibility mechanism is not available on Windows, but there is an
equally effective way to control which entry points are visible, by
supplying a definitions file
pkgnme/src/pkgname-win.def: only entry points
listed in that file will be visible.  Again using stats as an
example, it has
 
Next: Organization of header files, Previous: Controlling visibility, Up: The R API   [Contents][Index] It is possible to build Mathlib, the R set of mathematical
functions documented in Rmath.h, as a standalone library
libRmath under both Unix-alikes and Windows.  (This includes the
functions documented in Numerical analysis subroutines as from
that header file.)
 The library is not built automatically when R is installed, but can
be built in the directory src/nmath/standalone in the R
sources: see the file README there.  To use the code in your own
C program include
 and link against ‘-lRmath’ (and perhaps ‘-lm’).  There is an
example file test.c.
 A little care is needed to use the random-number routines. You will
need to supply the uniform random number generator
 or use the one supplied (and with a dynamic library or DLL you will have
to use the one supplied, which is the Marsaglia-multicarry with an entry
points
 to set its seeds and
 to read the seeds).
 
Previous: Standalone Mathlib, Up: The R API   [Contents][Index] The header files which R installs are in directory
R_INCLUDE_DIR (default R_HOME/include).  This
currently includes
 The following headers are included by R.h:
 The graphics systems are exposed in headers
R_ext/GraphicsEngine.h, R_ext/GraphicsDevice.h (which it
includes) and R_ext/QuartzDevice.h. Facilities for defining
custom connection implementations are provided in
R_ext/Connections.h, but make sure you consult the file before
use.
 Let us re-iterate the advice to include system headers before the R
header files, especially Rinternals.h (included by
Rdefines.h) and Rmath.h, which redefine names which may be
used in system headers (fewer if ‘R_NO_REMAP’ is defined, or
‘R_NO_REMAP_RMATH’ for Rmath.h, as from R 3.1.0).
 
Next: Linking GUIs and other front-ends to R, Previous: The R API, Up: Top   [Contents][Index] R programmers will often want to add methods for existing generic
functions, and may want to add new generic functions or make existing
functions generic.  In this chapter we give guidelines for doing so,
with examples of the problems caused by not adhering to them.
 This chapter only covers the ‘informal’ class system copied from S3,
and not with the S4 (formal) methods of package methods.
 First, a caveat: a function named gen.cl will
be invoked by the generic gen for class cl, so
do not name functions in this style unless they are intended to be
methods.
 The key function for methods is NextMethod, which dispatches the
next method.  It is quite typical for a method function to make a few
changes to its arguments, dispatch to the next method, receive the
results and modify them a little.  An example is
 Note that the example above works because there is a next method,
the default method, not that a new method is selected when the class is
changed.
 Any method a programmer writes may be invoked from another method
by NextMethod, with the arguments appropriate to the
previous method.  Further, the programmer cannot predict which method
NextMethod will pick (it might be one not yet dreamt of), and the
end user calling the generic needs to be able to pass arguments to the
next method.  For this to work
 A method must have all the arguments of the generic, including
… if the generic does.
 It is a grave misunderstanding to think that a method needs only to
accept the arguments it needs.  The original S version of
predict.lm did not have a … argument, although
predict did.  It soon became clear that predict.glm needed
an argument dispersion to handle over-dispersion.  As
predict.lm had neither a dispersion nor a …
argument, NextMethod could no longer be used.  (The legacy, two
direct calls to predict.lm, lives on in predict.glm in
R, which is based on the workaround for S3 written by Venables &
Ripley.)
 Further, the user is entitled to use positional matching when calling
the generic, and the arguments to a method called by UseMethod
are those of the call to the generic.  Thus
 A method must have arguments in exactly the same order as the
generic.
 To see the scale of this problem, consider the generic function
scale, defined as
 Suppose an unthinking package writer created methods such as
 Then for x of class "foo" the calls
 would do most likely do different things, to the justifiable
consternation of the end user.
 To add a further twist, which default is used when a user calls
scale(x) in our example?  What if
 and x has class c("bar", "foo")?  It is the default
specified in the method that is used, but the default
specified in the generic may be the one the user sees.
This leads to the recommendation:
 If the generic specifies defaults, all methods should use the same defaults.
 An easy way to follow these recommendations is to always keep generics
simple, e.g.
 Only add parameters and defaults to the generic if they make sense in
all possible methods implementing it.
 
Previous: Generic functions and methods, Up: Generic functions and methods   [Contents][Index] When creating a new generic function, bear in mind that its argument
list will be the maximal set of arguments for methods, including those
written elsewhere years later.  So choosing a good set of arguments may
well be an important design issue, and there need to be good arguments
not to include a … argument.
 If a … argument is supplied, some thought should be given
to its position in the argument sequence.  Arguments which follow
… must be named in calls to the function, and they must be
named in full (partial matching is suppressed after …).
Formal arguments before … can be partially matched, and so
may ‘swallow’ actual arguments intended for ….  Although it
is commonplace to make the … argument the last one, that is
not always the right choice.
 Sometimes package writers want to make generic a function in the base
package, and request a change in R.  This may be justifiable, but
making a function generic with the old definition as the default method
does have a small performance cost.  It is never necessary, as a package
can take over a function in the base package and make it generic by
something like
 Earlier versions of this manual suggested assigning foo.default <-
base::foo.  This is not a good idea, as it captures the base
function at the time of installation and it might be changed as R is
patched or updated.
 The same idea can be applied for functions in other packages with namespaces.
 
Next: Function and variable index, Previous: Generic functions and methods, Up: Top   [Contents][Index] There are a number of ways to build front-ends to R: we take this to
mean a GUI or other application that has the ability to submit commands
to R and perhaps to receive results back (not necessarily in a text
format).  There are other routes besides those described here, for
example the package Rserve (from CRAN, see also
https://www.rforge.net/Rserve/) and connections to Java in
‘JRI’ (part of the rJava package on CRAN) and
the Omegahat/Bioconductor package ‘SJava’.
 Note that the APIs described in this chapter are only intended to be
used in an alternative front-end: they are not part of the API made
available for R packages and can be dangerous to use in a
conventional package (although packages may contain alternative
front-ends).  Conversely some of the functions from the API (such as
R_alloc) should not be used in front-ends.
 
Next: Embedding R under Windows, Previous: Linking GUIs and other front-ends to R, Up: Linking GUIs and other front-ends to R   [Contents][Index] R can be built as a shared library120 if configured with --enable-R-shlib.  This
shared library can be used to run R from alternative front-end
programs.  We will assume this has been done for the rest of this
section.  Also, it can be built as a static library if configured with
--enable-R-static-lib, and that can be used in a very similar
way (at least on Linux: on other platforms one needs to ensure that all
the symbols exported by libR.a are linked into the front-end).
 The command-line R front-end, R_HOME/bin/exec/R, is one
such example, and the former GNOME (see package gnomeGUI
on CRAN’s ‘Archive’ area) and OS X consoles are others.
The source for R_HOME/bin/exec/R is in file
src/main/Rmain.c and is very simple
 indeed, misleadingly simple.  Remember that
R_HOME/bin/exec/R is run from a shell script
R_HOME/bin/R which sets up the environment for the
executable, and this is used for
 The first two of these can be achieved for your front-end by running it
via R CMD. So, for example
 will both work in a standard R installation. (R CMD looks
first for executables in R_HOME/bin.  These command-lines
need modification if a sub-architecture is in use.) If you do not want
to run your front-end in this way, you need to ensure that R_HOME
is set and LD_LIBRARY_PATH is suitable.  (The latter might well
be, but modern Unix/Linux systems do not normally include
/usr/local/lib (/usr/local/lib64 on some architectures),
and R does look there for system components.)
 The other senses in which this example is too simple are that all the
internal defaults are used and that control is handed over to the
R main loop.  There are a number of small examples121 in the
tests/Embedding directory.  These make use of
Rf_initEmbeddedR in src/main/Rembedded.c, and essentially
use
 If you do not want to pass R arguments, you can fake an argv
array, for example by
 However, to make a GUI we usually do want to run run_Rmainloop
after setting up various parts of R to talk to our GUI, and arranging
for our GUI callbacks to be called during the R mainloop.
 One issue to watch is that on some platforms Rf_initEmbeddedR and
Rf_endEmbeddedR change the settings of the FPU (e.g. to allow
errors to be trapped and to make use of extended precision registers).
 The standard code sets up a session temporary directory in the usual
way, unless R_TempDir is set to a non-NULL value before
Rf_initEmbeddedR is called.  In that case the value is assumed to
contain an existing writable directory (no check is done), and it is not
cleaned up when R is shut down.
 Rf_initEmbeddedR sets R to be in interactive mode: you can set
R_Interactive (defined in Rinterface.h) subsequently to
change this.
 Note that R expects to be run with the locale category
‘LC_NUMERIC’ set to its default value of C, and so should
not be embedded into an application which changes that.
 It is the user’s responsibility to attempt to initialize only once.  To
protect the R interpreter, Rf_initialize_R will exit the
process if re-initialization is attempted.
 
Next: Setting R callbacks, Previous: Embedding R under Unix-alikes, Up: Embedding R under Unix-alikes   [Contents][Index] Suitable flags to compile and link against the R (shared or static)
library can be found by
 (These apply only to an uninstalled copy or a standard install.)
 If R is installed, pkg-config is available and neither
sub-architectures nor an OS X framework have been used, alternatives for
a shared R library are
 and for a static R library
 (This may work for an installed OS framework if pkg-config is
taught where to look for libR.pc: it is installed inside the
framework.)
 However, a more comprehensive way is to set up a Makefile to
compile the front-end.  Suppose file myfe.c is to be compiled to
myfe. A suitable Makefile might be
 invoked as
 Additional flags which $(MAIN_LINK) includes are, amongst others,
those to select OpenMP and --export-dynamic for the GNU linker
on some platforms.  In principle $(LIBS) is not needed
when using a shared R library as libR is linked against
those libraries, but some platforms need the executable also linked
against them.
 
Next: Registering symbols, Previous: Compiling against the R library, Up: Embedding R under Unix-alikes   [Contents][Index] For Unix-alikes there is a public header file Rinterface.h that
makes it possible to change the standard callbacks used by R in a
documented way.  This defines pointers (if R_INTERFACE_PTRS is
defined)
 which allow standard R callbacks to be redirected to your GUI.  What
these do is generally documented in the file src/unix/system.txt.
 This should display the message, which may have multiple lines:  it
should be brought to the user’s attention immediately.
 This function invokes actions (such as change of cursor) when R
embarks on an extended computation (which=1) and when such
a state terminates (which=0).
 These functions interact with a console.
 R_ReadConsole prints the given prompt at the console and then
does a fgets(3)–like operation, transferring up to buflen
characters into the buffer buf. The last two bytes should be
set to ‘"\n\0"’ to preserve sanity.  If hist is non-zero,
then the line should be added to any command history which is being
maintained.  The return value is 0 is no input is available and >0
otherwise.
 R_WriteConsoleEx writes the given buffer to the console,
otype specifies the output type (regular output or
warning/error). Call to R_WriteConsole(buf, buflen) is equivalent
to R_WriteConsoleEx(buf, buflen, 0). To ensure backward
compatibility of the callbacks, ptr_R_WriteConsoleEx is used only
if ptr_R_WriteConsole is set to NULL.  To ensure that
stdout() and stderr() connections point to the console,
set the corresponding files to NULL via
 R_ResetConsole is called when the system is reset after an error.
R_FlushConsole is called to flush any pending output to the
system console.  R_ClearerrConsole clears any errors associated
with reading from the console.
 This function is used to display the contents of files.
 Choose a file and return its name in buf of length len.
Return value is 0 for success, > 0 otherwise.
 Send a file to an editor window.
 Send nfile files to an editor, with titles possibly to be used for
the editor window(s).
 .Internal functions for loadhistory, savehistory
and timestamp.
 If the console has no history mechanism these can be as
simple as
 The R_addhistory function should return silently if no history
mechanism is present, as a user may be calling timestamp purely
to write the time stamp to the console.
 This should abort R as rapidly as possible, displaying the message.
A possible implementation is
 This function invokes any actions which occur at system termination.
It needs to be quite complex:
 These callbacks should never be changed in a running R session (and
hence cannot be called from an extension package).
 .External functions for dataentry (and edit on
matrices and data frames), View and select.list.  These
can be changed if they are not currently in use.
 
Next: Meshing event loops, Previous: Setting R callbacks, Up: Embedding R under Unix-alikes   [Contents][Index] An application embedding R needs a different way of registering
symbols because it is not a dynamic library loaded by R as would be
the case with a package.  Therefore R reserves a special
DllInfo entry for the embedding application such that it can
register symbols to be used with .C, .Call etc.  This
entry can be obtained by calling getEmbeddingDllInfo, so a
typical use is
 The native routines defined by cMethods and callMethods
should be present in the embedding application.  See Registering native routines for details on registering symbols in general.
 
Next: Threading issues, Previous: Registering symbols, Up: Embedding R under Unix-alikes   [Contents][Index] One of the most difficult issues in interfacing R to a front-end is
the handling of event loops, at least if a single thread is used.  R
uses events and timers for
 Specifically, the Unix-alike command-line version of R runs separate
event loops for
 There is a protocol for adding event handlers to the first two types of
event loops, using types and functions declared in the header
R_ext/eventloop.h and described in comments in file
src/unix/sys-std.c.  It is possible to add (or remove) an input
handler for events on a particular file descriptor, or to set a polling
interval (via R_wait_usec) and a function to be called
periodically via R_PolledEvents: the polling mechanism is used by
the tcltk package.
 It is not intended that these facilities are used by packages, but if
they are needed exceptionally, the package should ensure that it cleans
up and removes its handlers when its namespace is unloaded.
 An alternative front-end needs both to make provision for other R
events whilst waiting for input, and to ensure that it is not frozen out
during events of the second type.  This is not handled very well in the
existing examples.  The GNOME front-end ran a private handler for polled
events by setting
 whilst it is waiting for console input.  This obviously handles events
for Gtk windows (such as the graphics device in the gtkDevice
package), but not X11 events (such as the X11() device) or for
other event handlers that might have been registered with R.  It does
not attempt to keep itself alive whilst R is waiting on sockets.  The
ability to add a polled handler as R_timeout_handler is used by
the tcltk package.
 
Previous: Meshing event loops, Up: Embedding R under Unix-alikes   [Contents][Index] Embedded R is designed to be run in the main thread, and all the
testing is done in that context.  There is a potential issue with the
stack-checking mechanism where threads are involved.  This uses two
variables declared in Rinterface.h (if CSTACK_DEFNS is
defined) as
 Note that uintptr_t is a C99 type for which a substitute is
defined in R, so your code needs to define HAVE_UINTPTR_T
appropriately.
 These will be set122 when Rf_initialize_R is called, to values appropriate to the
main thread.  Stack-checking can be disabled by setting
R_CStackLimit = (uintptr_t)-1 immediately after
Rf_initialize_R is called, but it is better to if possible set
appropriate values.  (What these are and how to determine them are
OS-specific, and the stack size limit may differ for secondary threads.
If you have a choice of stack size, at least 10Mb is recommended.)
 You may also want to consider how signals are handled: R sets signal
handlers for several signals, including SIGINT, SIGSEGV,
SIGPIPE, SIGUSR1 and SIGUSR2, but these can all be
suppressed by setting the variable R_SignalHandlers (declared in
Rinterface.h) to 0.
 Note that these variables must not be changed by an R
package: a package should not calling R internals which
makes use of the stack-checking mechanism on a secondary thread.
 
Previous: Embedding R under Unix-alikes, Up: Linking GUIs and other front-ends to R   [Contents][Index] All Windows interfaces to R call entry points in the DLL
R.dll, directly or indirectly.  Simpler applications may find it
easier to use the indirect route via (D)COM.
 
Next: Calling R.dll directly, Previous: Embedding R under Windows, Up: Embedding R under Windows   [Contents][Index] (D)COM is a standard Windows mechanism used for communication
between Windows applications.  One application (here R) is run as COM
server which offers services to clients, here the front-end calling
application.  The services are described in a ‘Type Library’ and are
(more or less) language-independent, so the calling application can be
written in C or C++ or Visual Basic or Perl or Python and so on.
The ‘D’ in (D)COM refers to ‘distributed’, as the client and server can
be running on different machines.
 The basic R distribution is not a (D)COM server, but two addons are
currently available that interface directly with R and provide a
(D)COM server:
 Recent versions have usage restrictions.
 
Next: Finding R_HOME, Previous: Using (D)COM, Up: Embedding R under Windows   [Contents][Index] The R DLL is mainly written in C and has _cdecl entry
points.  Calling it directly will be tricky except from C code (or C++
with a little care).
 There is a version of the Unix-alike interface calling
 which is an entry point in R.dll.  Examples of its use (and a
suitable Makefile.win) can be found in the tests/Embedding
directory of the sources.  You may need to ensure that
R_HOME/bin is in your PATH so the R DLLs are found.
 Examples of calling R.dll directly are provided in the directory
src/gnuwin32/front-ends, including a simple command-line
front end rtest.c whose code is
 The ideas are
 An underlying theme is the need to keep the GUI ‘alive’, and this has
not been done in this example.  The R callback R_ProcessEvents
needs to be called frequently to ensure that Windows events in R
windows are handled expeditiously.  Conversely, R needs to allow the
GUI code (which is running in the same process) to update itself as
needed – two ways are provided to allow this:
 It may be that no R GraphApp windows need to be considered, although
these include pagers, the windows() graphics device, the R
data and script editors and various popups such as choose.file()
and select.list().  It would be possible to replace all of these,
but it seems easier to allow GraphApp to handle most of them.
 It is possible to run R in a GUI in a single thread (as
RGui.exe shows) but it will normally be easier123 to
use multiple threads.
 Note that R’s own front ends use a stack size of 10Mb, whereas MinGW
executables default to 2Mb, and Visual C++ ones to 1Mb.  The latter
stack sizes are too small for a number of R applications, so
general-purpose front-ends should use a larger stack size.
 
Previous: Calling R.dll directly, Up: Embedding R under Windows   [Contents][Index] Both applications which embed R and those which use a system
call to invoke R (as Rscript.exe, Rterm.exe or
R.exe) need to be able to find the R bin directory.
The simplest way to do so is the ask the user to set an environment
variable R_HOME and use that, but naive users may be flummoxed as
to how to do so or what value to use.
 The R for Windows installers have for a long time allowed the value
of R_HOME to be recorded in the Windows Registry: this is
optional but selected by default.  Where it is recorded has
changed over the years to allow for multiple versions of R to be
installed at once, and to allow 32- and 64-bit versions of R to be
installed on the same machine.
 The basic Registry location is Software\R-core\R.  For an
administrative install this is under HKEY_LOCAL_MACHINE and on a
64-bit OS HKEY_LOCAL_MACHINE\Software\R-core\R is by default
redirected for a 32-bit application, so a 32-bit application will see
the information for the last 32-bit install, and a 64-bit application
that for the last 64-bit install.  For a personal install, the
information is under HKEY_CURRENT_USER\Software\R-core\R which is
seen by both 32-bit and 64-bit applications and so records the last
install of either architecture.  To circumvent this, there are locations
Software\R-core\R32 and Software\R-core\R64 which always
refer to one architecture.
 When R is installed and recording is not disabled then two string
values are written at that location for keys InstallPath and
Current Version, and these keys are removed when R is
uninstalled.  To allow information about other installed versions to be
retained, there is also a key named something like 3.0.0 or
3.0.0 patched or 3.1.0 Pre-release with a value for
InstallPath.
 So a comprehensive algorithm to search for R_HOME is something
like
 Prior to R 2.12.0 R.dll and the various front-end executables
were in R_HOME\bin, but they are now in R_HOME\bin\i386 or
R_HOME\bin\x64.  So you may need to arrange to look first in the
architecture-specific subdirectory and then in R_HOME\bin.
 
Next: Concept index, Previous: Linking GUIs and other front-ends to R, Up: Top   [Contents][Index] 
Previous: Function and variable index, Up: Top   [Contents][Index] although this is a persistent
mis-usage.  It seems to stem from S, whose analogues of R’s packages
were officially known as library sections and later as
chapters, but almost always referred to as libraries. This
seems to be commonly used for a file in ‘markdown’ format.  Be aware
that most users of R will not know that, nor know how to view such a
file: platforms such as OS X and Windows do not have a default viewer
set in their file associations.  The CRAN package web pages
render such files in HTML: the converter used expects the file to be
encoded in UTF-8. currently, top-level files
.Rbuildignore and .Rinstignore, and
vignettes/.install_extras. false positives are possible, but only a handful have been
seen so far. at least if this
is done in a locale which matches the package encoding. and
required by CRAN, so checked by R CMD check
--as-cran. But it is checked for Open Source packages
by R CMD check --as-cran. Duplicate definitions may
trigger a warning: see User-defined macros. even one wrapped in \donttest. This includes all packages
directly called by library and require calls, as well as
data obtained via data(theirdata, package = "somepkg")
calls: R CMD check will warn about all of these.  But there
are subtler uses which it will not detect: e.g. if package A uses
package B and makes use of functionality in package B which uses package
C which package B suggests or enhances, then package C needs to be in
the ‘Suggests’ list for package A.  Nor will undeclared uses in
included files be reported, nor unconditional uses of packages listed
under ‘Enhances’. Extensions
.S and .s arise from code originally written for S(-PLUS),
but are commonly used for assembler code.  Extension .q was used
for S, which at one time was tentatively called QPE. but they should be in the encoding
declared in the DESCRIPTION file. This is true for OSes which
implement the ‘C’ locale: Windows’ idea of the ‘C’ locale uses
the WinAnsi charset. More precisely, they can
contain the English alphanumeric characters and the symbols
‘$ - _ . + ! ' ( ) , ;  = &’. Note that Ratfor is not supported.
If you have Ratfor source code, you need to convert it to FORTRAN.  Only
FORTRAN 77 (which we write in upper case) is supported on all platforms,
but most also support Fortran-95 (for which we use title case).  If you
want to ship Ratfor source files, please do so in a subdirectory of
src and not in the main subdirectory. either or both of which may not be supported on particular
platforms Using .hpp is not guaranteed to be
portable. There
is also ‘__APPLE_CC__’, but that indicates a compiler with
Apple-specific features, not the OS.  It is used in
Rinlinedfuns.h. the POSIX
terminology, called ‘make variables’ by GNU make. on all platforms from R 3.1.0 The best way to generate such a
file is to copy the .Rout from a successful run of R CMD
check.  If you want to generate it separately, do run R with options
--vanilla --slave and with environment variable
LANGUAGE=en set to get messages in English.  Be careful not to use
output with the option --timings (and note that
--as-cran sets it). e.g.
https://tools.ietf.org/html/rfc4180. People who have trouble with
case are advised to use .rda as a common error is to refer to
abc.RData as abc.Rdata! The script
should only assume a POSIX-compliant /bin/sh – see
http://pubs.opengroup.org/onlinepubs/9699919799/utilities/V3_chap02.html.
In particular bash extensions must not be used, and not all
R platforms have a bash command, let alone one at
/bin/bash.  All known shells used with R support the use of
backticks, but not all support ‘$(cmd)’. in POSIX parlance: GNU make
calls these ‘make variables’. at least on Unix-alikes: the Windows build currently
resolves such dependencies to a static FORTRAN library when
Rblas.dll is built. http://www.openmp.org/,
https://en.wikipedia.org/wiki/OpenMP,
https://computing.llnl.gov/tutorials/openMP/ Some builds of clang
3.7.0 have support for OpenMP 3.1 (this is built by default as from
3.8.0), but even if the compiler has, the libomp library may not
be installed.  At the time of writing Apple builds of clang
for OS X had no support. Windows default, not MinGW-w64 default. Which it was at the time of writing with GCC,
Solaris Studio, Intel and Clang 3.7.x compilers. some Windows toolchains had the
typo ‘_REENTRANCE’ instead. Cygwin used g77 up to 2011, and some pre-built
versions of R for Unix OSes still do. Some distributions, notably Debian, have supplied a
build of clang with g++’s headers and library.
Conversely, Apple’s command named g++ is based on
clang using libcxx. For
details of these and related macros, see file config.site in
the R sources. On systems which use sub-architectures,
architecture-specific versions such as ~/.R/check.Renviron.i386
take precedence. A suitable file.exe is
part of the Windows toolset: it checks for gfile if a suitable
file is not found: the latter is available in the OpenCSW
collection for Solaris at http://www.opencsw.org.  The source
repository is ftp://ftp.astron.com/pub/file/. An exception is made
for subdirectories with names starting ‘win’ or ‘Win’. on most other platforms such runtime
libraries are dynamic, but static libraries are currently used on
Windows because the toolchain is not a standard part of the OS. or if option --use-valgrind is
used or environment variable _R_CHECK_ALWAYS_LOG_VIGNETTE_OUTPUT_
is set to a true value or if there are differences from a target output
file For example, in early
2014 gdata declared ‘Imports: gtools’ and gtools
declared ‘Imports: gdata’. loading, examples,
tests, running vignette code on all platforms from
R 3.1.0. called CVS or .svn or
.arch-ids or .bzr or .git (but not files called
.git) or .hg. called
.metadata. which is
an error: GNU make uses GNUmakefile. and to avoid problems with case-insensitive file
systems, lower-case versions of all these extensions. unless inhibited by using
‘BuildVignettes: no’ in the DESCRIPTION file. provided the conditions of the
package’s license are met: many, including CRAN, see the
omission of source components as incompatible with an Open Source
license. R_HOME/bin is prepended to the
PATH so that references to R or Rscript in the
Makefile do make use of the currently running version of R. Note that
lazy-loaded datasets are not in the package’s namespace so need
to be accessed via ::, e.g.
survival::survexp.us. they will be called
with two unnamed arguments, in that order. NB: this will only be read in all versions of R if
the package contains R code in a R directory. Note that this is the
basename of the shared object, and the appropriate extension (.so
or .dll) will be added. This was necessary at least prior to
R 3.0.2 as the methods package looked for its own R code on
the search path. This defaults to the same
pattern as exportPattern: use something like
exportClassPattern("^$") to override this. if it does, there will be opaque warnings about
replacing imports if the classes/methods are also imported. People use dev.new() to open a
device at a particular size: that is not portable but using
dev.new(noRStudioGD = TRUE) helps. Solaris make does not accept
CRLF-terminated Makefiles; Solaris warns about and some other
makes ignore incomplete final lines. This was apparently introduced
in SunOS 4, and is available elsewhere provided it is surrounded
by spaces. GNU make,
BSD make formerly in FreeBSD and OS X, AT&T make as implemented on
Solaris, pmake in FreeBSD, ‘Distributed Make’ (dmake),
part of Solaris Studio and available in other versions. For example, test
options -a and -e are not portable, and not supported
in the AT&T Bourne shell used on Solaris 10/11, even though they are in
the 2008 POSIX standard.  Nor does Solaris support ‘$(cmd)’. but
note that long long is not a standard C++ type, and C++ compilers
set up for strict checking will reject it. or where supported the variants _Exit and
_exit. This and
srandom are in any case not portable.  They are in POSIX but not
in the C99 standard, and not available on Windows. in libselinux. except perhaps the simplest kind as used by
download.file() in non-interactive use. Whereas the GNU linker reorders so -L options
are processed first, the Solaris one does not. some versions of OS X did not. Not doing so is the
default on Windows, overridden for the R executables.  It is also the
default on some Solaris compilers. These are not needed for the default compiler settings
on ‘x86_64’ but are likely to be needed on ‘ix86’. Select ‘Save as’, and select
‘Reduce file size’ from the ‘Quartz filter’ menu’: this can be accessed
in other ways, for example by Automator. except perhaps some
special characters such as backslash and hash which may be taken over
for currency symbols. Typically on a Unix-alike this is done by telling
fontconfig where to find suitable fonts to select glyphs
from. E.g. gcc 5.3 in C++11 mode. which often is the same as the header included by
the C compiler, but some compilers have wrappers for some of the C
headers. this object is available since R 2.8.0, so the
‘Depends’ field in the DESCRIPTION file should contain
something at least as restrictive as ‘R (>= 2.8’. e.g. \alias, \keyword and
\note sections. There can be exceptions: for example
Rd files are not allowed to start with a dot, and have to be
uniquely named on a case-insensitive file system. in the current locale, and with special
treatment for LaTeX special characters and with any
‘pkgname-package’ topic moved to the top of the list. Text between or after list items is discouraged. as defined by the R function
trimws. Currently it is
rendered differently only in HTML conversions, and LaTeX conversion
outside ‘\usage’ and ‘\examples’ environments. a common
example in CRAN packages is \link[mgcv]{gam}. There is only a fine
distinction between \dots and \ldots.  It is technically
incorrect to use \ldots in code blocks and tools::checkRd
will warn about this—on the other hand the current converters treat
them the same way in code blocks, and elsewhere apart from the small
distinction between the two in LaTeX. See the
examples section in the file Paren.Rd for an example. R
2.9.0 added support for UTF-8 Cyrillic characters in LaTeX, but on
some OSes this will need Cyrillic support added to LaTeX, so
environment variable _R_CYRILLIC_TEX_ may need to be set to a
non-empty value to enable this. R
has to be built to enable this, but the option
--enable-R-profiling is the default. For Unix-alikes these are intervals of CPU
time, and for Windows of elapsed time. With the exceptions of the commands
listed below: an object of such a name can be printed via an
explicit call to print. at the time of writing mainly for 10.9 with some
support for 10.8, none for the current 10.10. Those in some numeric, logical,
integer, raw, complex vectors and in memory allocated by
R_alloc. including using the data sections of R vectors after
they are freed. small
fixed-size arrays by default in gfortran, for example. currently only on ‘ix86’/‘x86_64’ Linux
and OS X (including the builds in Xcode 7 but not earlier Apple
releases).  On some platforms the runtime library, libasan, needs
to be installed separately, and for checking C++ you may also need
libubsan. part of the LLVM project and
in distributed in llvm RPMs and .debs on Linux.  It is not
currently shipped by Apple. as Ubuntu does. installed on some Linux systems as
asan_symbolize, and obtainable from
https://llvm.org/svn/llvm-project/compiler-rt/trunk/lib/asan/scripts/asan_symbolize.py:
it makes use of llvm-symbolizer if available. On some
platforms the runtime library, libubsan, needs to be installed
separately. e.g. src/main/dotcode.c and parts of the
Matrix sources with clang 3.7.0 amd later. or
the user manual for your version of clang, e.g.
http://llvm.org/releases/3.6.2/tools/docs/UsersManual.html. This includes the C++
UBSAN handlers, despite its name. but
works better if inlining and frame pointer optimizations are disabled. possibly after some platform-specific
translation, e.g. adding leading or trailing underscores. Note that this is then not checked for over-runs by
option CBoundsCheck = TRUE. but this is not currently done. whether or not ‘LinkingTo’ is used. so there needs to be a corresponding import or
importFrom entry in the NAMESPACE file. Even including C system headers in
such a block has caused compilation errors. dyld on OS X,
and DYLD_LIBRARY_PATHS below. That is,
similar to those defined in S version 4 from the 1990s: these are
not kept up to date and are not recommended for new projects.  Prior to
R 3.3.0 it was not compatible with defining R_NO_REMAP. see The R API: note that these are not all part of
the API. SEXP is an acronym for Simple
EXPression, common in LISP-like language syntaxes. If no coercion was required, coerceVector would
have passed the old object through unchanged. You can assign a copy of the object in the
environment frame rho using defineVar(symbol,
duplicate(value), rho)). see Character encoding issues for why this
might not be what is required. This is only guaranteed to show the
current interface: it is liable to change. Known problems are redefining
LENGTH, error, length, vector and
warning It is an optional C11
extension. https://en.wikipedia.org/wiki/Endianness. In the parlance of OS X
this is a dynamic library, and is the normal way to build R on
that platform. but these
are not part of the automated test procedures and so little tested. at least on platforms where the values are
available, that is having getrlimit and on Linux or having
sysctl supporting KERN_USRSTACK, including FreeBSD and OS
X. An
attempt to use only threads in the late 1990s failed to work correctly
under Windows 95, the predominant version of Windows at that time. 
Next: Acknowledgements   [Contents][Index] This is a guide to importing and exporting data to and from R.
 This manual is for R, version 3.3.1 (2016-06-21).
 Copyright © 2000–2016 R Core Team
 Permission is granted to make and distribute verbatim copies of this
manual provided the copyright notice and this permission notice are
preserved on all copies.
 Permission is granted to copy and distribute modified versions of this
manual under the conditions for verbatim copying, provided that the
entire resulting derived work is distributed under the terms of a
permission notice identical to this one.
 Permission is granted to copy and distribute translations of this manual
into another language, under the above conditions for modified versions,
except that this permission notice may be stated in a translation
approved by the R Core Team.
 
Next: Introduction, Previous: Top, Up: Top   [Contents][Index] The relational databases part of this manual is based in part on an
earlier manual by Douglas Bates and Saikat DebRoy.  The principal author
of this manual was Brian Ripley.
 Many volunteers have contributed to the packages used here.  The
principal authors of the packages mentioned are
 Brian Ripley is the author of the support for connections.
 
Next: Spreadsheet-like data, Previous: Acknowledgements, Up: Top   [Contents][Index] Reading data into a statistical system for analysis and exporting the
results to some other system for report writing can be frustrating tasks
that can take far more time than the statistical analysis itself, even
though most readers will find the latter far more appealing.
 This manual describes the import and export facilities available either
in R itself or via packages which are available from CRAN
or elsewhere.
 Unless otherwise stated, everything described in this manual is (at
least in principle) available on all platforms running R.
 In general, statistical systems like R are not particularly well
suited to manipulations of large-scale data.  Some other systems are
better than R at this, and part of the thrust of this manual is to
suggest that rather than duplicating functionality in R we can make
another system do the work!  (For example Therneau & Grambsch (2000)
commented that they preferred to do data manipulation in SAS and then
use package survival in S for the analysis.)  Database
manipulation systems are often very suitable for manipulating and
extracting data: several packages to interact with DBMSs are discussed
here.
 There are packages to allow functionality developed in languages such as
Java, perl and python to be directly integrated
with R code, making the use of facilities in these languages even
more appropriate.  (See the rJava package from CRAN
and the SJava, RSPerl and RSPython packages from the
Omegahat project, http://www.omegahat.net.)
 It is also worth remembering that R like S comes from the Unix
tradition of small re-usable tools, and it can be rewarding to use tools
such as awk and perl to manipulate data before import or
after export.  The case study in Becker, Chambers & Wilks (1988, Chapter
9) is an example of this, where Unix tools were used to check and
manipulate the data before input to S. The traditional Unix tools
are now much more widely available, including for Windows.
 
Next: Export to text files, Previous: Introduction, Up: Introduction   [Contents][Index] The easiest form of data to import into R is a simple text file, and
this will often be acceptable for problems of small or medium scale.
The primary function to import from a text file is scan, and this
underlies most of the more convenient functions discussed in
Spreadsheet-like data.
 However, all statistical consultants are familiar with being presented
by a client with a memory stick (formerly, a floppy disc or CD-R) of
data in some proprietary binary format, for example ‘an Excel
spreadsheet’ or ‘an SPSS file’.  Often the simplest thing to do is to
use the originating application to export the data as a text file (and
statistical consultants will have copies of the most common applications
on their computers for that purpose).  However, this is not always
possible, and Importing from other statistical systems discusses
what facilities are available to access such files directly from R.
For Excel spreadsheets, the available methods are summarized in
Reading Excel spreadsheets.  
 In a few cases, data have been stored in a binary form for compactness
and speed of access.  One application of this that we have seen several
times is imaging data, which is normally stored as a stream of bytes as
represented in memory, possibly preceded by a header.  Such data formats
are discussed in Binary files and Binary connections.
 For much larger databases it is common to handle the data using a
database management system (DBMS).  There is once again the option of
using the DBMS to extract a plain file, but for many such DBMSs the
extraction operation can be done directly from an R package:
See Relational databases.  Importing data via network connections is
discussed in Network interfaces.
 
Previous: Imports, Up: Imports   [Contents][Index] Unless the file to be imported from is entirely in ASCII, it
is usually necessary to know how it was encoded.  For text files, a good
way to find out something about its structure is the file
command-line tool (for Windows, included in Rtools).  This
reports something like
 Modern Unix-alike systems, including OS X, are likely to produce
UTF-8 files.  Windows may produce what it calls ‘Unicode’ files
(UCS-2LE or just possibly UTF-16LE1).  Otherwise most files will be in a
8-bit encoding unless from a Chinese/Japanese/Korean locale (which have
a wide range of encodings in common use).  It is not possible to
automatically detect with certainty which 8-bit encoding (although
guesses may be possible and file may guess as it did in the
example above), so you may simply have to ask the originator for some
clues (e.g. ‘Russian on Windows’).
 ‘BOMs’ (Byte Order Marks,
https://en.wikipedia.org/wiki/Byte_order_mark) cause problems for
Unicode files.  In the Unix world BOMs are rarely used, whereas in the
Windows world they almost always are for UCS-2/UTF-16 files, and often
are for UTF-8 files.  The file utility will not even recognize
UCS-2 files without a BOM, but many other utilities will refuse to read
files with a BOM and the IANA standards for UTF-16LE
and UTF-16BE prohibit it.  We have too often been reduced to
looking at the file with the command-line utility od or a hex
editor to work out its encoding.
 Note that utf8 is not a valid encoding name (UTF-8 is),
and macintosh is the most portable name for what is sometimes
called ‘Mac Roman’ encoding. 
 
Next: XML, Previous: Imports, Up: Introduction   [Contents][Index] Exporting results from R is usually a less contentious task, but
there are still a number of pitfalls.  There will be a target
application in mind, and normally a text file will be the most
convenient interchange vehicle.  (If a binary file is required, see
Binary files.)
 Function cat underlies the functions for exporting data.  It
takes a file argument, and the append argument allows a
text file to be written via successive calls to cat.  Better,
especially if this is to be done many times, is to open a file
connection for writing or appending, and cat to that connection,
then close it.
 The most common task is to write a matrix or data frame to file as a
rectangular grid of numbers, possibly with row and column labels.  This
can be done by the functions write.table and write.
Function write just writes out a matrix or vector in a specified
number of columns (and transposes a matrix).  Function
write.table is more convenient, and writes out a data frame (or
an object that can be coerced to a data frame) with row and column
labels.
 There are a number of issues that need to be considered in writing out a
data frame to a text file.
 Most of the conversions of real/complex numbers done by these functions
is to full precision, but those by write are governed by the
current setting of options(digits).  For more control, use
format on a data frame, possibly column-by-column.
 R prefers the header line to have no entry for the row names, so the
file looks like
 Some other systems require a (possibly empty) entry for the row names, which
is what write.table will provide if argument col.names = NA
is specified.  Excel is one such system.
 A common field separator to use in the file is a comma, as that is
unlikely to appear in any of the fields in English-speaking countries.
Such files are known as CSV (comma separated values) files, and wrapper
function write.csv provides appropriate defaults.  In some
locales the comma is used as the decimal point (set this in
write.table by dec = ",") and there CSV files use the
semicolon as the field separator: use write.csv2 for appropriate
defaults.  There is an IETF standard for CSV files (which mandates
commas and CRLF line endings, for which use eol = "\r\n"), RFC4180
(see https://tools.ietf.org/html/rfc4180), but what is more
important in practice is that the file is readable by the application it
is targeted at.
 Using a semicolon or tab (sep = "\t") are probably the safest
options.
 By default missing values are output as NA, but this may be
changed by argument na.  Note that NaNs are treated as
NA by write.table, but not by cat nor write.
 By default strings are quoted (including the row and column names).
Argument quote controls if  character and factor variables are
quoted: some programs, for example Mondrian, do not accept quoted
strings (which are the default).
 Some care is needed if the strings contain embedded quotes.  Three
useful forms are
 The second is the form of escape commonly used by spreadsheets.
 Text files do not contain metadata on their encodings, so for
non-ASCII data the file needs to be targetted to the
application intended to read it.  All of these functions can write to a
connection which allows an encoding to be specified for the file,
and write.table has a fileEncoding argument to make this
easier.
 The hard part is to know what file encoding to use.  For use on Windows,
it is best to use what Windows calls ‘Unicode’2, that is "UTF-16LE".  Using UTF-8 is a good way
to make portable files that will not easily be confused with any other
encoding, but even OS X applications (where UTF-8 is the system
encoding) may not recognize them, and Windows applications are most
unlikely to.  Apparently Excel:mac 2004/8 expects .csv files in
"macroman" encoding (the encoding used in much earlier versions
of Mac OS).
 Function write.matrix in package MASS provides a
specialized interface for writing matrices, with the option of writing
them in blocks and thereby reducing memory usage.
 It is possible to use sink to divert the standard R output to
a file, and thereby capture the output of (possibly implicit)
print statements.  This is not usually the most efficient route,
and the options(width) setting may need to be increased.
 Function write.foreign in package foreign uses
write.table to produce a text file and also writes a code file
that will read this text file into another statistical package. There is
currently support for export to SAS, SPSS and Stata.
 
Previous: Export to text files, Up: Introduction   [Contents][Index] When reading data from text files, it is the responsibility of the user
to know and to specify the conventions used to create that file,
e.g. the comment character, whether a header line is present, the value
separator, the representation for missing values (and so on) described
in Export to text files.  A markup language which can be used to
describe not only content but also the structure of the content can
make a file self-describing, so that one need not provide these details
to the software reading the data.
 The eXtensible Markup Language – more commonly known simply as
XML – can be used to provide such structure, not only for
standard datasets but also more complex data structures.
XML is becoming extremely popular and is emerging as a
standard for general data markup and exchange.  It is being used by
different communities to describe geographical data such as maps,
graphical displays, mathematics and so on.
 XML provides a way to specify the file’s encoding, e.g.
 although it does not require it.
 The XML package provides general facilities for reading and
writing XML documents within R. 
Package StatDataML on CRAN is one example building
on XML.
 NB: XML is available as a binary package for Windows, normally
from the ‘CRAN extras’ repository (which is selected by default on
Windows).
 
Next: Importing from other statistical systems, Previous: Introduction, Up: Top   [Contents][Index] In Export to text files we saw a number of variations on the
format of a spreadsheet-like text file, in which the data are presented
in a rectangular grid, possibly with row and column labels.  In this
section we consider importing such files into R.
 
Next: Fixed-width-format files, Previous: Spreadsheet-like data, Up: Spreadsheet-like data   [Contents][Index] The function read.table is the most convenient way to read in a
rectangular grid of data.  Because of the many possibilities, there are
several other functions that call read.table but change a group
of default arguments.
 Beware that read.table is an inefficient way to read in
very large numerical matrices: see scan below.
 Some of the issues to consider are:
 If the file contains non-ASCII character fields, ensure that
it is read in the correct encoding.  This is mainly an issue for reading
Latin-1 files in a UTF-8 locale, which can be done by something like
 Note that this will work in any locale which can represent Latin-1
strings, but not many Greek/Russian/Chinese/Japanese … locales.
 We recommend that you specify the header argument explicitly,
Conventionally the header line has entries only for the columns and not
for the row labels, so is one field shorter than the remaining lines.
(If R sees this, it sets header = TRUE.)  If presented with a
file that has a (possibly empty) header field for the row labels, read
it in by something like
 Column names can be given explicitly via the col.names; explicit
names override the header line (if present).
 Normally looking at the file will determine the field separator to be
used, but with white-space separated files there may be a choice between
the default sep = "" which uses any white space (spaces, tabs or
newlines) as a separator, sep = " " and sep = "\t".  Note
that the choice of separator affects the input of quoted strings.  
 If you have a tab-delimited file containing empty fields be sure to use
sep = "\t".
 By default character strings can be quoted by either ‘"’ or
‘'’, and in each case all the characters up to a matching quote are
taken as part of the character string.  The set of valid quoting
characters (which might be none) is controlled by the quote
argument.  For sep = "\n" the default is changed to quote =
"".
 If no separator character is specified, quotes can be escaped within
quoted strings by immediately preceding them by ‘\’, C-style.
 If a separator character is specified, quotes can be escaped within
quoted strings by doubling them as is conventional in spreadsheets.  For
example
 can be read by
 This does not work with the default separator.
 By default the file is assumed to contain the character string NA
to represent missing values, but this can be changed by the argument
na.strings, which is a vector of one or more character
representations of missing values.
 Empty fields in numeric columns are also regarded as missing values.
 In numeric columns, the values NaN, Inf and -Inf are
accepted.
 It is quite common for a file exported from a spreadsheet to have all
trailing empty fields (and their separators) omitted.  To read such
files set fill = TRUE.
 If a separator is specified, leading and trailing white space in
character fields is regarded as part of the field.  To strip the space,
use argument strip.white = TRUE.
 By default, read.table ignores empty lines.  This can be changed
by setting blank.lines.skip = FALSE, which will only be useful in
conjunction with fill = TRUE, perhaps to use blank rows to
indicate missing cases in a regular layout.
 Unless you take any special action, read.table reads all the
columns as character vectors and then tries to select a suitable class
for each variable in the data frame.  It tries in turn logical,
integer, numeric and complex, moving on if any
entry is not missing and cannot be converted.3
If all of these fail, the variable is converted to a factor.
 Arguments colClasses and as.is provide greater control.
Specifying as.is = TRUE suppresses conversion of character
vectors to factors (only).  Using colClasses allows the desired
class to be set for each column in the input: it will be faster and use
less memory.
 Note that colClasses and as.is are specified per 
column, not per variable, and so include the column of row names
(if any).
 By default, read.table uses ‘#’ as a comment character,
and if this is encountered (except in quoted strings) the rest of the
line is ignored. Lines containing only white space and a comment are
treated as blank lines.
 If it is known that there will be no comments in the data file, it is
safer (and may be faster) to use comment.char = "".
 Many OSes have conventions for using backslash as an escape character in
text files, but Windows does not (and uses backslash in path names).
It is optional in R whether such conventions are applied to data files.
 Both read.table and scan have a logical argument
allowEscapes.  This is false by default, and backslashes are then
only interpreted as (under circumstances described above) escaping
quotes.  If this set to be true, C-style escapes are interpreted, namely
the control characters \a, \b, \f, \n, \r, \t, \v and octal and
hexadecimal representations like \040 and \0x2A.  Any
other escaped character is treated as itself, including backslash.  Note
that Unicode escapes such as \uxxxx are never interpreted.
 This can be specified by the fileEncoding argument, for example
 If you know (correctly) the file’s encoding this will almost always
work.  However, we know of one exception, UTF-8 files with a BOM.  Some
people claim that UTF-8 files should never have a BOM, but some software
(apparently including Excel:mac) uses them, and many Unix-alike OSes do
not accept them.  So faced with a file which file reports as
 it can be read on Windows by
 but on a Unix-alike might need
 (This would most likely work without specifying an encoding in a UTF-8 locale.)
 Another problem with this (real-life) example is that whereas
file-5.03 reported the BOM, file-4.17 found on OS
10.5 (Leopard) did not.
 Convenience functions read.csv and read.delim provide
arguments to read.table appropriate for CSV and tab-delimited
files exported from spreadsheets in English-speaking locales.  The
variations read.csv2 and read.delim2 are appropriate for
use in those locales where the comma is used for the decimal point and
(for read.csv2) for spreadsheets which use semicolons to separate
fields.
 If the options to read.table are specified incorrectly, the error
message will usually be of the form
 or
 This may give enough information to find the problem, but the auxiliary
function count.fields can be useful to investigate further.
 Efficiency can be important when reading large data grids.  It will help
to specify comment.char = "", colClasses as one of the
atomic vector types (logical, integer, numeric, complex, character or
perhaps raw) for each column, and to give nrows, the number of
rows to be read (and a mild over-estimate is better than not specifying
this at all).  See the examples in later sections.
 
Next: Data Interchange Format (DIF), Previous: Variations on read.table, Up: Spreadsheet-like data   [Contents][Index] Sometimes data files have no field delimiters but have fields in
pre-specified columns.  This was very common in the days of punched
cards, and is still sometimes used to save file space.
 Function read.fwf provides a simple way to read such files,
specifying a vector of field widths.  The function reads the file into
memory as whole lines, splits the resulting character strings, writes
out a temporary tab-separated file and then calls read.table.
This is adequate for small files, but for anything more complicated we
recommend using the facilities of a language like perl to
pre-process the file.

 Function read.fortran is a similar function for fixed-format files,
using Fortran-style column specifications.
 
Next: Using scan directly, Previous: Fixed-width-format files, Up: Spreadsheet-like data   [Contents][Index] An old format sometimes used for spreadsheet-like data is DIF, or Data Interchange 
format.  
 Function read.DIF provides a simple way to read such files.  It takes 
arguments similar to read.table for assigning types to each of the columns.
 On Windows, spreadsheet programs often store spreadsheet data copied to
the clipboard in this format; read.DIF("clipboard") can read it
from there directly.  It is slightly more robust than
read.table("clipboard") in handling spreadsheets with empty
cells.
 
Next: Re-shaping data, Previous: Data Interchange Format (DIF), Up: Spreadsheet-like data   [Contents][Index] Both read.table and read.fwf use scan to read the
file, and then process the results of scan.  They are very
convenient, but sometimes it is better to use scan directly.
 Function scan has many arguments, most of which we have already
covered under read.table.  The most crucial argument is
what, which specifies a list of modes of variables to be read
from the file.  If the list is named, the names are used for the
components of the returned list.  Modes can be numeric, character or
complex, and are usually specified by an example, e.g. 0,
"" or 0i.  For example
 returns a list with three components and discards the fourth column in
the file.
 There is a function readLines which will be more convenient if
all you want is to read whole lines into R for further processing.
 One common use of scan is to read in a large matrix.  Suppose
file matrix.dat just contains the numbers for a 200 x 2000
matrix. Then we can use
 On one test this took 1 second (under Linux, 3 seconds under Windows on
the same machine) whereas
 took 10 seconds (and more memory), and 
 took 7 seconds.  The difference is almost entirely due to the overhead
of reading 2000 separate short columns: were they of length 2000,
scan took 9 seconds whereas read.table took 18 if used
efficiently (in particular, specifying colClasses) and 125 if
used naively.
 Note that timings can depend on the type read and the data. 
Consider reading a million distinct integers:
 and a million examples of a small set of codes:
 Note that these timings depend heavily on the operating system (the
basic reads in Windows take at least as twice as long as these Linux
times) and on the precise state of the garbage collector.
 
Next: Flat contingency tables, Previous: Using scan directly, Up: Spreadsheet-like data   [Contents][Index] Sometimes spreadsheet data is in a compact format that gives the
covariates for each subject followed by all the observations on that
subject.  R’s modelling functions need observations in a single
column.  Consider the following sample of data from repeated MRI brain
measurements
 There are two covariates and up to four measurements on each subject.
The data were exported from Excel as a file mr.csv.
 We can use stack to help manipulate these data to give a single
response.
 with result
 Function unstack goes in the opposite direction, and may be
useful for exporting data.
 Another way to do this is to use the function
reshape, by
 The reshape function has a more complicated syntax than
stack but can be used for data where the ‘long’ form has more
than the one column in this example. With direction="wide",
reshape can also perform the opposite transformation.
 Some people prefer the tools in packages reshape,
reshape2 and plyr.
 
Previous: Re-shaping data, Up: Spreadsheet-like data   [Contents][Index] Displaying higher-dimensional contingency tables in array form typically
is rather inconvenient.  In categorical data analysis, such information
is often represented in the form of bordered two-dimensional arrays with
leading rows and columns specifying the combination of factor levels
corresponding to the cell counts.  These rows and columns are typically
“ragged” in the sense that labels are only displayed when they change,
with the obvious convention that rows are read from top to bottom and
columns are read from left to right.  In R, such “flat” contingency
tables can be created using ftable,

which creates objects of class "ftable" with an appropriate print
method.
 As a simple example, consider the R standard data set
UCBAdmissions which is a 3-dimensional contingency table
resulting from classifying applicants to graduate school at UC Berkeley
for the six largest departments in 1973 classified by admission and sex.
 The printed representation is clearly more useful than displaying the
data as a 3-dimensional array.
 There is also a function read.ftable for reading in flat-like
contingency tables from files.

This has additional arguments for dealing with variants on how exactly
the information on row and column variables names and levels is
represented.  The help page for read.ftable has some useful
examples.  The flat tables can be converted to standard contingency
tables in array form using as.table.
 Note that flat tables are characterized by their “ragged” display of
row (and maybe also column) labels.  If the full grid of levels of the
row variables is given, one should instead use read.table to read
in the data, and create the contingency table from this using
xtabs.
 
Next: Relational databases, Previous: Spreadsheet-like data, Up: Top   [Contents][Index] In this chapter we consider the problem of reading a binary data file
written by another statistical system.  This is often best avoided, but
may be unavoidable if the originating system is not available. 
 In all cases the facilities described were written for data files from
specific versions of the other system (often in the early 2000s), and
have not necessarily been updated for the most recent versions of the
other system.
 
Next: Octave, Previous: Importing from other statistical systems, Up: Importing from other statistical systems   [Contents][Index] The recommended package foreign provides import facilities for
files produced by these statistical systems, and for export to Stata. In
some cases these functions may require substantially less memory than
read.table would. write.foreign (See Export to text files) provides an export mechanism with support currently for
SAS, SPSS and Stata.
 EpiInfo versions 5 and 6 stored data in a  self-describing fixed-width
text format. read.epiinfo will read these .REC files into
an R data frame. EpiData also produces data in this format.
 Function read.mtp imports a ‘Minitab Portable Worksheet’.  This
returns the components of the worksheet as an R list.
 Function read.xport reads a file in SAS Transport (XPORT) format
and return a list of data frames.  If SAS is available on your system,
function read.ssd can be used to create and run a SAS script that
saves a SAS permanent dataset (.ssd or .sas7bdat) in
Transport format.  It then calls read.xport to read the resulting
file.  (Package Hmisc has a similar function sas.get, also
running SAS.)  For those without access to SAS but running on Windows,
the SAS System Viewer (a zero-cost download) can be used to open SAS
datasets and export them to e.g. .csv format.
 Function read.S which can read binary objects produced by S-PLUS
3.x, 4.x or 2000 on (32-bit) Unix or Windows (and can read them on a
different OS).  This is able to read many but not all S objects: in
particular it can read vectors, matrices and data frames and lists
containing those.
 Function data.restore reads S-PLUS data dumps (created by
data.dump) with the same restrictions (except that dumps from the
Alpha platform can also be read).  It should be possible to read data
dumps from S-PLUS 5.x and later written with data.dump(oldStyle=T).
 If you have access to S-PLUS, it is usually more reliable to dump
the object(s) in S-PLUS and source the dump file in R.  For
S-PLUS 5.x and later you may need to use dump(..., oldStyle=T),
and to read in very large objects it may be preferable to use the dump
file as a batch script rather than use the source function.
 Function read.spss can read files created by the ‘save’ and
‘export’ commands in SPSS.  It returns a list with one
component for each variable in the saved data set. SPSS
variables with value labels are optionally converted to R factors.
 SPSS Data Entry is an application for creating data entry
forms.  By default it creates data files with extra formatting
information that read.spss cannot handle, but it is possible to
export the data in an ordinary SPSS format.
 Some third-party applications claim to produce data ‘in SPSS format’ but
with differences in the formats: read.spss may or may not be able
to handle these.
 Stata .dta files are a binary file format. Files from versions 5
up to 11 of Stata can be read and written by functions read.dta
and write.dta.  Stata variables with value labels are optionally
converted to (and from) R factors.  Stata version 12 by default
writes ‘format-115 datasets’: read.dta currently may not be able
to read those.
 read.systat reads those Systat SAVE files that are
rectangular data files (mtype = 1) written on little-endian
machines (such as from Windows).  These have extension .sys
or (more recently) .syd.
 
Previous: EpiInfo Minitab SAS S-PLUS SPSS Stata Systat, Up: Importing from other statistical systems   [Contents][Index] Octave is a numerical linear algebra system
(http://www.octave.org), and function read.octave in
package foreign can read in files in Octave text data format
created using the Octave command save -ascii, with support for
most of the common types of variables, including the standard atomic
(real and complex scalars, matrices, and N-d arrays, strings,
ranges, and boolean scalars and matrices) and recursive (structs, cells,
and lists) ones.
 
Next: Binary files, Previous: Importing from other statistical systems, Up: Top   [Contents][Index] 
Next: Overview of RDBMSs, Previous: Relational databases, Up: Relational databases   [Contents][Index] There are limitations on the types of data that R handles well.
Since all data being manipulated by R are resident in memory, and
several copies of the data can be created during execution of a
function, R is not well suited to extremely large data sets.  Data
objects that are more than a (few) hundred megabytes in size can cause
R to run out of memory, particularly on a 32-bit operating system.
 R does not easily support concurrent access to data.  That is, if
more than one user is accessing, and perhaps updating, the same data,
the changes made by one user will not be visible to the others.
 R does support persistence of data, in that you can save a data
object or an entire worksheet from one session and restore it at the
subsequent session, but the format of the stored data is specific to
R and not easily manipulated by other systems.
 Database management systems (DBMSs) and, in particular, relational
DBMSs (RDBMSs) are designed to do all of these things well.
Their strengths are
 The sort of statistical applications for which DBMS might be used are to
extract a 10% sample of the data, to cross-tabulate data to produce a
multi-dimensional contingency table, and to extract data group by group
from a database for separate analysis.
 Increasingly OSes are themselves making use of DBMSs for these reasons,
so it is nowadays likely that one will be already installed on your
(non-Windows) OS.  Akonadi
is used by KDE4 to store personal information.  Several OS X
applications, including Mail and Address Book, use SQLite.
 
Next: R interface packages, Previous: Why use a database?, Up: Relational databases   [Contents][Index] Traditionally there had been large (and expensive) commercial RDBMSs
(Informix; Oracle; Sybase;
IBM’s DB2;
Microsoft SQL
Server on Windows) and academic and small-system databases (such as
MySQL4, PostgreSQL, Microsoft
Access, …), the former marked out by much greater emphasis on data
security features.  The line is blurring, with MySQL and PostgreSQL
having more and more high-end features, and free ‘express’ versions
being made available for the commercial DBMSs.
 There are other commonly used data sources, including spreadsheets,
non-relational databases and even text files (possibly compressed).
Open Database Connectivity (ODBC) is a standard to use all of
these data sources.  It originated on Windows (see
https://msdn.microsoft.com/en-us/library/ms710252%28v=vs.85%29.aspx)
but is also implemented on Linux/Unix/OS X.
 All of the packages described later in this chapter provide clients to
client/server databases.  The database can reside on the same machine or
(more often) remotely.  There is an ISO standard (in fact
several: SQL92 is ISO/IEC 9075, also known as
ANSI X3.135-1992, and SQL99 is coming into use) for
an interface language called SQL (Structured Query Language,
sometimes pronounced ‘sequel’: see Bowman et al. 1996 and Kline
and Kline 2001) which these DBMSs support to varying degrees.
 
Next: Data types, Previous: Overview of RDBMSs, Up: Overview of RDBMSs   [Contents][Index] The more comprehensive R interfaces generate SQL behind the
scenes for common operations, but direct use of SQL is needed
for complex operations in all.  Conventionally SQL is written
in upper case, but many users will find it more convenient to use lower
case in the R interface functions.
 A relational DBMS stores data as a database of tables (or
relations) which are rather similar to R data frames, in that
they are made up of columns or fields of one type
(numeric, character, date, currency, …) and rows or
records containing the observations for one entity.
 SQL ‘queries’ are quite general operations on a relational
database.  The classical query is a SELECT statement of the type
 The first of these selects two columns from the R data frame
USArrests that has been copied across to a database table,
subsets on a third column and asks the results be sorted.  The second
performs a database join on two tables student and
school and returns four columns.  The third and fourth queries do
some cross-tabulation and return counts or averages.  (The five
aggregation functions are COUNT(*) and SUM, MAX, MIN and AVG, each
applied to a single column.)
 SELECT queries use FROM to select the table, WHERE to specify a
condition for inclusion (or more than one condition separated by AND or
OR), and ORDER BY to sort the result.  Unlike data frames, rows in RDBMS
tables are best thought of as unordered, and without an ORDER BY
statement the ordering is indeterminate.  You can sort (in
lexicographical order) on more than one column by separating them by
commas.  Placing DESC after an ORDER BY puts the sort in descending
order.
 SELECT DISTINCT queries will only return one copy of each distinct row
in the selected table.
 The GROUP BY clause selects subgroups of the rows according to the
criterion.  If more than one column is specified (separated by commas)
then multi-way cross-classifications can be summarized by one of the
five aggregation functions.  A HAVING clause allows the select to
include or exclude groups depending on the aggregated value.
 If the SELECT statement contains an ORDER BY statement that produces a
unique ordering, a LIMIT clause can be added to select (by number) a
contiguous block of output rows.  This can be useful to retrieve rows a
block at a time.  (It may not be reliable unless the ordering is unique,
as the LIMIT clause can be used to optimize the query.)
 There are queries to create a table (CREATE TABLE, but usually one
copies a data frame to the database in these interfaces), INSERT or
DELETE or UPDATE data.  A table is destroyed by a DROP TABLE ‘query’.
 Kline and Kline (2001) discuss the details of the implementation of SQL
in Microsoft SQL Server 2000, Oracle, MySQL and PostgreSQL.
 
Previous: SQL queries, Up: Overview of RDBMSs   [Contents][Index] Data can be stored in a database in various data types.  The range of
data types is DBMS-specific, but the SQL standard defines many
types, including the following that are widely implemented (often not by
the SQL name).
 Real number, with optional precision.  Often called real or
double or double precision.
 32-bit integer.  Often called int.
 16-bit integer
 fixed-length character string.  Often called char.
 variable-length character string.  Often called varchar.  Almost
always has a limit of 255 chars.
 true or false.  Sometimes called bool or bit.
 calendar date
 time of day
 date and time
 There are variants on time and timestamp, with
timezone.  Other types widely implemented are text and
blob, for large blocks of text and binary data, respectively.
 The more comprehensive of the R interface packages hide the type
conversion issues from the user.
 
Previous: Overview of RDBMSs, Up: Relational databases   [Contents][Index] There are several packages available on CRAN to help R
communicate with DBMSs.  They provide different levels of abstraction.
Some provide means to copy whole data frames to and from databases.  All
have functions to select data within the database via SQL
queries, and to retrieve the result as a whole as a
data frame or in pieces (usually as groups of rows).  
 All except RODBC are tied to one DBMS, but there has been a
proposal for a unified ‘front-end’ package DBI
(https://developer.r-project.org/db) in conjunction with a
‘back-end’, the most developed of which is RMySQL.  Also on
CRAN are the back-ends ROracle, RPostgreSQL and
RSQLite (which works with the bundled DBMS SQLite,
https://www.sqlite.org),  RJDBC (which uses Java and can
connect to any DBMS that has a JDBC driver) and RpgSQL (a
specialist interface to PostgreSQL built on top of RJDBC).
 The BioConductor project has updated RdbiPgSQL (formerly on
CRAN ca 2000), a first-generation interface to PostgreSQL.
 PL/R (http://www.joeconway.com/plr/) is a project to embed R into
PostgreSQL.
 Package RMongo provides an R interface to a Java client for
‘MongoDB’ (https://en.wikipedia.org/wiki/MongoDB) databases, which
are queried using JavaScript rather than SQL.  Package rmongodb is
another client using mongodb’s C driver.
 
Next: RODBC, Previous: R interface packages, Up: R interface packages   [Contents][Index] Package RMySQL on CRAN provides an interface to the
MySQL database system (see https://www.mysql.com and Dubois,
2000) or its fork MariaDB (see https://mariadb.org/).  The
description here applies to versions 0.5-0 and later: earlier
versions had a substantially different interface.  The current version
requires the DBI package, and this description will apply with
minor changes to all the other back-ends to DBI.
 MySQL exists on Unix/Linux/OS X and Windows: there is a ‘Community
Edition’ released under GPL but commercial licenses are also available.
MySQL was originally a ‘light and lean’ database.  (It preserves the
case of names where the operating file system is case-sensitive, so not
on Windows.)
 The call dbDriver("MySQL") returns a database connection manager
object, and then a call to dbConnect opens a database connection
which can subsequently be closed by a call to the generic function
dbDisconnect.  Use dbDriver("Oracle"),
dbDriver("PostgreSQL") or dbDriver("SQLite") with those
DBMSs and packages ROracle, RPostgreSQL or RSQLite
respectively.
 SQL queries can be sent by either dbSendQuery or
dbGetQuery.  dbGetquery sends the query and retrieves the
results as a data frame.  dbSendQuery sends the query and returns
an object of class inheriting from "DBIResult" which can be used
to retrieve the results, and subsequently used in a call to
dbClearResult to remove the result.
 Function fetch is used to retrieve some or all of the rows in the
query result, as a list.  The function dbHasCompleted indicates if
all the rows have been fetched, and dbGetRowCount returns the
number of rows in the result.
 These are convenient interfaces to read/write/test/delete tables in the
database.  dbReadTable and dbWriteTable copy to and from
an R data frame, mapping the row names of the data frame to the field
row_names in the MySQL table.
 
Previous: DBI, Up: R interface packages   [Contents][Index] Package RODBC on CRAN provides an interface to
database sources supporting an ODBC interface.  This is very
widely available, and allows the same R code to access different
database systems.  RODBC runs on Unix/Linux, Windows and OS X,
and almost all database systems provide support for ODBC.  We
have tested Microsoft SQL Server, Access, MySQL, PostgreSQL, Oracle and
IBM DB2 on Windows and MySQL, MariaDB, Oracle, PostgreSQL and SQLite on
Linux.
 ODBC is a client-server system, and we have happily connected to a DBMS
running on a Unix server from a Windows client, and vice versa.
 On Windows ODBC support is part of the OS.  On Unix/Linux you will need
an ODBC Driver Manager such as unixODBC
(http://www.unixODBC.org) or iOBDC (http://www.iODBC.org:
this is pre-installed in OS X) and an installed driver for your
database system.
 Windows provides drivers not just for DBMSs but also for Excel
(.xls) spreadsheets, DBase (.dbf) files and even text
files.  (The named applications do not need to be
installed. Which file formats are supported depends on the versions of
the drivers.)  There are versions for Excel and Access 2007/2010 (go to
https://www.microsoft.com/en-us/download/default.aspx, and
search for ‘Office ODBC’, which will lead to
AccessDatabaseEngine.exe), the ‘2007 Office System Driver’ (the
latter has a version for 64-bit Windows, and that will also read earlier
versions).
 On OS X the Actual Technologies
(https://www.actualtech.com/product_access.php) drivers
provide ODBC interfaces to Access databases (including Access 2007/2010)
and to Excel spreadsheets (not including Excel 2007/2010).
 Many simultaneous connections are possible.  A connection is opened by a
call to odbcConnect or odbcDriverConnect (which on the
Windows GUI allows a database to be selected via dialog boxes) which
returns a handle used for subsequent access to the database.  Printing a
connection will provide some details of the ODBC connection, and calling
odbcGetInfo will give details on the client and server.
 A connection is closed by a call to close or odbcClose,
and also (with a warning) when not R object refers to it and at the end
of an R session.
 Details of the tables on a connection can be found using
sqlTables.
 Function sqlSave copies an R data frame to a table in the
database, and sqlFetch copies a table in the database to an R
data frame.
 An SQL query can be sent to the database by a call to
sqlQuery.  This returns the result in an R data frame.
(sqlCopy sends a query to the database and saves the result as a
table in the database.)  A finer level of control is attained by first
calling odbcQuery and then sqlGetResults to fetch the
results.  The latter can be used within a loop to retrieve a limited
number of rows at a time, as can function sqlFetchMore.
 Here is an example using PostgreSQL, for which the ODBC driver
maps column and data frame names to lower case.  We use a database
testdb we created earlier, and had the DSN (data source name) set
up in ~/.odbc.ini under unixODBC.  Exactly the same code
worked using MyODBC to access a MySQL database under Linux or Windows
(where MySQL also maps names to lowercase).  Under Windows,
DSNs are set up in the ODBC applet in the Control
Panel (‘Data Sources (ODBC)’ in the ‘Administrative Tools’ section).

 As a simple example of using ODBC under Windows with a Excel
spreadsheet, we can read from a spreadsheet by
 Notice that the specification of the table is different from the name
returned by sqlTables: sqlFetch is able to map the
differences.
 
Next: Image files, Previous: Relational databases, Up: Top   [Contents][Index] Binary connections (Connections) are now the preferred way to
handle binary files.
 
Next: dBase files (DBF), Previous: Binary files, Up: Binary files   [Contents][Index] Packages hdf5, h5r, Bioconductor’s rhdf5,
RNetCDF, ncdf and ncdf4 on CRAN provide
interfaces to NASA’s HDF5 (Hierarchical Data Format, see
https://www.hdfgroup.org/HDF5/) and to UCAR’s netCDF data files
(network Common Data Form, see
http://www.unidata.ucar.edu/software/netcdf/).
 Both of these are systems to store scientific data in array-oriented
ways, including descriptions, labels, formats, units, ….  HDF5 also
allows groups of arrays, and the R interface maps lists
to HDF5 groups, and can write numeric and character vectors and
matrices.
 NetCDF’s version 4 format (confusingly, implemented in netCDF 4.1.1 and
later, but not in 4.0.1) includes the use of various HDF5 formats.  This
is handled by package ncdf4 whereas RNetCDF and
ncdf handle version 3 files.
 The availability of software to support these formats is somewhat
limited by platform, especially on Windows.
 
Previous: Binary data formats, Up: Binary files   [Contents][Index] dBase was a DOS program written by Ashton-Tate and later owned by
Borland which has a binary flat-file format that became popular, with
file extension .dbf.  It has been adopted for the ’Xbase’ family
of databases, covering dBase, Clipper, FoxPro and their Windows
equivalents Visual dBase, Visual Objects and Visual FoxPro (see
http://www.e-bachmann.dk/docs/xbase.htm).  A dBase file contains
a header and then a series of fields and so is most similar to an R
data frame.  The data itself is stored in text format, and can include
character, logical and numeric fields, and other types in later versions
(see for example
http://www.digitalpreservation.gov/formats/fdd/fdd000325.shtml
and
http://www.clicketyclick.dk/databases/xbase/format/index.html).
 Functions read.dbf and write.dbf provide ways to read and
write basic DBF files on all R platforms.  For Windows users
odbcConnectDbase in package RODBC provides more
comprehensive facilities to read DBF files via Microsoft’s dBase
ODBC driver (and the Visual FoxPro driver can also be used via
odbcDriverConnect).

 
Next: Connections, Previous: Binary files, Up: Top   [Contents][Index] A particular class of binary files are those representing images, and a
not uncommon request is to read such a file into R as a matrix.
 There are many formats for image files (most with lots of variants), and
it may be necessary to use external conversion software to first convert
the image into one of the formats for which a package currently provides
an R reader.  A versatile example of such software is ImageMagick and
its fork GraphicsMagick.  These provide command-line programs
convert and gm convert to convert images from one
format to another: what formats they can input is determined when they
are compiled, and the supported formats can be listed by e.g.
convert -list format.
 Package pixmap has a function read.pnm to read ‘portable
anymap’ images in PBM (black/white), PGM (grey) and PPM (RGB colour)
formats.  These are also known as ‘netpbm’ formats.
 Packages bmp, jpeg and png read the
formats after which they are named.  See also packages biOps
and Momocs, and Bioconductor package EBImage.
 TIFF is more a meta-format, a wrapper within which a very large variety
of image formats can be embedded.  Packages rtiff (orphaned)
and tiff can read some of the sub-formats (depending on the
external libtiff software against which they are compiled).
There some facilities for specialized sub-formats, for example in
Bioconductor package beadarray.
 Raster files are common in the geographical sciences, and package
rgdal provides an interface to GDAL which provides some
facilities of its own to read raster files and links to many others.
Which formats it supports is determined when GDAL is compiled: use
gdalDrivers() to see what these are for the build you are using.
It can be useful for uncommon formats such as JPEG 2000 (which is a
different format from JPEG, and not currently supported in the OS X
nor Windows binary versions of rgdal).
 
Next: Network interfaces, Previous: Image files, Up: Top   [Contents][Index] Connections are used in R in the sense of Chambers (1998) and
Ripley (2001), a set of functions to replace the use of file names by a
flexible interface to file-like objects.
 
Next: Output to connections, Previous: Connections, Up: Connections   [Contents][Index] The most familiar type of connection will be a file, and file
connections are created by function file.  File connections can
(if the OS will allow it for the particular file) be opened for reading
or writing or appending, in text or binary mode.  In fact, files can be
opened for both reading and writing, and R keeps a separate file
position for reading and writing.
 Note that by default a connection is not opened when it is created.  The
rule is that a function using a connection should open a connection
(needed) if the connection is not already open, and close a connection
after use if it opened it.  In brief, leave the connection in the state
you found it in.   There are generic functions open and
close with methods to explicitly open and close connections.
 Files compressed via the algorithm used by gzip can be used as
connections created by the function gzfile, whereas files
compressed by bzip2 can be used via bzfile.
 Unix programmers are used to dealing with special files stdin,
stdout and stderr.   These exist as terminal
connections in R.  They may be normal files, but they might also
refer to input from and output to a GUI console.  (Even with the standard
Unix R interface, stdin refers to the lines submitted from
readline rather than a file.)
 The three terminal connections are always open, and cannot be opened or
closed.  stdout and stderr are conventionally used for
normal output and error messages respectively.  They may normally go to
the same place, but whereas normal output can be re-directed by a call
to sink, error output is sent to stderr unless re-directed
by sink, type="message").  Note carefully the language used here:
the connections cannot be re-directed, but output can be sent to other
connections.
 Text connections are another source of input.  They allow R
character vectors to be read as if the lines were being read from a text
file.  A text connection is created and opened by a call to
textConnection, which copies the current contents of the
character vector to an internal buffer at the time of creation.
 Text connections can also be used to capture R output to a character
vector.  textConnection can be asked to create a new character
object or append to an existing one, in both cases in the user’s
workspace.  The connection is opened by the call to
textConnection, and at all times the complete lines output to the
connection are available in the R object.  Closing the connection
writes any remaining output to a final element of the character vector.
 Pipes are a special form of file that connects to another
process, and pipe connections are created by the function pipe.
Opening a pipe connection for writing (it makes no sense to append to a
pipe) runs an OS command, and connects its standard input to whatever
R then writes to that connection.  Conversely, opening a pipe
connection for input runs an OS command and makes its standard output
available for R input from that connection.
 URLs of types ‘http://’, ‘ftp://’ and ‘file://’
can be read from using the function url.  For convenience,
file will also accept these as the file specification and call
url.  On most platforms ‘https://’ are also accepted.
 Sockets can also be used as connections via function
socketConnection on platforms which support Berkeley-like sockets
(most Unix systems, Linux and Windows).  Sockets can be written to or
read from, and both client and server sockets can be used.
 
Next: Input from connections, Previous: Types of connections, Up: Connections   [Contents][Index] We have described functions cat, write, write.table
and sink as writing to a file, possibly appending to a file if 
argument append = TRUE, and this is what they did prior to R
version 1.2.0.
 The current behaviour is equivalent, but what actually happens is that
when the file argument is a character string, a file connection
is opened (for writing or appending) and closed again at the end of the
function call.  If we want to repeatedly write to the same file, it is
more efficient to explicitly declare and open the connection, and pass
the connection object to each call to an output function.  This also
makes it possible to write to pipes, which was implemented earlier in a
limited way via the syntax file = "|cmd" (which can still be
used).
 There is a function writeLines to write complete text lines
to a connection.
 Some simple examples are
 
Next: Listing and manipulating connections, Previous: Output to connections, Up: Connections   [Contents][Index] The basic functions to read from connections are scan and
readLines.  These take a character string argument and open a
file connection for the duration of the function call, but explicitly
opening a file connection allows a file to be read sequentially in
different formats.
 Other functions that call scan can also make use of connections,
in particular read.table.
 Some simple examples are
 For convenience, if the file argument specifies a FTP or HTTP
URL, the URL is opened for reading via url.
Specifying files via ‘file://foo.bar’ is also allowed.
 
Previous: Input from connections, Up: Input from connections   [Contents][Index] C programmers may be familiar with the ungetc function to push
back a character onto a text input stream.  R connections have the
same idea in a more powerful way, in that an (essentially) arbitrary
number of lines of text can be pushed back onto a connection via a call
to pushBack.
 Pushbacks operate as a stack, so a read request first uses each line
from the most recently pushbacked text, then those from earlier
pushbacks and finally reads from the connection itself.  Once a
pushbacked line is read completely, it is cleared.  The number of
pending lines pushed back can be found via a call to
pushBackLength.

 A simple example will show the idea.
 Pushback is only available for connections opened for input in text mode.
 
Next: Binary connections, Previous: Input from connections, Up: Connections   [Contents][Index] A summary of all the connections currently opened by the user can be
found by showConnections(), and a summary of all connections,
including closed and terminal connections, by showConnections(all
= TRUE)
 The generic function seek can be used to read and (on some
connections) reset the current position for reading or writing.
Unfortunately it depends on OS facilities which may be unreliable
(e.g. with text files under Windows).  Function isSeekable
reports if seek can change the position on the connection
given by its argument.
 The function truncate can be used to truncate a file opened for
writing at its current position.  It works only for file
connections, and is not implemented on all platforms.
 
Previous: Listing and manipulating connections, Up: Connections   [Contents][Index] Functions readBin and writeBin read to and write from
binary connections.  A connection is opened in binary mode by appending
"b" to the mode specification, that is using mode "rb" for
reading, and mode "wb" or "ab" (where appropriate) for
writing.  The functions have arguments
 In each case con is a connection which will be opened if
necessary for the duration of the call, and if a character string is
given it is assumed to specify a file name.
 It is slightly simpler to describe writing, so we will do that first.
object should be an atomic vector object, that is a vector of
mode numeric, integer, logical, character,
complex or raw, without attributes.  By default this is
written to the file as a stream of bytes exactly as it is represented in
memory.
 readBin reads a stream of bytes from the file and interprets them
as a vector of mode given by what.  This can be either an object
of the appropriate mode (e.g. what=integer()) or a character
string describing the mode (one of the five given in the previous
paragraph or "double" or "int").  Argument n
specifies the maximum number of vector elements to read from the
connection: if fewer are available a shorter vector will be returned.
Argument signed allows 1-byte and 2-byte integers to be
read as signed (the default) or unsigned integers.
 The remaining two arguments are used to write or read data for
interchange with another program or another platform.  By default binary
data is transferred directly from memory to the connection or vice
versa.  This will not suffice if the data are to be transferred to a
machine with a different architecture, but between almost all R
platforms the only change needed is that of byte-order.  Common PCs
(‘ix86’-based and ‘x86_64’-based machines), Compaq Alpha
and Vaxen are little-endian, whereas Sun Sparc, mc680x0 series,
IBM R6000, SGI and most others are big-endian.  (Network
byte-order (as used by XDR, eXternal Data Representation) is
big-endian.)  To transfer to or from other programs we may need to do
more, for example to read 16-bit integers or write single-precision real
numbers.  This can be done using the size argument, which
(usually) allows sizes 1, 2, 4, 8 for integers and logicals, and sizes
4, 8 and perhaps 12 or 16 for reals.  Transferring at different sizes
can lose precision, and should not be attempted for vectors containing
NA’s.
 Character strings are read and written in C format, that is as a string
of bytes terminated by a zero byte.  Functions readChar and
writeChar provide greater flexibility.
 
Previous: Binary connections, Up: Binary connections   [Contents][Index] Functions readBin and writeBin will pass missing and
special values, although this should not be attempted if a size change
is involved.
 The missing value for R logical and integer types is INT_MIN,
the smallest representable int defined in the C header
limits.h, normally corresponding to the bit pattern
0x80000000.
 The representation of the special values for R numeric and complex
types is machine-dependent, and possibly also compiler-dependent.  The
simplest way to make use of them is to link an external application
against the standalone Rmath library which exports double
constants NA_REAL, R_PosInf and R_NegInf, and
include the header Rmath.h which defines the macros ISNAN
and R_FINITE.
 If that is not possible, on all current platforms IEC 60559 (aka IEEE
754) arithmetic is used, so standard C facilities can be used to test
for or set Inf, -Inf and NaN values.  On such
platforms NA is represented by the NaN value with low-word
0x7a2 (1954 in decimal).
 Character missing values are written as NA, and there are no
provision to recognize character values as missing (as this can be done
by re-assigning them once read).
 
Next: Reading Excel spreadsheets, Previous: Connections, Up: Top   [Contents][Index] Some limited facilities are available to exchange data at a lower level
across network connections.
 
Next: Using download.file, Previous: Network interfaces, Up: Network interfaces   [Contents][Index] Base R comes with some facilities to communicate via
BSD sockets on systems that support them (including the common
Linux, Unix and Windows ports of R).  One potential problem with
using sockets is that these facilities are often blocked for security
reasons or to force the use of Web caches, so these functions may be
more useful on an intranet than externally.  For new projects it
is suggested that socket connections are used instead.
 The earlier low-level interface is given by functions make.socket,
read.socket, write.socket and close.socket.
 
Previous: Reading from sockets, Up: Network interfaces   [Contents][Index] Function download.file is provided to read a file from a
Web resource via FTP or HTTP and write it to a file.  Often this can be
avoided, as functions such as read.table and scan can read
directly from a URL, either by explicitly using url to open a
connection, or implicitly using it by giving a URL as the file
argument.
 
Next: References, Previous: Network interfaces, Up: Top   [Contents][Index] The most common R data import/export question seems to be ‘how do I read
an Excel spreadsheet’.  This chapter collects together advice and
options given earlier.  Note that most of the advice is for pre-Excel
2007 spreadsheets and not the later .xlsx format.
 The first piece of advice is to avoid doing so if possible!  If you have
access to Excel, export the data you want from Excel in tab-delimited or
comma-separated form, and use read.delim or read.csv to
import it into R.  (You may need to use read.delim2 or
read.csv2 in a locale that uses comma as the decimal point.)
Exporting a DIF file and reading it using read.DIF is another
possibility.
 If you do not have Excel, many other programs are able to read such
spreadsheets and export in a text format on both Windows and Unix, for
example Gnumeric (http://www.gnome.org/projects/gnumeric/) and
OpenOffice (https://www.openoffice.org).  You can also
cut-and-paste between the display of a spreadsheet in such a program and
R: read.table will read from the R console or, under Windows,
from the clipboard (via file = "clipboard" or
readClipboard).  The read.DIF function can also read from
the clipboard.
 Note that an Excel .xls file is not just a spreadsheet: such
files can contain many sheets, and the sheets can contain formulae,
macros and so on.  Not all readers can read other than the first sheet,
and may be confused by other contents of the file.
 Windows users (of 32-bit R) can use odbcConnectExcel in
package RODBC.  This can select rows and columns from any of the
sheets in an Excel spreadsheet file (at least from Excel 97–2003,
depending on your ODBC drivers: by calling odbcConnect directly
versions back to Excel 3.0 can be read).  The version
odbcConnectExcel2007 will read the Excel 2007 formats as well as
earlier ones (provided the drivers are installed, including with 64-bit
Windows R: see RODBC).  OS X users can also use RODBC if
they have a suitable driver (e.g. that from Actual Technologies).
 Perl users have contributed a module
OLE::SpreadSheet::ParseExcel and a program xls2csv.pl to
convert Excel 95–2003 spreadsheets to CSV files.  Package gdata
provides a basic wrapper in its read.xls function.  With suitable
Perl modules installed this function can also read Excel 2007
spreadsheets.
 32-bit Windows package xlsReadWrite from
http://www.swissr.org/ and CRAN has a function read.xls to
read .xls files (based on a third-party non-Open-Source Delphi
component).
 Packages dataframes2xls and WriteXLS each contain a function
to write one or more data frames to an .xls file, using
Python and Perl respectively.  Another version of write.xls in
available in package xlsReadWrite.
 Two packages which can read and and manipulate Excel 2007/10
spreadsheets but not earlier formats are xlsx (which requires
Java) and the Omegahat package RExcelXML.
 Package XLConnect can read, write and manipulate both Excel
97–2003 and Excel 2007/10 spreadsheets, requiring Java.
 
Next: Function and variable index, Previous: Reading Excel spreadsheets, Up: Top   [Contents][Index] R. A. Becker, J. M. Chambers and A. R. Wilks (1988)
The New S Language.  A Programming Environment for Data Analysis
and Graphics.  Wadsworth & Brooks/Cole.
 J. Bowman, S. Emberson and M. Darnovsky (1996) The
Practical SQL Handbook.  Using Structured Query Language.
Addison-Wesley.
 J. M. Chambers (1998) Programming with Data.  A Guide to the S
Language. Springer-Verlag.
 P. Dubois (2000) MySQL. New Riders.
 M. Henning and S. Vinoski (1999) Advanced CORBA Programming
with C++. Addison-Wesley.
 K. Kline and D. Kline (2001) SQL in a Nutshell. O’Reilly.
 B. Momjian (2000) PostgreSQL: Introduction and Concepts.
Addison-Wesley.
Also available at http://momjian.us/main/writings/pgsql/aw_pgsql_book/.
 B. D. Ripley (2001) Connections. \R News, 1/1, 16–7.
  \https://www.r-project.org/doc/Rnews/Rnews_2001-1.pdf
 T. M. Therneau and P. M. Grambsch (2000) Modeling Survival
Data.  Extending the Cox Model. Springer-Verlag.
 E. J. Yarger, G. Reese and T. King (1999) MySQL & mSQL.
O’Reilly.
 
Next: Concept index, Previous: References, Up: Top   [Contents][Index] 
Previous: Function and variable index, Up: Top   [Contents][Index] the
distinction is subtle,
https://en.wikipedia.org/wiki/UTF-16/UCS-2, and the use of
surrogate pairs is very rare. Even then,
Windows applications may expect a Byte Order Mark which the
implementation of iconv used by R may or may not add depending
on the platform. This is normally
fast as looking at the first entry rules out most of the possibilities. and forks, notably MariaDB. 
Next: Introduction   [Contents][Index] This is an introduction to the R language, explaining evaluation,
parsing, object oriented programming, computing on the language, and so
forth.
 This manual is for R, version 3.3.1 (2016-06-21).
 Copyright © 2000–2016 R Core Team
 Permission is granted to make and distribute verbatim copies of this
manual provided the copyright notice and this permission notice are
preserved on all copies.
 Permission is granted to copy and distribute modified versions of this
manual under the conditions for verbatim copying, provided that the
entire resulting derived work is distributed under the terms of a
permission notice identical to this one.
 Permission is granted to copy and distribute translations of this manual
into another language, under the above conditions for modified versions,
except that this permission notice may be stated in a translation
approved by the R Core Team.
 
Next: Objects, Previous: Top, Up: Top   [Contents][Index] R is a system for statistical computation and graphics. It
provides, among other things, a programming language, high level
graphics, interfaces to other languages and debugging facilities.  This
manual details and defines the R language.
 The R language is a dialect of S which was designed in the 1980s
and has been in widespread use in the statistical community since.
Its principal designer, John M. Chambers, was awarded the 1998 ACM
Software Systems Award for S.
 The language syntax has a superficial similarity with C, but the
semantics are of the FPL (functional programming language) variety with
stronger affinities with Lisp and APL.  In particular, it
allows “computing on the language”, which in turn makes it possible to
write functions that take expressions as input, something that is often
useful for statistical modeling and graphics.
 It is possible to get quite far using R interactively, executing

simple expressions from the command line.  Some users may never need to
go beyond that level, others will want to write their own functions
either in an ad hoc fashion to systematize repetitive work or with the
perspective of writing add-on packages for new functionality.
 The purpose of this manual is to document the language per se.
That is, the objects that it works on, and the details of the expression
evaluation process, which are useful to know when programming R
functions.  Major subsystems for specific tasks, such as graphics, are
only briefly described in this manual and will be documented separately.
 Although much of the text will equally apply to S, there are also
some substantial differences, and in order not to confuse the issue we
shall concentrate on describing R.
 The design of the language contains a number of fine points and
common pitfalls which may surprise the user.  Most of these are due to
consistency considerations at a deeper level, as we shall explain.
There are also a number of useful shortcuts and idioms, which allow the
user to express quite complicated operations succinctly.  Many of these
become natural once one is familiar with the underlying concepts.  In
some cases, there are multiple ways of performing a task, but some of
the techniques will rely on the language implementation, and others work
at a higher level of abstraction.  In such cases we shall indicate the
preferred usage.
 Some familiarity with R is assumed.  This is not an introduction to
R but rather a programmers’ reference manual.  Other manuals provide
complementary information: in particular Preface in An
Introduction to R provides an introduction to R and System and
foreign language interfaces in Writing R Extensions details
how to extend R using compiled code.
 
Next: Evaluation of expressions, Previous: Introduction, Up: Top   [Contents][Index] In every computer language

variables provide a means of accessing the data stored in memory.  R
does not provide direct access to the computer’s memory but rather
provides a number of specialized data structures we will refer to as

objects.  These objects
are referred to through symbols or variables.  In R, however, the
symbols are themselves objects and can be manipulated in the same way as
any other object.  This is different from many other languages and has
wide ranging effects.
 In this chapter we provide preliminary descriptions of the various data
structures provided in R.  More detailed discussions of many of them
will be found in the subsequent chapters.  The R specific function
typeof


returns the type of an R object.  Note that in the C code
underlying R, all objects are pointers to a structure with typedef
SEXPREC; the different R data types are represented in C by
SEXPTYPE, which determines how the information in the various
parts of the structure is used.
 The following table describes the possible values returned by
typeof and what they are.
 Users cannot easily get hold of objects of types marked with a ‘***’.
 Function mode gives information about the mode of an object
in the sense of Becker, Chambers & Wilks (1988), and is more compatible
with other implementations of the S language.

Finally, the function storage.mode returns the storage mode
of its argument in the sense of Becker et al. (1988).  It is generally
used when calling functions written in another language, such as C or
FORTRAN, to ensure that R objects have the data type expected by the
routine being called.  (In the S language, vectors with integer or
real values are both of mode "numeric", so their storage modes
need to be distinguished.)
 R

objects are often coerced to different

types during computations.
There are also many functions available to perform explicit

coercion.
When programming in the R language the type of an object generally
doesn’t affect the computations, however, when dealing with foreign
languages or the operating system it is often necessary to ensure that
an object is of the correct type.
 
Next: Attributes, Previous: Objects, Up: Objects   [Contents][Index] 
Next: List objects, Previous: Basic types, Up: Basic types   [Contents][Index] Vectors can be thought of as contiguous cells containing data.  Cells
are accessed through

indexing operations such as
x[5].  More details are given in Indexing.
 R has six basic (‘atomic’) vector types: logical, integer, real,
complex, string (or character) and raw.  The modes and storage modes for
the different vector types are listed in the following table.
 Single numbers, such as 4.2, and strings, such as "four
point two" are still vectors, of length 1; there are no more basic
types.  Vectors with length zero are possible (and useful).
 String vectors have mode and storage mode "character". A single
element of a character vector is often referred to as a character
string.
 
Next: Language objects, Previous: Vector objects, Up: Basic types   [Contents][Index] Lists (“generic vectors”) are another kind of data storage.  Lists
have elements, each of which can contain any type of R object, i.e.
the elements of a list do not have to be of the same type.  List
elements are accessed through three different

indexing operations.
These are explained in detail in Indexing.
 Lists are vectors, and the basic vector types are referred to as
atomic vectors where it is necessary to exclude lists.
 
Next: Expression objects, Previous: List objects, Up: Basic types   [Contents][Index] There are three types of objects that constitute the R language.
They are calls, expressions, and names.



Since R has objects of type "expression" we will try to avoid
the use of the word expression in other contexts.  In particular
syntactically correct expressions will be referred to as
statements.

 These objects have modes "call", "expression", and
"name", respectively.
 They can be created directly from expressions using the quote
mechanism and converted to and from lists by the as.list and
as.call functions.



Components of the

parse tree can be extracted using the standard
indexing operations.
 
Previous: Language objects, Up: Language objects   [Contents][Index] Symbols refer to R

objects.  The

name of any R object is usually a
symbol.  Symbols can be created through the functions as.name and
quote.
 Symbols have mode "name", storage mode "symbol", and type
"symbol".  They can be

coerced to and from character strings
using as.character and as.name.



They naturally appear as atoms of parsed expressions, try e.g.
as.list(quote(x + y)).
 
Next: Function objects, Previous: Language objects, Up: Basic types   [Contents][Index] In R one can have objects of type "expression".  An
expression contains one or more statements.  A statement is a
syntactically correct collection of

tokens.

Expression objects are special language objects which contain parsed but
unevaluated R statements.  The main difference is that an expression
object can contain several such expressions.  Another more subtle
difference is that objects of type "expression" are only

evaluated when
explicitly passed to eval, whereas other language objects may get
evaluated in some unexpected cases.
 An

expression object behaves much like a list and its components should
be accessed in the same way as the components of a list.
 
Next: NULL object, Previous: Expression objects, Up: Basic types   [Contents][Index] In R functions are objects and can be manipulated in much the same
way as any other object.  Functions (or more precisely, function
closures) have three basic components:  a formal argument list, a body
and an

environment.  The argument list is a comma-separated list of
arguments.  An

argument can be a symbol, or a ‘symbol =
default’ construct, or the special argument ‘...’.  The
second form of argument is used to specify a default value for an
argument.  This value will be used if the function is called without any
value specified for that argument.  The ‘...’  argument is special
and can contain any number of arguments.  It is generally used if the
number of arguments is unknown or in cases where the arguments will be
passed on to another function.
 The body is a parsed R statement.  It is usually a collection of
statements in braces but it can be a single statement, a symbol or even
a constant.
 A function’s


environment is the environment that was active at the time
that the function was created.  Any symbols bound in that environment
are captured and available to the function. This combination of
the code of the function and the bindings in its environment is called a
‘function closure’, a term from functional programming theory. In this
document we generally use the term ‘function’, but use ‘closure’ to
emphasize the importance of the attached environment.
 It is possible to extract and manipulate the three parts of a closure
object using formals, body, and environment
constructs (all three can also be used on the left hand side of

assignments).



The last of these can be used to remove unwanted environment capture.
 When a function is called, a new environment (called the
evaluation environment) is created, whose enclosure (see
Environment objects) is the environment from the function closure.
This new environment is initially populated with the unevaluated
arguments to the function; as evaluation proceeds, local variables are
created within it.
 There is also a facility for converting functions to and from list
structures using as.list and as.function.

These have been included to provide compatibility with S and their
use is discouraged.
 
Next: Builtin objects and special forms, Previous: Function objects, Up: Basic types   [Contents][Index] There is a special object called NULL.  It is used whenever there
is a need to indicate or specify that an object is absent.  It should not be
confused with a vector or list of zero length.

 The NULL object has no type and no modifiable properties.  There
is only one NULL object in R, to which all instances refer. To
test for NULL use is.null.  You cannot set attributes on
NULL.
 
Next: Promise objects, Previous: NULL object, Up: Basic types   [Contents][Index] These two kinds of object contain the builtin



functions of R, i.e., those that are displayed as .Primitive
in code listings (as well as those accessed via the .Internal
function and hence not user-visible as objects).  The difference between
the two lies in the argument handling.  Builtin functions have all
their arguments evaluated and passed to the internal function, in
accordance with call-by-value, whereas special functions pass the
unevaluated arguments to the internal function.
 From the R language, these objects are just another kind of function.
The is.primitive function can distinguish them from interpreted

functions.
 
Next: Dot-dot-dot, Previous: Builtin objects and special forms, Up: Basic types   [Contents][Index] Promise objects are part of R’s lazy evaluation mechanism.  They
contain three slots: a value, an expression, and an

environment.  When a


function is called the arguments are matched and then each of the formal
arguments is bound to a promise.  The expression that was given for that
formal argument and a pointer to the environment the function was called
from are stored in the promise.
 Until that argument is accessed there is no value associated with
the promise.  When the argument is accessed, the stored expression is

evaluated in the stored environment, and the result is returned.  The
result is also saved by
the promise.  The substitute function will extract the content
of the expression slot.  This allows the programmer to
access either the value or the expression associated with the promise.
 Within the R language, promise objects are almost only seen
implicitly: actual function arguments are of this type.  There is also a
delayedAssign function that will make a promise out of an
expression.  There is generally no way in R code to check whether an
object is a promise or not, nor is there a way to use R code to
determine the environment of a promise.
 
Next: Environment objects, Previous: Promise objects, Up: Basic types   [Contents][Index] The ‘...’ object type is stored as a type of pairlist.  The
components of ‘...’ can be accessed in the usual pairlist manner
from C code, but is not easily accessed as an object in interpreted
code.  The object can be captured as a list, so for example in
table one sees
 If a function has ‘...’ as a formal argument then any actual
arguments that do not match a formal argument are matched with
‘...’.
 
Next: Pairlist objects, Previous: Dot-dot-dot, Up: Basic types   [Contents][Index] Environments can be thought of as consisting of two things.  A
frame, consisting of a set of symbol-value pairs, and an
enclosure, a pointer to an enclosing environment. When R
looks up the value for a symbol the frame is examined and if a
matching symbol is found its value will be returned.  If not, the
enclosing environment is then accessed and the process repeated.
Environments form a tree structure in which the enclosures play the
role of parents.  The tree of environments is rooted in an empty

environment, available through emptyenv(), which has no parent.
It is the direct parent of the environment of the base package

(available through the baseenv() function). Formerly
baseenv() had the special value NULL, but as from
version 2.4.0, the use of NULL as an environment is defunct.
 Environments are created implicitly by function calls, as described in
Function objects and Lexical environment.  In this case the
environment contains the variables local to the function (including the
arguments), and its enclosure is the environment of the currently called
function.  Environments may also be created directly by new.env.

The frame content of an environment can be accessed and manipulated by
use of ls, get and assign as well as eval and
evalq.
 The parent.env function may be used to access the enclosure of
an environment.
 Unlike most other R objects, environments are not copied when passed
to functions or used in assignments.  Thus, if you assign the same
environment to several symbols and change one, the others will change
too.  In particular, assigning attributes to an environment can lead to
surprises.
 
Next: Any-type, Previous: Environment objects, Up: Basic types   [Contents][Index] Pairlist objects are similar to Lisp’s dotted-pair lists.  They are used
extensively in the internals of R, but are rarely visible in
interpreted code, although they are returned by formals, and can
be created by (e.g.) the pairlist function. A zero-length
pairlist is NULL, as would be expected in Lisp but in contrast to
a zero-length list.

Each such object has three slots, a CAR value, a CDR value and a TAG
value.  The TAG value is a text string and CAR and CDR usually
represent, respectively, a list item (head) and the remainder (tail) of
the list with a NULL object as terminator (the CAR/CDR terminology is
traditional Lisp and originally referred to the address and decrement
registers on an early 60’s IBM computer).
 Pairlists are handled in the R language in exactly the same way as
generic vectors (“lists”).  In particular, elements are accessed using
the same [[]] syntax.  The use of pairlists is deprecated since
generic vectors are usually more efficient to use.  When an internal
pairlist is accessed from R it is generally (including when
subsetted) converted to a generic vector.
 In a very few cases pairlists are user-visible: one is .Options.
 
Previous: Pairlist objects, Up: Basic types   [Contents][Index] It is not really possible for an object to be of “Any” type, but it is
nevertheless a valid type value.  It gets used in certain (rather rare)
circumstances, e.g. as.vector(x, "any"), indicating that type

coercion should not be done.
 
Next: Special compound objects, Previous: Basic types, Up: Objects   [Contents][Index] All objects except NULL can have one or more attributes attached
to them.  Attributes are stored as a pairlist where all elements are
named, but should be thought of as a set of name=value pairs.  A listing
of the attributes can be obtained using attributes and set by
attributes<-,


individual components are accessed using attr and attr<-.


 Some attributes have special accessor

functions (e.g. levels<-
for factors) and these should be used when available. In addition to
hiding details of implementation they may perform additional operations.
R attempts to intercept calls to attr<- and to
attributes<- that involve the special attributes and enforces
the consistency checks.
 Matrices and arrays are simply vectors with the attribute dim and
optionally dimnames attached to the vector.
 Attributes are used to implement the class structure used in R.  If an
object has a class attribute then that attribute will be examined
during

evaluation.  The class structure in R is described in detail
in Object-oriented programming.
 
Next: Dimensions, Previous: Attributes, Up: Attributes   [Contents][Index] A names attribute, when present, labels the individual elements of
a vector or list.  When an object is printed the names attribute,
when present, is used to label the elements.  The names attribute
can also be used for indexing purposes, for example,
quantile(x)["25%"].
 One may get and set the names using names and names<-
constructions.



The latter will perform the necessary consistency checks to ensure that
the names attribute has the proper type and length.
 Pairlists and one-dimensional arrays are treated specially. For pairlist
objects, a virtual names attribute is used; the names
attribute is really constructed from the tags of the list components.
For one-dimensional arrays the names attribute really accesses
dimnames[[1]].
 
Next: Dimnames, Previous: Names, Up: Attributes   [Contents][Index] The dim attribute is used to implement arrays.  The content of
the array is stored in a vector in column-major order and the dim
attribute is a vector of integers specifying the respective extents of
the array.  R ensures that the length of the vector is the product of
the lengths of the dimensions. The length of one or more dimensions may
be zero.
 A vector is not the same as a one-dimensional array since the latter has
a dim attribute of length one, whereas the former has no
dim attribute.
 
Next: Classes, Previous: Dimensions, Up: Attributes   [Contents][Index] Arrays may name each dimension separately using the dimnames
attribute which is a list of character vectors.  The dimnames
list may itself have names which are then used for extent headings when
printing arrays.
 
Next: Time series attributes, Previous: Dimnames, Up: Attributes   [Contents][Index] R has an elaborate class system1, principally controlled via
the class attribute.  This attribute is a character vector
containing the list of classes that an object inherits from.  This forms
the basis of the “generic methods” functionality in R.
 This attribute can be accessed and manipulated virtually without
restriction by users.  There is no checking that an object actually
contains the components that class methods expect.  Thus, altering the
class attribute should be done with caution, and when they are
available specific creation and

coercion functions should be preferred.
 
Next: Copying of attributes, Previous: Classes, Up: Attributes   [Contents][Index] The tsp attribute is used to hold parameters of time series,
start, end, and frequency.  This construction is mainly used to handle
series with periodic substructure such as monthly or quarterly data.
 
Previous: Time series attributes, Up: Attributes   [Contents][Index] Whether attributes should be copied when an object is altered is a
complex area, but there are some general rules (Becker, Chambers &
Wilks, 1988, pp. 144–6).
 Scalar functions (those which operate element-by-element on a vector and
whose output is similar to the input) should preserve attributes (except
perhaps class).
 Binary operations normally copy most attributes from the longer argument
(and if they are of the same length from both, preferring the values on
the first).  Here ‘most’ means all except the names, dim
and dimnames which are set appropriately by the code for the
operator.
 Subsetting (other than by an empty index) generally drops all attributes
except names, dim and dimnames which are reset as
appropriate.  On the other hand, subassignment generally preserves
attributes even if the length is changed.  Coercion drops all
attributes.
 The default method for sorting drops all attributes except names, which
are sorted along with the object.
 
Previous: Attributes, Up: Objects   [Contents][Index] 
Next: Data frame objects, Previous: Special compound objects, Up: Special compound objects   [Contents][Index] Factors are used to describe items that can have a finite number of
values (gender, social class, etc.).  A factor has a levels
attribute and class "factor".  Optionally, it may also contain a
contrasts attribute which controls the parametrisation used when
the factor is used in a


modeling functions.
 A factor may be purely nominal or may have ordered categories.  In the
latter case, it should be defined as such and have a class vector
c("ordered"," factor").
 Factors are currently implemented using an integer array to specify the
actual levels and a second array of names that are mapped to the
integers.  Rather unfortunately users often make use of the
implementation in order to make some calculations easier.  This,
however, is an implementation issue and is not guaranteed to hold in all
implementations of R.
 
Previous: Factors, Up: Special compound objects   [Contents][Index] Data frames are the R structures which most closely mimic the SAS or
SPSS data set, i.e. a “cases by variables” matrix of data.
 A data frame is a list of vectors, factors, and/or matrices all having
the same length (number of rows in the case of matrices).  In addition,
a data frame generally has a names attribute labeling the
variables and a row.names attribute for labeling the cases.
 A data frame can contain a list that is the same length as the other
components.  The list can contain elements of differing lengths thereby
providing a data structure for ragged arrays.  However, as of this
writing such arrays are not generally handled correctly.
 
Next: Functions, Previous: Objects, Up: Top   [Contents][Index] When a user types a command at the prompt (or when an expression is read
from a file) the first thing that happens to it is that the command is
transformed by the

parser into an internal representation.  The
evaluator executes parsed R expressions and returns the value of the
expression.  All expressions have a value.  This is the core of the
language.
 This chapter describes the basic mechanisms of the evaluator, but avoids
discussion of specific functions or groups of functions which are
described in separate chapters later on or where the help pages should
be sufficient documentation.
 Users can construct expressions and invoke the evaluator on them.
 
Next: Control structures, Previous: Evaluation of expressions, Up: Evaluation of expressions   [Contents][Index] 
Next: Symbol lookup, Previous: Simple evaluation, Up: Simple evaluation   [Contents][Index] Any number typed directly at the prompt is a constant and is evaluated.
 Perhaps unexpectedly, the number returned from the expression 1
is a numeric.  In most cases, the difference between an integer and a
numeric value will be unimportant as R will do the right thing when
using the numbers.  There are, however, times when we would like to
explicitly create an integer value for a constant.  We can do this by
calling the function as.integer or using various other
techniques. But perhaps the simplest approach is to qualify our 
constant with the suffix character ‘L’.
For example, to create the integer value 1, we might use
 We can use the ‘L’ suffix to qualify any number with the intent of
making it an explicit integer.  So ‘0x10L’ creates the integer value
16 from the hexadecimal representation.  The constant 1e3L gives 1000
as an integer rather than a numeric value and is equivalent to 1000L.
(Note that the ‘L’ is treated as qualifying the term 1e3 and not the
3.)  If we qualify a value with ‘L’ that is not an integer value,
e.g. 1e-3L, we get a warning and the numeric value is created.
A warning is also created if there is an unnecessary decimal point
in the number, e.g. 1.L.
 We get a syntax error when using ‘L’ with complex numbers,
e.g. 12iL gives an error.
 Constants are fairly boring and to do more we need symbols.
 
Next: Function calls, Previous: Constants, Up: Simple evaluation   [Contents][Index] When a new variable is created it must have a

name so it can be referenced and it usually has a value.  The name itself is a

symbol.
When a symbol is

evaluated its

value is returned.  Later we shall
explain in detail how to determine the value associated with a symbol.
 In this small example y is a symbol and its value is 4.  A symbol
is an R object too, but one rarely needs to deal with symbols
directly, except when doing “programming on the language”
(Computing on the language).
 
Next: Operators, Previous: Symbol lookup, Up: Simple evaluation   [Contents][Index] Most of the computations carried out in R involve the evaluation of
functions.  We will also refer to this as

function invocation.
Functions are invoked by name with a list of arguments separated by
commas.
 In this example the function mean was called with one argument,
the vector of integers from 1 to 10.
 R contains a huge number of functions with different purposes.  Most
are used for producing a result which is an R object, but others are
used for their side effects, e.g., printing and plotting functions.
 Function calls can have tagged (or named) arguments, as in
plot(x, y, pch = 3).  Arguments without tags are known as
positional since the function must distinguish their meaning from
their sequential positions among the arguments of the call, e.g., that
x denotes the abscissa variable and y the ordinate.  The
use of tags/names is an obvious convenience for functions with a large
number of optional arguments.
 A special type of function calls can appear on the left hand side of
the

assignment operator as in
 What this construction really does is to call the function
class<- with the original object and the right hand side.  This
function performs the modification of the object and returns the result
which is then stored back into the original variable.  (At least
conceptually, this is what happens.  Some additional effort is made to
avoid unnecessary data duplication.)
 
Previous: Function calls, Up: Simple evaluation   [Contents][Index] R allows the use of arithmetic expressions using operators similar to
those of the C programming language, for instance
 Expressions can be grouped using parentheses, mixed with function calls,
and assigned to variables in a straightforward manner
 R contains a number of operators.  They are listed in the table
below.
 Except for the syntax, there is no difference between applying an
operator and calling a function.  In fact, x + y can equivalently
be written `+`(x, y).  Notice that since ‘+’ is a
non-standard function name, it needs to be quoted.
 R deals with entire vectors of data at a time, and most of the
elementary operators and basic mathematical functions like log
are vectorized (as indicated in the table above).  This means that
e.g. adding two vectors of the same length will create a vector
containing the element-wise sums, implicitly looping over the vector
index.  This applies also to other operators like -, *,
and / as well as to higher dimensional structures.  Notice in
particular that multiplying two matrices does not produce the usual
matrix product (the %*% operator exists for that purpose).  Some
finer points relating to vectorized operations will be discussed in
Elementary arithmetic operations.
 To access individual elements of an atomic vector, one generally uses
the x[i] construction.
 List components are more commonly accessed using x$a or
x[[i]].
 Indexing constructs can also appear on the right hand side of an

assignment.
 Like the other operators, indexing is really done by functions, and one
could have used  `[`(x, 2) instead of x[2].
 R’s indexing operations contain many advanced features which are
further described in Indexing.
 
Next: Elementary arithmetic operations, Previous: Simple evaluation, Up: Evaluation of expressions   [Contents][Index] Computation in R consists of sequentially evaluating
statements.  Statements, such as x<-1:10 or
mean(y), can be separated by either a semi-colon or a new line.
Whenever the

evaluator is presented with a syntactically complete
statement that statement is evaluated and the value returned.
The result of evaluating a statement can be referred to as the value of
the statement2  The value can
always be assigned to a symbol.
 Both semicolons and new lines can be used to separate statements.  A
semicolon always indicates the end of a statement while a new line
may indicate the end of a statement.  If the current statement is
not syntactically complete new lines are simply ignored by the
evaluator.  If the session is interactive the prompt changes from
‘>’ to ‘+’.
 Statements can be grouped together using braces ‘{’ and ‘}’.
A group of statements is sometimes called a block.  Single
statements are evaluated when a new line is typed at the end of the
syntactically complete statement.  Blocks are not evaluated until a new
line is entered after the closing brace.  In the remainder of this
section, statement refers to either a single statement or a
block.
 
Next: Looping, Previous: Control structures, Up: Control structures   [Contents][Index] The if/else statement conditionally evaluates two
statements.  There is a condition which is evaluated and if the
value is TRUE then the first statement is evaluated;
otherwise the second statement will be evaluated.  The
if/else statement returns, as its value, the value of the
statement that was selected.  The formal syntax is
 First, statement1 is evaluated to yield value1.  If
value1 is a logical vector with first element TRUE then
statement2 is evaluated.  If the first element of value1 is
FALSE then statement3 is evaluated.  If value1 is a
numeric vector then statement3 is evaluated when the first element
of value1 is zero and otherwise statement2 is evaluated.
Only the first element of value1 is used.  All other elements are
ignored.  If value1 has any type other than a logical or a numeric
vector an error is signalled.
 if/else statements can be used to avoid numeric problems
such as taking the logarithm of a negative number.  Because
if/else statements are the same as other statements you
can assign the value of them.  The two examples below are equivalent.
 The else clause is optional.  The statement if(any(x <= 0))
x <- x[x <= 0] is valid.  When the if statement is not in a
block the else, if present, must appear on the same line as
the end of statement2.  Otherwise the new line at the end of
statement2 completes the if and yields a syntactically
complete statement that is evaluated.  A simple solution is to use a
compound statement wrapped in braces, putting the else on the
same line as the closing brace that marks the end of the statement.
 if/else statements can be nested.
 One of the even numbered statements will be evaluated and the resulting
value returned.  If the optional else clause is omitted and all
the odd numbered statements evaluate to FALSE no statement
will be evaluated and NULL is returned.
 The odd numbered statements are evaluated, in order, until one
evaluates to TRUE and then the associated even numbered
statement is evaluated.  In this example, statement6 will
only be evaluated if statement1 is FALSE and
statement3 is FALSE and statement5 is TRUE.
There is no limit to the number of else if clauses that are
permitted.
 
Next: repeat, Previous: if, Up: Control structures   [Contents][Index] R has three statements that provide explicit
looping.3  They are for, while and
repeat.  The two built-in constructs, next and
break, provide additional control over the evaluation. 
R provides other functions for
implicit looping such as tapply, apply, and lapply.
In addition many operations, especially arithmetic ones, are vectorized
so you may not need to use a loop.
 There are two statements that can be used to explicitly control looping.
They are break and next.


The break statement causes an exit from the innermost loop that
is currently being executed.  The next statement immediately
causes control to return to the start of the loop.  The next iteration
of the loop (if there is one) is then executed.  No statement below
next in the current loop is evaluated.
 The value returned by a loop statement is always NULL
and is returned invisibly.
 
Next: while, Previous: Looping, Up: Control structures   [Contents][Index] The repeat statement causes repeated evaluation of the body until
a break is specifically requested.  This means that you need to be
careful when using repeat because of the danger of an infinite
loop.  The syntax of the repeat loop is
 When using repeat, statement must be a block statement.
You need to both perform some computation and test whether or not to
break from the loop and usually this requires two statements.
 
Next: for, Previous: repeat, Up: Control structures   [Contents][Index] The while statement is very similar to the repeat
statement.  The syntax of the while loop is
 where statement1 is evaluated and if its value is TRUE then
statement2 is evaluated.  This process continues until
statement1 evaluates to FALSE.
 
Next: switch, Previous: while, Up: Control structures   [Contents][Index] The syntax of the for loop is
 where vector can be either a vector or a list.  For each element
in vector the variable name is set to the value of that
element and statement1 is evaluated.  A side effect is that the
variable name still exists after the loop has concluded and it has
the value of the last element of vector that the loop was
evaluated for.
 
Previous: for, Up: Control structures   [Contents][Index] Technically speaking, switch is just another function, but its
semantics are close to those of control structures of other programming
languages.
 The syntax is
 where the elements of list may be named.  First, statement
is evaluated and the result, value, obtained.  If value is a
number between 1 and the length of list then the corresponding
element of list is evaluated and the result returned.  If value
is too large or too small NULL is returned.
 If value is a character vector then the element of ‘...’ with
a name that exactly matches value is evaluated.  If there is no
match a single unnamed argument will be used as a default.  If no
default is specified, NULL is returned.
 A common use of switch is to branch according to the character
value of one of the arguments to a function.
 switch returns either the value of the statement that was
evaluated or NULL if no statement was evaluated.
 To choose from a list of alternatives that already exists switch
may not be the best way to select one for evaluation.  It is often
better to use eval and the subset operator, [[, directly
via eval(x[[condition]]).
 
Next: Indexing, Previous: Control structures, Up: Evaluation of expressions   [Contents][Index] In this section, we discuss the finer points of the rules that apply to
basic operation like addition or multiplication of two vectors or
matrices.
 
Next: Propagation of names, Previous: Elementary arithmetic operations, Up: Elementary arithmetic operations   [Contents][Index] If one tries to add two structures with a different number of elements,
then the shortest is recycled to length of longest.  That is, if for
instance you add c(1, 2, 3) to a six-element vector then you will
really add c(1, 2, 3, 1, 2, 3).  If the length of the longer
vector is not a multiple of the shorter one, a warning is given.
 As from R 1.4.0, any arithmetic operation involving a zero-length
vector has a zero-length result.
 
Next: Dimensional attributes, Previous: Recycling rules, Up: Elementary arithmetic operations   [Contents][Index] propagation of names (first one wins, I think - also if it has no
names?? —- first one *with names* wins, recycling causes shortest to
lose names)
 
Next: NA handling, Previous: Propagation of names, Up: Elementary arithmetic operations   [Contents][Index] (matrix+matrix, dimensions must match. vector+matrix: first recycle,
then check if dims fit, error if not)
 
Previous: Dimensional attributes, Up: Elementary arithmetic operations   [Contents][Index] Missing values in the statistical sense, that is, variables whose value
is not known, have the value NA. This should not be confused with
the missing property for a function argument that has not been
supplied (see Arguments).



 As the elements of an atomic vector must be of the same type there are
multiple types of NA values.  There is one case where this is
particularly important to the user.  The default type of NA is
logical, unless coerced to some other type, so the appearance of
a missing value may trigger logical rather than numeric indexing (see
Indexing for details).
 Numeric and logical calculations with NA generally return
NA. In cases where the result of the operation would be the same
for all possible values the NA could take, the operation may
return this value. In particular, ‘FALSE & NA’ is FALSE,
‘TRUE | NA’ is TRUE.  NA is not equal to any other
value or to itself; testing for NA is done using is.na.

However, an NA value will match another NA value in
match.
 Numeric calculations whose result is undefined, such as ‘0/0’,
produce the value NaN.  This exists only in the double
type and for real or imaginary components of the complex type.  The
function is.nan is provided to check specifically for

NaN, is.na also returns TRUE for NaN.

Coercing NaN to logical or integer type gives an NA of the
appropriate type, but coercion to character gives the string
"NaN".  NaN values are incomparable so tests of equality
or collation involving NaN will result in NA.  They are
regarded as matching any NaN value (and no other value, not even
NA) by match.
 The NA of character type is as from R 1.5.0 distinct from the
string "NA".  Programmers who need to specify an explicit string
NA should use ‘as.character(NA)’ rather than "NA", or
set elements to NA using is.na<-.
 As from R 2.5.0 there are constants NA_integer_,
NA_real_, NA_complex_ and NA_character_ which will
generate (in the parser) an NA value of the appropriate type,
and will be used in deparsing when it is not otherwise possible to
identify the type of an NA (and the control options ask
for this to be done).
 There is no NA value for raw vectors.
 
Next: Scope of variables, Previous: Elementary arithmetic operations, Up: Evaluation of expressions   [Contents][Index] R contains several constructs which allow access to individual
elements or subsets through indexing operations.  In the case of the
basic vector types one can access the i-th element using x[i],
but there is also indexing of lists, matrices, and multi-dimensional
arrays.  There are several forms of indexing in addition to indexing
with a single integer.  Indexing can be used both to extract part of an
object and to replace parts of an object (or to add parts).
 R has three basic indexing operators, with syntax displayed by the
following examples
 For vectors and matrices the [[ forms are rarely used, although
they have some slight semantic differences from the [ form (e.g.
it drops any names or dimnames attribute, and that partial
matching is used for character indices).  When indexing
multi-dimensional structures with a single index, x[[i]] or
x[i] will return the ith sequential element of x.
 For lists, one generally uses [[ to select any single element,
whereas [ returns a list of the selected elements.
 The [[ form allows only a single element to be selected using
integer or character indices, whereas [ allows indexing by
vectors.  Note though that for a list or other recursive object, the
index can be a vector and each element of the vector is applied in
turn to the list, the selected component, the selected component of
that component, and so on. The result is still a single element.
 The form using $ applies to recursive objects such as lists and
pairlists.  It allows only a literal character string or a symbol as the
index.  That is, the index is not computable: for cases where you need
to evaluate an expression to find the index, use x[[expr]].  When
$ is applied to a non-recursive object the result used to be
always NULL: as from R 2.6.0 this is an error.
 
Next: Indexing matrices and arrays, Previous: Indexing, Up: Indexing   [Contents][Index] R allows some powerful constructions using vectors as indices.  We
shall discuss indexing of simple vectors first.  For simplicity, assume
that the expression is x[i].  Then the following possibilities
exist according to the type of i.
 If i is positive and exceeds length(x) then the
corresponding selection is NA.  Negative out of bounds values
for i are silently disregarded since R version 2.6.0, S compatibly,
as they mean to drop non-existing elements and that is an empty operation
(“no-op”). 
 A special case is the zero index, which has null effects: x[0] is
an empty vector and otherwise including zeros among positive or negative
indices has the same effect as if they were omitted.
 Indexing with a missing (i.e. NA) value gives an NA
result.  This rule applies also to the case of logical indexing,
i.e. the elements of x that have an NA selector in
i get included in the result, but their value will be NA.

 Notice however, that there are different modes of NA—the
literal constant is of mode "logical", but it is frequently
automatically coerced to other types.  One effect of this is that
x[NA] has the length of x, but x[c(1, NA)] has
length 2.  That is because the rules for logical indices apply in the
former case, but those for integer indices in the latter.
 Indexing with [ will also carry out the relevant subsetting of
any names attributes.
 
Next: Indexing other structures, Previous: Indexing by vectors, Up: Indexing   [Contents][Index] Subsetting multi-dimensional structures generally follows the same rules
as single-dimensional indexing for each index variable, with the
relevant component of dimnames taking the place of names.
A couple of special rules apply, though:
 Normally, a structure is accessed using the number of indices
corresponding to its dimension.  It is however also possible to use a
single index in which case the dim and dimnames attributes
are disregarded and the result is effectively that of c(m)[i].
Notice that m[1] is usually very different from m[1, ] or
m[, 1].
 It is possible to use a matrix of integers as an index.  In this case,
the number of columns of the matrix should match the number of
dimensions of the structure, and the result will be a vector with length
as the number of rows of the matrix.  The following example shows how
to extract the elements m[1, 1] and m[2, 2] in one
operation.
 Indexing matrices may not contain negative indices.  NA and
zero values are allowed: rows in an index matrix containing a zero are
ignored, whereas rows containing an NA produce an NA in
the result.
 Both in the case of using a single

index and in matrix indexing, a names attribute is used if
present, as had the structure been one-dimensional.
 If an indexing operation causes the result to have one of its extents of
length one, as in selecting a single slice of a three-dimensional matrix
with (say) m[2, , ], the corresponding dimension is generally
dropped from the result.  If a single-dimensional structure results, a
vector is obtained.  This is occasionally undesirable and can be turned
off by adding the ‘drop = FALSE’ to the indexing operation.  Notice
that this is an additional argument to the [ function and doesn’t
add to the index count.  Hence the correct way of selecting the first
row of a matrix as a 1 by n matrix is m[1, , drop =
FALSE].  Forgetting to disable the dropping feature is a common cause
of failure in general subroutines where an index occasionally, but not
usually has length one.  This rule still applies to a one-dimensional
array, where any subsetting will give a vector result unless ‘drop
= FALSE’ is used.
 Notice that vectors are distinct from one-dimensional arrays in that the
latter have dim and dimnames attributes (both of length
one).  One-dimensional arrays are not easily obtained from subsetting
operations but they can be constructed explicitly and are returned by
table.  This is sometimes useful because the elements of the
dimnames list may themselves be named, which is not the case for
the names attribute.
 Some operations such as m[FALSE, ] result in structures in which
a dimension has zero extent.  R generally tries to handle these
structures sensibly.
 
Next: Subset assignment, Previous: Indexing matrices and arrays, Up: Indexing   [Contents][Index] The operator [ is a generic function which allows class methods
to be added, and the $ and [[ operators likewise.  Thus,
it is possible to have user-defined indexing operations for any
structure.  Such a function, say [.foo is called with a set of
arguments of which the first is the structure being indexed and the rest
are the indices.  In the case of $, the index argument is of mode
"symbol" even when using the x$"abc" form.  It is
important to be aware that class methods do not necessarily behave in
the same way as the basic methods, for example with respect to partial
matching.
 The most important example of a class method for [ is that used
for data frames.  It is not described in detail here (see the help
page for [.data.frame, but in broad terms, if two indices are
supplied (even if one is empty) it creates matrix-like indexing for a
structure that is basically a list of vectors of the same length.  If a
single index is supplied, it is interpreted as indexing the list of
columns—in that case the drop argument is ignored, with a
warning.
 The basic operators $ and [[ can be applied to
environments.  Only character indices are allowed and no partial
matching is done.
 
Previous: Indexing other structures, Up: Indexing   [Contents][Index] Assignment to subsets of a structure is a special case of a general
mechanism for complex assignment:
 The result of this command is as if the following had been executed
 Note that the index is first converted to a numeric index and then the
elements are replaced sequentially along the numeric index, as if a
for loop had been used.  Any existing variable called 
`*tmp*` will be overwritten and deleted, and this variable name 
should not be used in code.
 The same mechanism can be applied to functions other than [.  The
replacement function has the same name with <- pasted on.  Its last
argument, which must be called value, is the new value to be
assigned.  For example,
 is equivalent to
 Nesting of complex assignments is evaluated recursively
 is equivalent to
 Complex assignments in the enclosing environment (using <<-) are
also permitted:
 is equivalent to 
 and also to
 Only the target variable is evaluated in the enclosing environment, so 
 uses the local value of i on both the LHS and RHS, and the local
value of e on the RHS of the superassignment statement.  It sets
e in the outer environment to
 That is, the superassignment is equivalent to the four lines
 Similarly
 is equivalent to 
 and not to
 These two candidate interpretations differ only if there is also a
local variable x.  It is a good idea to avoid having a local
variable with the same name as the target variable of a
superassignment.  As this case was handled incorrectly in versions
1.9.1 and earlier there must not be a serious need for such code.
 
Previous: Indexing, Up: Evaluation of expressions   [Contents][Index] Almost every programming language has a set of scoping rules, allowing
the same name to be used for different objects.  This allows, e.g., a
local variable in a function to have the same name as a global object.
 R uses a lexical scoping model, similar to languages like
Pascal.  However, R is a functional programming language and
allows dynamic creation and manipulation of functions and language
objects, and has additional features reflecting this fact.
 
Next: Lexical environment, Previous: Scope of variables, Up: Scope of variables   [Contents][Index] The global

environment is the root of the user workspace.  An

assignment operation from the command line will cause the relevant
object to belong to the global environment.  Its enclosing environment
is the next environment on the search path, and so on back to the
empty environment that is the enclosure of the base environment.
 
Next: Stacks, Previous: Global environment, Up: Scope of variables   [Contents][Index] Every call to a

function creates a


frame which contains the local
variables created in the function, and is evaluated in an environment,
which in combination creates a new environment.
 Notice the terminology: A frame is a set of variables, an environment is
a nesting of frames (or equivalently: the innermost frame plus the
enclosing environment).
 Environments may be assigned to variables or be contained in other
objects.  However, notice that they are not standard objects—in
particular, they are not copied on assignment.
 A closure (mode "function") object will contain the environment
in which it is created as part of its definition (By default.  The
environment can be manipulated using environment<-).  When the
function is subsequently called, its 

evaluation environment is created with the closure’s environment as
enclosure.  Notice that this is not
necessarily the environment of the caller!
 Thus, when a variable is requested inside a

function, it is first sought
in the 

evaluation environment, then in the enclosure, the enclosure of
the enclosure, etc.; once the global environment or the environment of
a package is reached, the
search continues up the search path
to the environment of the base package.  If the variable is not
found there, the search will proceed next to the empty environment, and
will fail.
 
Next: Search path, Previous: Lexical environment, Up: Scope of variables   [Contents][Index] Every time a

function is invoked a new evaluation frame is created.  At
any point in time during the computation the currently active
environments are accessible through the call stack.  Each time a
function is invoked a special construct called a context is created
internally and is placed on a list of contexts.  When a function has
finished evaluating its context is removed from the call stack.
 Making variables defined higher up the call stack available is called

dynamic scope. The binding for a variable is then determined by the most
recent (in time) definition of the variable.  This contradicts the
default scoping rules in R, which use the bindings in the

environment
in which the function was defined (lexical scope). Some functions,
particularly those that use and manipulate model formulas, need to
simulate dynamic scope by directly accessing the call stack.
 Access to the

call stack is provided through a family of functions which
have names that start with ‘sys.’.  They are listed briefly below.
 Get the call for the specified context.
 Get the evaluation frame for the specified context.
 Get the environment frame for all active contexts.
 Get the function being invoked in the specified context.
 Get the parent of the current function invocation.
 Get the calls for all the active contexts.
 Get the evaluation frames for all the active contexts.
 Get the numeric labels for all active contexts.
 Set a function to be executed when the specified context is exited.
 Calls sys.frames, sys.parents and sys.calls.
 Get the evaluation frame for the specified parent context.
 
Previous: Stacks, Up: Scope of variables   [Contents][Index] In addition to the evaluation


environment structure, R has a search
path of environments which are searched for variables not found
elsewhere.  This is used for two things: packages of functions and
attached user data.
 The first element of the search path is the global environment and the
last is the base package.  An Autoloads environment is used for
holding proxy objects that may be loaded on demand.  Other environments
are inserted in the path using attach or library.
 Packages which have a namespace have a different search path.
When a search for an R object is started from an object in such a
package, the package itself is searched first, then its imports, then
the base namespace and finally the global environment and the rest of the
regular search path.  The effect is that references to other objects in
the same package will be resolved to the package, and objects cannot be
masked by objects of the same name in the global environment or in other
packages.
 
Next: Object-oriented programming, Previous: Evaluation of expressions, Up: Top   [Contents][Index] 
Next: Functions as objects, Previous: Functions, Up: Functions   [Contents][Index] While R can be very useful as a data analysis tool most users very
quickly find themselves wanting to write their own

functions.  This is
one of the real advantages of R.  Users can program it and they can,
if they want to, change the system level functions to functions that
they find more appropriate.
 R also provides facilities that make it easy to document any
functions that you have created.  See Writing R documentation in Writing R Extensions.
 
Next: Arguments, Previous: Writing functions, Up: Writing functions   [Contents][Index] The syntax for writing a

function is
 The first component of the function declaration is the keyword
function which indicates to R that you want to create a
function.
 An

argument list is a comma separated list of formal arguments.  A
formal argument can be a symbol, a statement of the form
‘symbol = expression’, or the special formal argument
‘...’.
 The body can be any valid R expression.  Generally, the body
is a group of expressions contained in curly braces (‘{’ and
‘}’).
 Generally

functions are assigned to symbols but they don’t need to be.
The value returned by the call to function is a function.  If
this is not given a name it is referred to as an

anonymous
function. Anonymous functions are most frequently used as arguments to
other functions such as the apply family or outer.
 Here is a simple function: echo <- function(x) print(x).  So
echo is a function that takes a single argument and when
echo is invoked it prints its argument.
 
Previous: Syntax and examples, Up: Writing functions   [Contents][Index] The formal arguments to the function define the variables whose values
will be supplied at the time the function is invoked.  The names of
these arguments can be used within the function body where they obtain
the value supplied at the time of function invocation.
 Default values for arguments can be specified using the special form
‘name = expression’.  In this case, if the user does
not specify a value for the argument when the function is invoked the
expression will be associated with the corresponding symbol.  When a
value is needed the expression is

evaluated in the evaluation
frame of the function.
 Default behaviours can also be specified by using the function
missing.  When missing is called with the

name of a formal
argument it returns TRUE if the formal argument was not matched
with any actual argument and has not been subsequently modified in the
body of the function.  An argument that is missing will thus
have its default value, if any. The missing function does not
force evaluation of the argument.
 The special type of argument ‘...’ can contain any number of
supplied arguments.  It is used for a variety of purposes.  It allows
you to write a

function that takes an arbitrary number of arguments.  It
can be used to absorb some arguments into an intermediate function which
can then be extracted by functions called subsequently.
 
Next: Evaluation, Previous: Writing functions, Up: Functions   [Contents][Index] Functions are first class objects in R.  They can be used anywhere
that an R object is required.  In particular they can be passed as
arguments to functions and returned as values from functions.  See 
Function objects for the details.
 
Previous: Functions as objects, Up: Functions   [Contents][Index] 
Next: Argument matching, Previous: Evaluation, Up: Evaluation   [Contents][Index] When a

function is called or invoked a new

evaluation frame is created.
In this frame the formal arguments are matched with the supplied
arguments according to the rules given in Argument matching.  The
statements in the body of the function are evaluated sequentially in
this

environment frame.
 The enclosing frame of the evaluation frame is the environment frame
associated with the function being invoked.  This may be different from
S.  While many functions have .GlobalEnv as their environment
this does not have to be true and functions defined in packages with
namespaces (normally) have the package namespace as their environment.
 
Next: Argument evaluation, Previous: Evaluation environment, Up: Evaluation   [Contents][Index] This subsection applies to closures but not to primitive functions.  The
latter typically ignore tags and do positional matching, but their help
pages should be consulted for exceptions, which include log,
round, signif, rep and seq.int.
 The first thing that occurs in a

function evaluation is the matching of
formal to the actual or supplied arguments.
This is done by a three-pass process:
 If any arguments remain unmatched an error is declared.
 Argument matching is augmented by the functions match.arg,
match.call and match.fun.



Access to the partial matching algorithm used by R is via
pmatch.
 
Next: Scope, Previous: Argument matching, Up: Evaluation   [Contents][Index] One of the most important things to know about the

evaluation of
arguments to a

function is that supplied arguments and default arguments
are treated differently.  The supplied arguments to a function are
evaluated in the evaluation frame of the calling function.  The default
arguments to a function are evaluated in the evaluation frame of the
function.
 The semantics of invoking a function in R argument are
call-by-value.  In general, supplied arguments behave as if they
are local variables initialized with the value supplied and the

name of
the corresponding formal argument.  Changing the value of a supplied
argument within a function will not affect the value of the variable in
the calling frame.
 R has a form of lazy evaluation of function arguments.  Arguments are
not evaluated until needed.  It is important to realize that in some
cases the argument will never be evaluated.  Thus, it is bad style to
use arguments to functions to cause side-effects.  While in C it is
common to use the form, foo(x = y) to invoke foo with the
value of y and simultaneously to assign the value of y to
x this same style should not be used in R. There is no
guarantee that the argument will ever be evaluated and hence the

assignment may not take place.
 It is also worth noting that the effect of foo(x <- y) if the
argument is evaluated is to change the value of x in the calling

environment and not in the 

evaluation environment of foo.
 It is possible to access the actual (not default) expressions used as
arguments inside the function.  The mechanism is implemented via
promises.  When a

function is being evaluated the actual expression used as an argument is
stored in the promise together with a pointer to the environment the
function was called from.  When (if) the argument is evaluated the
stored expression is evaluated in the environment that the function was
called from.  Since only a pointer to the environment is used any
changes made to that environment will be in effect during this
evaluation.  The resulting value is then also stored in a separate spot
in the promise.  Subsequent evaluations retrieve this stored value (a
second evaluation is not carried out).  Access to the unevaluated
expression is also available using substitute.
 When a

function is called, each formal argument is assigned a promise in the
local environment of the call with the expression slot containing the
actual argument (if it exists) and the environment slot containing the
environment of the caller.  If no actual argument for a formal argument
is given in the call and there is a default expression, it is similarly
assigned to the expression slot of the formal argument, but with the

environment set
to the local environment.
 The process of filling the value slot of a promise by

evaluating the
contents of the expression slot in the promise’s environment is called
forcing the promise.  A promise will only be forced once, the
value slot content being used directly later on.
 A promise is forced when its value is needed.  This usually happens
inside internal


functions, but a promise can also be forced by direct evaluation of the
promise itself.  This is occasionally useful when a default expression
depends on the value of another formal argument or other variable in the
local environment.  This is seen in the following example where the lone
label ensures that the label is based on the value of x
before it is changed in the next line.
 The expression slot of a promise can itself involve other promises.
This happens whenever an unevaluated argument is passed as an argument
to another function.  When forcing a promise, other promises in its
expression will also be forced recursively as they are evaluated.
 
Previous: Argument evaluation, Up: Evaluation   [Contents][Index] Scope or the scoping rules are simply the set of rules used by the

evaluator to find a value for a

symbol.  Every computer language has a
set of such rules.  In R the rules are fairly simple but there do
exist mechanisms for subverting the usual, or default rules.
 R adheres to a set of rules that are called lexical scope.
This means the variable

bindings in effect at the time the expression
was created are used to provide values for any unbound symbols in the
expression.
 Most of the interesting properties of

scope are involved with evaluating

functions and we concentrate on this issue.  A symbol can be either

bound or unbound.  All of the formal arguments to a function provide
bound symbols in the body of the function.  Any other symbols in the
body of the function are either local variables or unbound variables.  A
local variable is one that is defined within the function.  Because R
has no formal definition of variables, they are simply used as needed,
it can be difficult to determine whether a variable is local or not.
Local variables must first be defined, this is typically done by having
them on the left-hand side of an

assignment.
 During the evaluation process if an unbound symbol is detected then R
attempts to find a value for it.  The scoping rules determine how this
process proceeds.  In R the

environment of the function is searched
first, then its enclosure and so on until the global environment is reached.
 The global environment heads a search list of environments that are searched
sequentially for a matching symbol.  The value of the first match is then used.
 When this set of rules is combined with the fact that

functions can be
returned as values from other functions then some rather nice, but at
first glance peculiar, properties obtain.
 A simple example:
 A rather interesting question is what happens when h is
evaluated.  To describe this we need a bit more notation.  Within a

function body variables can be bound, local or unbound.  The bound
variables are those that match the formal arguments to the function.
The local variables are those that were created or defined within the
function body.  The unbound variables are those that are neither local
nor bound.  When a function body is evaluated there is no problem
determining values for local variables or for bound variables.  Scoping
rules determine how the language will find values for the unbound
variables.
 When h(3) is evaluated we see that its body is that of
g. Within that body x is bound to the formal argument
and y is unbound.  In a language with

lexical scope x will be associated with the value 3 and
y with the value 10 local to f so h(3) should return the value 13.
In R this is indeed what happens.
 In S, because of the different scoping rules one will get an error
indicating that y is not found, unless there is a variable
y in your workspace in which case its value will be used.
 
Next: Computing on the language, Previous: Functions, Up: Top   [Contents][Index] Object-oriented programming is a style of programming that has become
popular in recent years.  Much of the popularity comes from the fact
that it makes it easier to write and maintain complicated systems.  It
does this through several different mechanisms.
 Central to any object-oriented language are the concepts of class and of
methods.  A class is a definition of an object.  Typically a
class contains several slots that are used to hold class-specific
information.  An object in the language must be an instance of some
class.  Programming is based on objects or instances of classes.
 Computations are carried out via methods.  Methods are basically

functions that are specialized to carry out specific calculations on
objects, usually of a specific class.  This is what makes the language
object oriented.  In R, generic functions are used to
determine the appropriate method.  The generic function is responsible
for determining the class of its argument(s) and uses that information
to select the appropriate method.
 Another feature of most object-oriented languages is the concept of
inheritance.  In most programming problems there are usually many
objects that are related to one another.  The programming is
considerably simplified if some components can be reused.
 If a class inherits from another class then generally it gets all the
slots in the parent class and can extend it by adding new slots.  On
method dispatching (via the generic functions) if a method for the class
does not exist then a method for the parent is sought.
 In this chapter we discuss how this general strategy has been
implemented in R and discuss some of the limitations within the
current design.  One of the advantages that most object systems impart
is greater consistency.  This is achieved via the rules that are checked
by the compiler or interpreter.  Unfortunately because of the way that
the object system is incorporated into R this advantage does not
obtain.  Users are cautioned to use the object system in a
straightforward manner.  While it is possible to perform some rather
interesting feats these tend to lead to obfuscated code and may depend
on implementation details that will not be carried forward.
 The greatest use of object oriented programming in R is through
print methods, summary methods and plot methods.
These methods allow us to have one generic

function call, plot
say, that dispatches on the type of its argument and calls a plotting
function that is specific to the data supplied.
 In order to make the concepts clear we will consider the implementation
of a small system designed to teach students about probability.  In this
system the objects are probability functions and the methods we will
consider are methods for finding moments and for plotting.
Probabilities can always be represented in terms of the cumulative
distribution function but can often be represented in other ways.  For
example as a density, when it exists or as a moment generating function
when it exists.
 
Next: Inheritance, Previous: Object-oriented programming, Up: Object-oriented programming   [Contents][Index] Rather than having a full-fledged

object-oriented system R has a
class system and a mechanism for dispatching based on the class of an
object.  The dispatch mechanism for interpreted code relies on four
special objects that are stored in the evaluation frame.  These special
objects are .Generic, .Class, .Method and
.Group.  There is a separate dispatch mechanism used for internal
functions and types that will be discussed elsewhere.
 The class system is facilitated through the class attribute.
This attribute is a character vector of class names.  So to create an
object of class "foo" one simply attaches a class attribute with
the string ‘"foo"’ in it.  Thus, virtually anything can be turned
in to an object of class "foo".
 The object system makes use of

generic functions via two
dispatching functions, UseMethod and NextMethod.  The
typical use of the object system is to begin by calling a generic
function.  This is typically a very simple function and consists of a
single line of code.  The system function mean is just such a
function,
 When mean is called it can have any number of arguments but its
first argument is special and the class of that first argument is used
to determine which method should be called.  The variable .Class
is set to the class attribute of x, .Generic is set to the
string "mean" and a search is made for the correct method to
invoke.  The class attributes of any other arguments to mean are
ignored.
 Suppose that x had a class attribute that contained "foo"
and "bar", in that order.  Then R would first search for a
function called mean.foo and if it did not find one it would then
search for a function mean.bar and if that search was also
unsuccessful then a final search for mean.default would be made.
If the last search is unsuccessful R reports an error.  It is a good
idea to always write a default method.  Note that the functions
mean.foo etc. are referred to, in this context, as methods.
 NextMethod provides another mechanism for dispatching.  A

function may have a call to NextMethod anywhere in it.  The
determination of which method should then be invoked is based primarily
on the current values of .Class and .Generic.  This is
somewhat problematic since the method is really an ordinary function and
users may call it directly.  If they do so then there will be no values
for .Generic or .Class.
 If a method is invoked directly and it contains a call to
NextMethod then the first argument to NextMethod is used
to determine the

generic function.  An error is signalled if this
argument has not been supplied; it is therefore a good idea to always
supply this argument.
 In the case that a method is invoked directly the class attribute of the
first argument to the method is used as the value of .Class.
 Methods themselves employ NextMethod to provide a form of
inheritance.  Commonly a specific method performs a few operations to
set up the data and then it calls the next appropriate method through a
call to NextMethod.
 Consider the following simple example.  A point in two-dimensional
Euclidean space can be specified by its Cartesian (x-y) or polar
(r-theta) coordinates.  Hence, to store information about the location
of the point, we could define two classes, "xypoint" and
"rthetapoint".  All the ‘xypoint’ data structures are lists with
an x-component and a y-component.  All ‘rthetapoint’ objects are lists
with an r-component and a theta-component.
 Now, suppose we want to get the x-position from either type of object.
This can easily be achieved through

generic functions.  We define the
generic function xpos as follows.
 Now we can define methods:
 The user simply calls the function xpos with either
representation as the argument.  The internal dispatching method finds
the class of the object and calls the appropriate methods.
 It is pretty easy to add other representations.  One need not write a
new generic function only the methods.  This makes it easy to add to
existing systems since the user is only responsible for dealing with the
new representation and not with any of the existing representations.
 The bulk of the uses of this methodology are to provide specialized
printing for objects of different types; there are about 40 methods for
print.
 
Next: Method dispatching, Previous: Definition, Up: Object-oriented programming   [Contents][Index] The class attribute of an object can have several elements.  When a

generic function is called the first inheritance is mainly handled
through NextMethod.  NextMethod determines the method
currently being evaluated, finds the next class from th
 FIXME: something is missing here
 
Next: UseMethod, Previous: Inheritance, Up: Object-oriented programming   [Contents][Index] Generic functions should consist of a single statement.  They should
usually be of the form foo <- function(x, ...) UseMethod("foo",
x).  When UseMethod is called, it determines the appropriate 
method and then that method is invoked with the same arguments, in
the same order as the call to the generic, as if the call had been made
directly to the method.
 In order to determine the correct method the class attribute of the
first argument to the generic is obtained and used to find the correct
method.  The

name of the generic function is combined with the first element of the
class attribute into the form, generic.class and a
function with that name is sought.  If the function is found then it is
used.  If no such function is found then the second element of the class
attribute is used, and so on until all the elements of the class
attribute have been exhausted.  If no method has been found at that
point then the method generic.default is used.  If
the first argument to the generic function has no class attribute then
generic.default is used.  Since the introduction of
namespaces the methods may not be accessible by their names
(i.e. get("generic.class") may fail), but they will
be accessible by getS3method("generic","class").
 Any object can have a class attribute.  This attribute can have
any number of elements.  Each of these is a string that defines a class.
When a generic function is invoked the class of its first argument is
examined.
 
Next: NextMethod, Previous: Method dispatching, Up: Object-oriented programming   [Contents][Index] UseMethod is a special function and it behaves differently from
other function calls.  The syntax of a call to it is
UseMethod(generic, object), where generic is
the name of the generic function, object is the object used to
determine which method should be chosen.  UseMethod can only be
called from the body of a function.
 UseMethod changes the evaluation model in two ways.  First, when
it is invoked it determines the next method (function) to be called.  It
then invokes that function using the current evaluation

environment; this process will be described shortly.  The second way in
which UseMethod changes the evaluation environment is that it
does not return control to the calling function.  This means, that any
statements after a call to UseMethod are guaranteed not to be
executed.
 When UseMethod is invoked the generic function is the specified
value in the call to UseMethod.  The object to dispatch on is
either the supplied second argument or the first argument to the current
function.  The class of the argument is determined and the first element
of it is combined with the name of the generic to determine the
appropriate method.  So, if the generic had name foo and the
class of the object is "bar", then R will search for a method
named foo.bar.  If no such method exists then the inheritance
mechanism described above is used to locate an appropriate method.
 Once a method has been determined R invokes it in a special way.
Rather than creating a new evaluation

environment R uses the
environment of the current function call (the call to the generic).  Any

assignments or evaluations that were made before the call to
UseMethod will be in effect.  The arguments that were used in the
call to the generic are rematched to the formal arguments of the
selected method.
 When the method is invoked it is called with arguments that are the same
in number and have the same names as in the call to the generic.  They
are matched to the arguments of the method according to the standard
R rules for argument matching.  However the object, i.e. the first
argument has been evaluated.
 The call to UseMethod has the effect of placing some special
objects in the evaluation frame.  They are .Class,
.Generic and .Method.  These special objects are used to
by R to handle the method dispatch and inheritance.  .Class is
the class of the object, .Generic is the name of the generic
function and .Method is the name of the method currently being
invoked.  If the method was invoked through one of the internal
interfaces then there may also be an object called .Group.  This
will be described in Section Group methods.  After the initial
call to UseMethod these special variables, not the object itself,
control the selection of subsequent methods.
 The body of the method is then evaluated in the standard fashion.  In
particular variable look-up in the body follows the rules for the
method.  So if the method has an associated environment then that is
used.  In effect we have replaced the call to the generic by a call to
the method.  Any local

assignments in the frame of the generic will be
carried forward into the call to the method.  Use of this feature
is discouraged.  It is important to realize that control will never
return to the generic and hence any expressions after a call to
UseMethod will never be executed.
 Any arguments to the generic that were evaluated prior to the call to
UseMethod remain evaluated.
 If the first argument to UseMethod is not supplied it is assumed
to be the name of the current function.  If two arguments are supplied
to UseMethod then the first is the name of the method and the
second is assumed to be the object that will be dispatched on.  It is
evaluated so that the required method can be determined.  In this case
the first argument in the call to the generic is not evaluated and is
discarded.  There is no way to change the other arguments in the call to
the method; these remain as they were in the call to the generic.  This
is in contrast to NextMethod where the arguments in the call to
the next method can be altered.
 
Next: Group methods, Previous: UseMethod, Up: Object-oriented programming   [Contents][Index] NextMethod is used to provide a simple inheritance mechanism.
 Methods invoked as a result of a call to NextMethod behave as if
they had been invoked from the previous method.  The arguments to the
inherited method are in the same order and have the same names as the
call to the current method.  This means that they are the same as for
the call to the generic.  However, the expressions for the arguments are
the names of the corresponding formal arguments of the current method.
Thus the arguments will have values that correspond to their value at
the time NextMethod was invoked.
 Unevaluated arguments remain unevaluated.  Missing arguments remain
missing.
 The syntax for a call to NextMethod is NextMethod(generic,
object, ...).  If the generic is not supplied the value of
.Generic is used.  If the object is not supplied the first
argument in the call to the current method is used.  Values in the
‘...’ argument are used to modify the arguments of the next method.
 It is important to realize that the choice of the next method depends on
the current values of .Generic and .Class and not on the
object.  So changing the object in a call to NextMethod affects
the arguments received by the next method but does not affect the choice
of the next method.
 Methods can be called directly.  If they are then there will be no
.Generic, .Class or .Method.  In this case the
generic argument of NextMethod must be specified.  The
value of .Class is taken to be the class attribute of the object
which is the first argument to the current function.  The value of
.Method is the name of the current function.  These choices for
default values ensure that the behaviour of a method doesn’t change
depending on whether it is called directly or via a call to a generic.
 An issue for discussion is the behaviour of the ‘...’ argument to
NextMethod.  The White Book describes the behaviour as follows:
 - named arguments replace the corresponding arguments in the call to
  the current method.  Unnamed arguments go at the start of the argument
  list.
 What I would like to do is:
 -first do the argument matching for NextMethod;
  -if the object or generic are changed fine
  -first if a named list element matches an argument (named or not) the
  list value replaces the argument value.
  - the first unnamed list element
 Values for lookup:
 Class: comes first from .Class, second from the first argument to the
 method and last from the object specified in the call to NextMethod
 Generic: comes first from .Generic, if nothing then from the first
 argument to the method and if it’s still missing from the call to
 NextMethod
 Method: this should just be the current function name.
 
Next: Writing methods, Previous: NextMethod, Up: Object-oriented programming   [Contents][Index] For several types of

internal functions R provides a dispatching
mechanism for operators.  This means that operators such as == or
< can have their behaviour modified for members of special
classes.  The functions and operators have been grouped into three
categories and group methods can be written for each of these
categories.  There is currently no mechanism to add groups.  It is
possible to write methods specific to any function within a group.
 The following table lists the functions for the different Groups.
 abs, acos, acosh, asin, asinh, atan, atanh, ceiling, cos, cosh, cospi, cumsum,
exp, floor, gamma, lgamma, log, log10, round, signif, sin, sinh, sinpi, 
tan, tanh, tanpi, trunc
 all, any, max, min, prod, range, sum
 +, -, *, /, ^, < , >,
<=, >=, !=, ==, %%, %/%,
&, |, !
 For operators in the Ops group a special method is invoked if the two
operands taken together suggest a single method.  Specifically, if both
operands correspond to the same method or if one operand corresponds to
a method that takes precedence over that of the other operand.  If they
do not suggest a single method then the default method is used.  Either
a group method or a class method dominates if the other operand has no
corresponding method.  A class method dominates a group method.
 When the group is Ops the special variable .Method is a string
vector with two elements.  The elements of .Method are set to the
name of the method if the corresponding argument is a member of the
class that was used to determine the method.  Otherwise the
corresponding element of .Method is set to the zero length
string, "".
 
Previous: Group methods, Up: Object-oriented programming   [Contents][Index] Users can easily write their own methods and generic functions.  A

generic function is simply a function with a call to UseMethod.
A method is simply a function that has been invoked via method dispatch.
This can be as a result of a call to either UseMethod or
NextMethod.
 It is worth remembering that methods can be called directly.  That means
that they can be entered without a call to UseMethod having been
made and hence the special variables .Generic, .Class and
.Method will not have been instantiated.  In that case the
default rules detailed above will be used to determine these.
 The most common use of

generic functions is to provide print and
summary methods for statistical objects, generally the output of
some model fitting process.  To do this, each model attaches a class
attribute to its output and then provides a special method that takes
that output and provides a nice readable version of it.  The user then
needs only remember that print or summary will provide
nice output for the results of any analysis.
 
Next: System and foreign language interfaces, Previous: Object-oriented programming, Up: Top   [Contents][Index] R belongs to a class of programming languages in which subroutines
have the ability to modify or construct other subroutines and evaluate
the result as an integral part of the language itself.  This is similar
to Lisp and Scheme and other languages of the “functional programming”
variety, but in contrast to FORTRAN and the ALGOL family.  The Lisp
family takes this feature to the extreme by the “everything is a list”
paradigm in which there is no distinction between programs and data.
 R presents a friendlier interface to programming than Lisp does, at
least to someone used to mathematical formulas and C-like control
structures, but the engine is really very Lisp-like.  R allows direct
access to

parsed expressions and functions and allows you to alter and
subsequently execute them, or create entirely new functions from
scratch.
 There is a number of standard applications of this facility, such as
calculation of analytical derivatives of expressions, or the generation
of polynomial functions from a vector of coefficients.  However, there
are also uses that are much more fundamental to the workings of the
interpreted part of R.  Some of these are essential to the reuse of
functions as components in other functions, as the (admittedly not very
pretty) calls to model.frame that are constructed in several
modeling and plotting routines.  Other uses simply allow elegant
interfaces to useful functionality.  As an example, consider the
curve function, which allows you to draw the graph of a function
given as an expression like sin(x) or the facilities for plotting
mathematical expressions.
 In this chapter, we give an introduction to the set of facilities that
are available for computing on the language.
 
Next: Substitutions, Previous: Computing on the language, Up: Computing on the language   [Contents][Index] There are three kinds of language objects that are available for
modification, calls, expressions, and functions.  At this point, we
shall concentrate on the call objects.  These are sometimes referred to
as “unevaluated expressions”, although this terminology is somewhat
confusing.  The most direct method of obtaining a call object is to use
quote with an expression argument, e.g.,
 The arguments are not evaluated, the result is simply the parsed
argument.  The objects e1 and e2 may be evaluated later
using eval, or simply manipulated as data.  It is perhaps most
immediately obvious why the e2 object has mode "call",
since it involves a call to the plot function with some
arguments.  However, e1 actually has exactly the same structure
as a call to the binary operator + with two arguments, a fact
that gets clearly displayed by the following
 The components of a call object are accessed using a list-like syntax,
and may in fact be converted to and from lists using as.list and
as.call
 When keyword argument matching is used, the keywords can be used as list
tags:
 All the components of the call object have mode "name" in the
preceding examples.  This is true for identifiers in calls, but the
components of a call can also be constants—which can be of any type,
although the first component had better be a function if the call is to
be evaluated successfully—or other call objects, corresponding to
subexpressions.  Objects of mode

name can be constructed from character
strings using as.name, so one might modify the e2 object
as follows
 To illustrate the fact that subexpressions are simply components that
are themselves calls, consider
 All grouping parentheses in input are preserved in parsed expressions.
They are represented as a function call with one argument, so that
4 - (2 - 2) becomes "-"(4, "(" ("-"(2, 2))) in prefix
notation.  In evaluations, the ‘(’ operator just returns its
argument.
 This is a bit unfortunate, but it is not easy to write a

parser/deparser
combination that both preserves user input, stores it in minimal form
and ensures that parsing a deparsed expression gives the same expression
back.
 As it happens, R’s parser is not perfectly invertible, nor is its
deparser, as the following examples show
 Deparsed expressions should, however, evaluate to an equivalent value
to the original expression (up to rounding error).
 ...internal storage of flow control constructs...note Splus
incompatibility...
 
Next: More on evaluation, Previous: Direct manipulation of language objects, Up: Computing on the language   [Contents][Index] It is in fact not often that one wants to modify the innards of an
expression like in the previous section.  More frequently, one wants to
simply get at an expression in order to deparse it and use it for
labeling plots, for instance.  An example of this is seen at the
beginning of plot.default:

 This causes the variable or expression given as the x argument to
plot to be used for labeling the x-axis later on.
 The function used to achieve this is substitute which takes the
expression x and substitutes the expression that was passed
through the formal argument x.  Notice that for this to happen,
x must carry information about the expression that creates its
value.  This is related to the

lazy evaluation scheme of R
(see Promise objects).  A formal argument is really a
promise, an object with three slots, one for the expression that
defines it, one for the environment in which to evaluate that expression,
and one for the value of that expression once evaluated. substitute
will recognize a promise variable and substitute the value of its
expression slot.  If substitute is invoked inside a function, the
local variables of the function are also subject to substitution.
 The argument to substitute does not have to be a simple
identifier, it can be an expression involving several variables and
substitution will occur for each of these.  Also, substitute has
an additional argument which can be an environment or a list in which
the variables are looked up.  For example:
 Notice that quoting was necessary to substitute the x.  This kind
of construction comes in handy in connection with the facilities for
putting math expression in graphs, as the following case shows
 It is important to realize that the substitutions are purely lexical;
there is no checking that the resulting call objects make sense if they
are evaluated.  substitute(x <- x + 1, list(x = 2)) will happily
return 2 <- 2 + 1.  However, some parts of R make up their own
rules for what makes sense and what does not and might actually have a
use for such ill-formed expressions.  For example, using the “math in
graphs” feature often involves constructions that are syntactically
correct, but which would be meaningless to evaluate, like
‘{}>=40*" years"’.
 Substitute will not evaluate its first argument.  This leads to the
puzzle of how to do substitutions on an object that is contained in a
variable.  The solution is to use substitute once more, like this
 The exact rules for substitutions are as follows:  Each

symbol in the

parse tree for the first is matched against the second argument, which
can be a tagged list or an environment frame.  If it is a simple local
object, its value is inserted, except if matching against the
global environment.  If it is a promise (usually a function argument),
the promise expression is substituted.  If the symbol is not matched, it
is left untouched.  The special exception for substituting at the top
level is admittedly peculiar.  It has been inherited from S and the
rationale is most likely that there is no control over which variables
might be bound at that level so that it would be better to just make
substitute act as quote.
 The rule of promise substitution is slightly different from that of
S if the local variable is modified before substitute is
used.  R will then use the new value of the variable, whereas S
will unconditionally use the argument expression—unless it was a
constant, which has the curious consequence that f((1)) may be
very different from f(1) in S.  The R rule is considerably
cleaner, although it does have consequences in connection with

lazy
evaluation that comes as a surprise to some.  Consider
 This looks straightforward, but one will discover that the y label
becomes an ugly c(...) expression.  It happens because the rules
of lazy evaluation cause the evaluation of the ylab expression
to happen after y has been modified.  The solution is to
force ylab to be evaluated first, i.e.,
 Notice that one should not use eval(ylab) in this situation.  If
ylab is a language or expression object, then that would cause
the object to be evaluated as well, which would not at all be desirable
if a math expression like quote(log[e](y)) was being passed.
 A variant on substitute is bquote, which is used to replace some subexpressions with their values.  The example from above
 could be written more compactly as
 The expression is quoted except for the contents of .()
subexpressions, which are replaced with their values. There is an
optional argument to compute the values in a different
environment. The syntax for bquote is borrowed from the LISP
backquote macro.
 
Next: Evaluation of expression objects, Previous: Substitutions, Up: Computing on the language   [Contents][Index] The eval function was introduced earlier in this chapter as a
means of evaluating call objects.  However, this is not the full story.
It is also possible to specify the

environment in which the evaluation
is to take place. By default this is the evaluation frame from which
eval is called, but quite frequently it needs to be set to
something else.

 Very often, the relevant evaluation frame is that of the parent of the
current frame (cf. ???).  In particular, when the object to evaluate
is the result of a substitute operation of the function
arguments, it will contain variables that make sense to the caller only
(notice that there is no reason to expect that the variables of the
caller are in the

lexical scope of the callee).  Since evaluation in the
parent frame occurs frequently, an eval.parent function exists as
a shorthand for eval(expr, sys.frame(sys.parent())).
 Another case that occurs frequently is evaluation in a list or a data
frame.  For instance, this happens in connection with the
model.frame function when a data argument is given.
Generally, the terms of the model formula need to be evaluated in
data, but they may occasionally also contain references to items
in the caller of model.frame.  This is sometimes useful in
connection with simulation studies.  So for this purpose one needs not
only to evaluate an expression in a list, but also to specify an
enclosure into which the search continues if the variable is not in the
list.  Hence, the call has the form
 Notice that evaluation in a given environment may actually change that
environment, most obviously in cases involving the

assignment operator,
such as
 This is also true when evaluating in lists, but the original list does
not change because one is really working on a copy.
 
Next: Manipulation of function calls, Previous: More on evaluation, Up: Computing on the language   [Contents][Index] Objects of mode "expression" are defined in Expression objects.  They are very similar to lists of call objects.
 Notice that evaluating an expression object evaluates each call in turn,
but the final value is that of the last call.  In this respect it
behaves almost identically to the compound language object
quote({2 + 2; 3 + 4}).  However, there is a subtle difference:
Call objects are indistinguishable from subexpressions in a parse tree.
This means that they are automatically evaluated in the same way a
subexpression would be.  Expression objects can be recognized during
evaluation and in a sense retain their quotedness.  The evaluator will
not evaluate an expression object recursively, only when it is passed
directly to eval function as above.  The difference can be seen
like this:
 The deparser represents an expression object by the call
that creates it.  This is similar to the way it handles numerical
vectors and several other objects that do not have a specific external
representation.  However, it does lead to the following bit of
confusion:
 I.e., e and ee look identical when printed, but one is a
call that generates an expression object and the other is the object
itself.
 
Next: Manipulation of functions, Previous: Evaluation of expression objects, Up: Computing on the language   [Contents][Index] It is possible for a

function to find out how it has been called by
looking at the result of sys.call as in the following example of
a function that simply returns its own call:
 However, this is not really useful except for debugging because it
requires the function to keep track of argument matching in order to
interpret the call.  For instance, it must be able to see that the 2nd
actual argument gets matched to the first formal one (x in the
above example).
 More often one requires the call with all actual arguments bound to the
corresponding formals.  To this end, the function match.call is
used.  Here’s a variant of the preceding example, a function that
returns its own call with arguments matched
 Notice that the second argument now gets matched to x and appears
in the corresponding position in the result.
 The primary use of this technique is to call another function with the
same arguments, possibly deleting some and adding others.  A typical
application is seen at the start of the lm function:
 Notice that the resulting call is

evaluated in the parent frame, in
which one can be certain that the involved expressions make sense.  The
call can be treated as a list object where the first element is the name
of the function and the remaining elements are the actual argument
expressions, with the corresponding formal argument names as tags.
Thus, the technique to eliminate undesired arguments is to assign
NULL, as seen in lines 2 and 3, and to add an argument one uses
tagged list

assignment (here to pass drop.unused.levels = TRUE)
as in line 4.  To change the name of the function called, assign to the
first element of the list and make sure that the value is a name, either
using the as.name("model.frame") construction here or
quote(model.frame).
 The match.call function has an expand.dots argument which
is a switch which if set to FALSE lets all ‘...’ arguments
be collected as a single argument with the tag ‘...’.

 The ‘...’ argument is a list (a pairlist to be precise), not a call
to list like it is in S:
 One reason for using this form of match.call is simply to get rid
of any ‘...’ arguments in order not to be passing unspecified
arguments on to functions that may not know them.  Here’s an example
paraphrased from plot.formula:
 A more elaborate application is in update.default where a set of
optional extra arguments can add to, replace, or cancel those of the
original call:
 Notice that care is taken to modify existing arguments individually in
case extras[[a]] == NULL.  Concatenation does not work on call
objects without the coercion as shown; this is arguably a bug.
 Two further functions exist for the construction of function calls,
namely call and do.call.
 The function call allows creation of a call object from the
function name and the list of arguments
 As seen, the value of x rather than the

symbol is inserted in the
call, so it is distinctly different from round(x).  The form is
used rather rarely, but is occasionally useful where the name of a
function is available as a character variable.
 The function do.call is related, but evaluates the call immediately
and takes the arguments from an object of mode "list" containing
all the arguments.  A natural use of this is when one wants to apply a
function like cbind to all elements of a list or data frame.

 Other uses include variations over constructions like do.call("f",
list(...)).  However, one should be aware that this involves evaluation
of the arguments before the actual function call, which may defeat
aspects of lazy evaluation and argument substitution in the function
itself.  A similar remark applies to the call function.
 
Previous: Manipulation of function calls, Up: Computing on the language   [Contents][Index] It is often useful to be able to manipulate the components of a

function
or closure.  R provides a set of interface functions for this
purpose.
 Returns the expression that is the body of the function.
 Returns a list of the formal arguments to the function.  This is a
pairlist.
 Returns the environment associated with the function.
 This sets the body of the function to the supplied expression.
 Sets the formal arguments of the function to the supplied list.
 Sets the environment of the function to the specified environment.
 It is also possible to alter the bindings of different variables in the
environment of the function, using code along the lines of evalq(x
<- 5, environment(f)).
 It is also possible to convert a

function to a list using
as.list.  The result is the concatenation of the list of formal
arguments with the function body.  Conversely such a list can be
converted to a function using as.function.  This functionality is
mainly included for S compatibility.  Notice that environment
information is lost when as.list is used, whereas
as.function has an argument that allows the environment to be
set.
 
Next: Exception handling, Previous: Computing on the language, Up: Top   [Contents][Index] 
Next: Foreign language interfaces, Previous: System and foreign language interfaces, Up: System and foreign language interfaces   [Contents][Index] Access to the operating system shell is via the R function
system.

The details will differ by platform (see the on-line help), and about
all that can safely be assumed is that the first argument will be a
string command that will be passed for execution (not necessarily
by a shell) and the second argument will be internal which if
true will collect the output of the command into an R character
vector.
 The functions system.time

and proc.time

are available for timing (although the information available may be
limited on non-Unix-like platforms).
 Information from the operating system

environment can be accessed and manipulated with
 A uniform set of file access functions is provided on all platforms:
 There are also functions for manipulating file names and paths in a
platform-independent way.
 
Next: .Internal and .Primitive, Previous: Operating system access, Up: System and foreign language interfaces   [Contents][Index] See System and foreign language interfaces in Writing R
Extensions for the details of adding functionality to R via compiled
code.
 Functions .C and .Fortran provide a standard interface to
compiled code that has been linked into R, either at build time or
via dyn.load.  They are primarily intended for compiled C and
FORTRAN code respectively, but the .C function can be used with
other languages which can generate C interfaces, for example C++.
 Functions .Call and .External provide interfaces which allow
compiled code (primarily compiled C code) to manipulate R objects.
 
Previous: Foreign language interfaces, Up: System and foreign language interfaces   [Contents][Index] The .Internal and .Primitive interfaces are used to call
C code compiled into R at build time.
See .Internal vs .Primitive in R Internals.
 
Next: Debugging, Previous: System and foreign language interfaces, Up: Top   [Contents][Index] The exception handling facilities in R are provided through two
mechanisms.  Functions such as stop or warning can be
called directly or options such as "warn" can be used to control
the handling of problems.
 
Next: warning, Previous: Exception handling, Up: Exception handling   [Contents][Index] A call to stop halts the evaluation of the current expression,
prints the message argument and returns execution to top-level.
 
Next: on.exit, Previous: stop, Up: Exception handling   [Contents][Index] The function warning takes a single argument that is a character
string.  The behaviour of a call to warning depends on the value
of the option "warn".  If "warn" is negative warnings are
ignored.  If it is zero, they are stored and printed after the top-level
function has completed.  If it is one, they are printed as they occur
and if it is 2 (or larger) warnings are turned into errors.
 If "warn" is zero (the default), a variable last.warning
is created and the messages associated with each call to warning
are stored, sequentially, in this vector.  If there are fewer than 10
warnings they are printed after the function has finished evaluating.
If there are more than 10 then a message indicating how many warnings
occurred is printed.  In either case last.warning contains the
vector of messages, and warnings provides a way to access and
print it.
 
Next: Error options, Previous: warning, Up: Exception handling   [Contents][Index] A function can insert a call to on.exit at any point in the body
of a function.  The effect of a call to on.exit is to store the
value of the body so that it will be executed when the function exits.
This allows the function to change some system parameters and to ensure
that they are reset to appropriate values when the function is finished.
The on.exit is guaranteed to be executed when the function exits
either directly or as the result of a warning.
 An error in the evaluation of the on.exit code causes an
immediate jump to top-level without further processing of the
on.exit code.
 on.exit takes a single argument which is an expression to be
evaluated when the function is exited.
 
Previous: on.exit, Up: Exception handling   [Contents][Index] There are a number of options variables that can be used to
control how R handles errors and warnings.  They are listed in the
table below.
 Controls the printing of warnings.
 Sets an expression that is to be evaluated when a warning occurs.  The
normal printing of warnings is suppressed if this option is set.
 Installs an expression that will be evaluated when an error occurs.
The normal printing of error messages and warning messages precedes the
evaluation of the expression.
 Expressions installed by options("error") are evaluated before
calls to on.exit are carried out.
 One can use options(error = expression(q("yes"))) to get R to
quit when an error has been signalled.  In this case an error will cause
R to shut down and the global environment will be saved.
 
Next: Parser, Previous: Exception handling, Up: Top   [Contents][Index] Debugging code has always been a bit of an art.  R provides several
tools that help users find problems in their code.  These tools halt
execution at particular points in the code and the current state of the
computation can be inspected.
 Most debugging takes place either through calls to browser or
debug.  Both of these functions rely on the same internal
mechanism and both provide the user with a special prompt.  Any command
can be typed at the prompt.  The evaluation

environment for the command
is the currently active environment.  This allows you to examine the
current state of any variables etc.
 There are five special commands that R interprets differently.  They
are,
 Go to the next statement if the function is being debugged.  Continue
execution if the browser was invoked.
 Continue the execution.
 Execute the next statement in the function.  This works from the browser
as well.
 Show the call stack
 Halt execution and jump to the top-level immediately.
 If there is a local variable with the same name as one of the special
commands listed above then its value can be accessed by using
get.  A call to get with the name in quotes will retrieve
the value in the current

environment.
 The debugger provides access only to interpreted expressions.  If a
function calls a foreign language (such as C) then no access to the
statements in that language is provided.  Execution will halt on the
next statement that is evaluated in R. A symbolic debugger such as
gdb can be used to debug compiled code.
 
Next: debug/undebug, Previous: Debugging, Up: Debugging   [Contents][Index] A call to the function browser causes R to halt execution at
that point and to provide the user with a special prompt.  Arguments to
browser are ignored.
 
Next: trace/untrace, Previous: browser, Up: Debugging   [Contents][Index] The debugger can be invoked on any function by using the command
debug(fun).  Subsequently, each time that function is
evaluated the debugger is invoked.  The debugger allows you to control
the evaluation of the statements in the body of the function.  Before
each statement is executed the statement is printed out and a special
prompt provided.  Any command can be given, those in the table above
have special meaning.
 Debugging is turned off by a call to undebug with the function as
an argument.
 
Next: traceback, Previous: debug/undebug, Up: Debugging   [Contents][Index] Another way of monitoring the behaviour of R is through the
trace mechanism.  trace is called with a single argument
that is the name of the function you want to trace.  The name does not
need to be quoted but for some functions you will need to quote the name
in order to avoid a syntax error.
 When trace has been invoked on a function then every time that
function is evaluated the call to it is printed out.  This mechanism is
removed by calling untrace with the function as an argument.
 
Previous: trace/untrace, Up: Debugging   [Contents][Index] When an error has caused a jump to top-level a special variable called
.Traceback is placed into the base environment.
.Traceback is a character vector with one entry for each function
call that was active at the time the error occurred.  An examination of
.Traceback can be carried out by a call to traceback.
 
Next: Function and Variable Index, Previous: Debugging, Up: Top   [Contents][Index] The parser is what converts the textual representation of R code into
an internal form which may then be passed to the R evaluator which
causes the specified instructions to be carried out.  The internal form
is itself an R object and can be saved and otherwise manipulated
within the R system.
 
Next: Comments, Previous: Parser, Up: Parser   [Contents][Index] 
Next: Internal representation, Previous: The parsing process, Up: The parsing process   [Contents][Index] Parsing in R occurs in three different variants:
 The read-eval-print loop forms the basic command line interface to R.
Textual input is read until a complete R expression is available.
Expressions may be split over several input lines.  The primary prompt
(by default ‘> ’) indicates that the parser is ready for a new
expression, and a continuation prompt (by default ‘+ ’) indicates
that the parser expects the remainder of an incomplete expression.  The
expression is converted to internal form during input and the parsed
expression is passed to the evaluator and the result is printed (unless
specifically made invisible).  If the parser finds itself in a state
which is incompatible with the language syntax, a “Syntax Error” is
flagged and the parser resets itself and resumes input at the beginning
of the next input line.
 Text files can be parsed using the parse function.  In
particular, this is done during execution of the source
function, which allows commands to be stored in an external file and
executed as if they had been typed at the keyboard.  Note, though, that
the entire file is parsed and syntax checked before any evaluation takes
place.
 Character strings, or vectors thereof, can be parsed using the
text= argument to parse.  The strings are treated exactly
as if they were the lines of an input file.
 
Next: Deparsing, Previous: Modes of parsing, Up: The parsing process   [Contents][Index] Parsed expressions are stored in an R object containing the parse
tree.  A fuller description of such objects can be found in
Language objects and Expression objects.  Briefly, every
elementary R expression is stored in

function call form, as a list
with the first element containing the function name and the remainder
containing the arguments, which may in turn be further R expressions.
The list elements can be named, corresponding to tagged matching of
formal and actual arguments.  Note that all R syntax elements
are treated in this way, e.g. the assignment x <- 1 is encoded
as "<-"(x, 1).
 
Previous: Internal representation, Up: The parsing process   [Contents][Index] Any R object can be converted to an R expression using
deparse.  This is frequently used in connection with output of
results, e.g. for labeling plots.  Notice that only objects of mode
"expression" can be expected to be unchanged by reparsing the
output of deparsing.  For instance, the numeric vector 1:5 will
deparse as "c(1, 2, 3, 4, 5)", which will reparse as a call to
the function c.  As far as possible, evaluating the deparsed and
reparsed expression gives the same result as evaluating the original,
but there are a couple of awkward exceptions, mostly involving
expressions that weren’t generated from a textual representation in the
first place.
 
Next: Tokens, Previous: The parsing process, Up: Parser   [Contents][Index] Comments in R are ignored by the parser.  Any text from a

# character
to the end of the line is taken to be a comment, unless
the # character is inside a quoted string. For example,
 
Next: Expressions, Previous: Comments, Up: Parser   [Contents][Index] Tokens are the elementary building blocks of a programming language.
They are recognised during lexical analysis which (conceptually,
at least) takes place prior to the syntactic analysis performed by the
parser itself.
 
Next: Identifiers, Previous: Tokens, Up: Tokens   [Contents][Index] There are five types of constants: integer, logical, numeric, complex and string.
 In addition, there are four special constants, NULL, NA,
Inf, and NaN.
 NULL is used to indicate the empty object.  NA is used for
absent (“Not Available”) data values.  Inf denotes infinity and
NaN is not-a-number in the IEEE floating point calculus
(results of the operations respectively 1/0 and 0/0, for
instance).
 Logical constants are either TRUE or FALSE.
 Numeric constants follow a similar syntax to that of the C language.
They consist of an integer part consisting of zero or more digits,
followed optionally by ‘.’ and a fractional part of zero or more
digits optionally followed by an exponent part consisting of an ‘E’
or an ‘e’, an optional sign and a string of one or more digits.
Either the fractional or the decimal part can be empty, but not both at
once.
 Numeric constants can also be hexadecimal, starting with ‘0x’ or
‘0x’ followed by zero or more digits, ‘a-f’ or ‘A-F’.  
Hexadecimal floating point constants are supported using C99 syntax, e.g.
‘0x1.1p1’.
 There is now a separate class of integer constants.  They are created
by using the qualifier L at the end of the number.  For
example, 123L gives an integer value rather than a numeric
value.  The suffix L can be used to qualify any non-complex
number with the intent of creating an integer.  So it can be used with
numbers given by hexadecimal or scientific notation. However, if the
value is not a valid integer, a warning is emitted and the numeric
value created.  The following shows examples of valid integer
constants, values which will generate a warning and give numeric
constants and syntax errors.
 A warning is emitted for decimal values that contain an unnecessary
decimal point, e.g. 1.L.  It is an error to have a decimal
point in a hexadecimal constant without the binary exponent.
 Note also that a preceding sign (+ or -) is treated as a
unary operator, not as part of the constant.
 Up-to-date information on the currently accepted formats can be found by
?NumericConstants.
 Complex constants have the form of a decimal numeric constant followed
by ‘i’.  Notice that only purely imaginary numbers are actual
constants, other complex numbers are parsed a unary or binary operations
on numeric and imaginary numbers.
 String constants are delimited by a pair of single (‘'’) or double
(‘"’) quotes and can contain all other printable characters.
Quotes and other special characters within strings are specified using
escape sequences:
 single quote
 double quote
 newline
 carriage return
 tab character
 backspace
 bell
 form feed
 vertical tab
 backslash itself
 character with given octal code – sequences of one, two or three digits
in the range 0 ... 7 are accepted.
 character with given hex code – sequences of one or two hex digits
(with entries 0 ... 9 A ... F a ... f).
 (where multibyte locales are supported, otherwise an error).
Unicode character with given hex code – sequences of up to four hex
digits. The character needs to be valid in the current locale.
 (where multibyte locales are supported and not on Windows, otherwise an
error).  Unicode character with given hex code – sequences of up to
eight hex digits.
 A single quote may also be embedded directly in a double-quote delimited
string and vice versa.
 As from R 2.8.0, a ‘nul’ (\0) is not allowed in a character
string, so using \0 in a string constant terminates the constant
(usually with a warning): further characters up to the closing quote are
scanned but ignored.
 
Next: Reserved words, Previous: Literal constants, Up: Tokens   [Contents][Index] Identifiers consist of a sequence of letters, digits, the period
(‘.’) and the underscore.  They must not start with a digit or
an underscore, or with a period followed by a digit.
 The definition of a letter depends on the current locale: the precise
set of characters allowed is given by the C expression (isalnum(c)
|| c == ‘.’ || c == ‘_’) and will include accented letters in many
Western European locales.
 Notice that identifiers starting with a period are not by default listed
by the ls function and that ‘...’ and ‘..1’,
‘..2’, etc. are special.
 Notice also that objects can have names that are not identifiers.  These
are generally accessed via get and assign, although they
can also be represented by text strings in some limited circumstances
when there is no ambiguity (e.g. "x" <- 1). As get and
assign are not restricted to names that are identifiers they do
not recognise subscripting operators or replacement functions. The
following pairs are not equivalent


 
Next: Special operators, Previous: Identifiers, Up: Tokens   [Contents][Index] The following identifiers have a special meaning and cannot be used
for object names
 
Next: Separators, Previous: Reserved words, Up: Tokens   [Contents][Index] R allows user-defined infix operators.  These have the form of a
string of characters delimited by the ‘%’ character.  The string
can contain any printable character except ‘%’.  The escape sequences
for strings do not apply here.
 Note that the following operators are predefined
 
Next: Operator tokens, Previous: Special operators, Up: Tokens   [Contents][Index] Although not strictly tokens, stretches of whitespace characters
(spaces, tabs and formfeeds, on Windows and UTF-8 locales other Unicode
whitespace characters4) serve to delimit tokens in case of
ambiguity, (compare x<-5 and x < -5).
 Newlines have a function which is a combination of token separator and
expression terminator.  If an expression can terminate at the end of
the line the parser will assume it does so, otherwise the newline is
treated as whitespace.  Semicolons (‘;’) may be used to separate
elementary 

expressions on the same line.
 Special rules apply to the else keyword: inside a compound
expression, a newline before else is discarded, whereas at the
outermost level, the newline terminates the if construction and a
subsequent else causes a syntax error.  This somewhat anomalous
behaviour occurs because R should be usable in interactive mode and
then it must decide whether the input expression is complete,
incomplete, or invalid as soon as the user presses RET.
 The comma (‘,’) is used to separate function arguments and multiple
indices.
 
Next: Grouping, Previous: Separators, Up: Tokens   [Contents][Index] R uses the following operator tokens
 (Several of the operators have different meaning inside model formulas)
 
Next: Indexing tokens, Previous: Operator tokens, Up: Tokens   [Contents][Index] Ordinary parentheses—‘(’ and ‘)’—are used for explicit
grouping within expressions and to delimit the argument lists for
function definitions and function calls.
 Braces—‘{’ and ‘}’—delimit blocks of expressions in
function definitions, conditional expressions, and iterative constructs.
 
Previous: Grouping, Up: Tokens   [Contents][Index] Indexing of arrays and vectors is performed using the single and double
brackets, ‘[]’ and ‘[[]]’.  Also, indexing tagged lists
may be done using the ‘$’ operator.
 
Next: Directives, Previous: Tokens, Up: Parser   [Contents][Index] An R program consists of a sequence of R expressions.  An
expression can be a simple expression consisting of only a constant or
an identifier, or it can be a compound expression constructed from other
parts (which may themselves be expressions).
 The following sections detail the various syntactical constructs that
are available.
 
Next: Infix and prefix operators, Previous: Expressions, Up: Expressions   [Contents][Index] A function call takes the form of a function reference followed by a
comma-separated list of arguments within a set of parentheses.
 The function reference can be either
 Each argument can be tagged (tag=expr), or just be a
simple expression.  It can also be empty or it can be one of the special
tokens ‘...’, ‘..2’, etc.
 A tag can be an identifier or a text string.
 Examples:
 
Next: Index constructions, Previous: Function calls (expressions), Up: Expressions   [Contents][Index] The order of precedence (highest first) of the operators is
 Note that : precedes binary +/-, but not ^. Hence,
1:3-1 is 0 1 2, but 1:2^3 is 1:8. 
 The exponentiation operator ‘^’ and the

left assignment plus minus operators
‘<- - = <<-’ group right to left, all other operators group left to
right.  That is, 2 ^ 2 ^ 3 is 2 ^ 8, not 4 ^ 3,
whereas 1 - 1 - 1 is -1, not 1.
 Notice that the operators %% and %/% for integer
remainder and divide have higher precedence than multiply and divide.
 Although it is not strictly an operator, it also needs mentioning that
the ‘=’ sign is used for tagging arguments in
function calls and
for assigning default values in function definitions.
 The ‘$’ sign is in some sense an operator, but does not allow
arbitrary right hand sides and is discussed under Index constructions.  It has higher precedence than any of the other
operators.
 The parsed form of a unary or binary operation is completely equivalent
to a function call with the operator as the function name and the
operands as the function arguments.
 Parentheses are recorded as equivalent to a unary operator, with name
"(", even in cases where the parentheses could be inferred from
operator precedence (e.g., a * (b + c)).
 Notice that the

assignment symbols are operators just like the arithmetic, relational,
and logical ones.  Any expression is allowed also on the target side of
an assignment, as far as the parser is concerned (2 + 2 <- 5 is a
valid expression as far as the parser is concerned.  The evaluator will
object, though).  Similar comments apply to the model formula operator.
 
Next: Compound expressions, Previous: Infix and prefix operators, Up: Expressions   [Contents][Index] R has three indexing constructs, two of which are syntactically
similar although with somewhat different semantics:
 The object can formally be any valid expression, but it is
understood to denote or evaluate to a subsettable object.  The arguments
generally evaluate to numerical or character indices, but other kinds of
arguments are possible (notably drop = FALSE).
 Internally, these index constructs are stored as function calls with
function name "[" respectively "[[".
 The third index construction is
 Here, object is as above, whereas tag is an identifier or a
text string.  Internally, it is stored as a function call with name
"$"
 
Next: Flow control elements, Previous: Index constructions, Up: Expressions   [Contents][Index] A compound expression is of the form
 The semicolons may be replaced by newlines.  Internally, this is stored
as a function call with "{" as the function name and the
expressions as arguments.
 
Next: Function definitions, Previous: Compound expressions, Up: Expressions   [Contents][Index] R contains the following control structures as special syntactic
constructs
 The expressions in these constructs will typically be compound
expressions.
 Within the loop constructs (while, repeat, for),
one may use break (to terminate the loop) and next (to
skip to the next iteration).
 Internally, the constructs are stored as function calls:
 
Previous: Flow control elements, Up: Expressions   [Contents][Index] A

function definition is of the form
 The function body is an expression, often a compound expression.  The
arglist is a comma-separated list of items each of which can be an
identifier, or of the form ‘identifier = default’, or
the special token ‘...’.  The default can be any valid
expression.
 Notice that function arguments unlike list tags, etc., cannot have
“strange names” given as text strings.
 Internally, a function definition is stored as a function call with
function name function and two arguments, the arglist and
the body.  The arglist is stored as a tagged pairlist where
the tags are the argument names and the values are the default
expressions.
 
Previous: Expressions, Up: Parser   [Contents][Index] The parser currently only supports one directive, #line.  
This is similar to the C-preprocessor directive of the same name.  The
syntax is
 where nn is an integer line number, and the optional filename 
(in required double quotes) names the source file.
 Unlike the C directive, #line must appear as the first five characters
on a line.  As in C, nn and "filename" entries may be separated
from it by whitespace.  And unlike C, any following text on the line will be
treated as a comment and ignored.
 This directive tells the parser that the following line should be assumed to 
be line nn of file filename.  (If the filename is not given,
it is assumed to be the same as for the previous directive.)  This is not
typically used by users, but may be used by preprocessors so that
diagnostic messages refer to the original file.
 
Next: Concept Index, Previous: Parser, Up: Top   [Contents][Index] 
Next: References, Previous: Function and Variable Index, Up: Top   [Contents][Index] 
Previous: Concept Index, Up: Top   [Contents][Index] Richard A. Becker, John M. Chambers and Allan R. Wilks (1988),
The New S Language. Chapman & Hall, New York.
This book is often called the “Blue Book”.
 actually two, but this draft
manual predates the methods package. Evaluation always takes place in an

environment.
See Scope of variables for more details. Looping is the repeated evaluation of a statement or
block of statements. such as U+A0, non-breaking space,
and U+3000, ideographic space. 
Next: Obtaining R   [Contents][Index] This is a guide to installation and administration for R.
 This manual is for R, version 3.3.1 (2016-06-21).
 Copyright © 2001–2016 R Core Team
 Permission is granted to make and distribute verbatim copies of this
manual provided the copyright notice and this permission notice are
preserved on all copies.
 Permission is granted to copy and distribute modified versions of this
manual under the conditions for verbatim copying, provided that the
entire resulting derived work is distributed under the terms of a
permission notice identical to this one.
 Permission is granted to copy and distribute translations of this manual
into another language, under the above conditions for modified versions,
except that this permission notice may be stated in a translation
approved by the R Core Team.
 
Next: Installing R under Unix-alikes, Previous: Top, Up: Top   [Contents][Index] Sources, binaries and documentation for R can be obtained via
CRAN, the “Comprehensive R Archive Network” whose current
members are listed at https://CRAN.R-project.org/mirrors.html.
 
Next: Getting patched and development versions, Previous: Obtaining R, Up: Obtaining R   [Contents][Index] The simplest way is to download the most recent
R-x.y.z.tar.gz file, and unpack it with
 on systems that have a suitable1  tar installed.  On other systems you need to
have the gzip program installed, when you can use
 The pathname of the directory into which the sources are unpacked should
not contain spaces, as most make programs (and specifically
GNU make) do not expect spaces.
 If you want the build to be usable by a group of users, set umask
before unpacking so that the files will be readable by the target group
(e.g., umask 022 to be usable by all users).  Keep this
setting of umask whilst building and installing.
 If you use a recent GNU version of tar and do this
as a root account (which on Windows includes accounts with administrator
privileges) you may see many warnings about changing ownership.  In
which case you can use
 and perhaps also include the option --no-same-permissions.

(These options can also be set in the TAR_OPTIONS environment
variable: if more than one option is included they should be separated
by spaces.)
 
Previous: Getting and unpacking the sources, Up: Obtaining R   [Contents][Index] A patched version of the current release, ‘r-patched’, and the
current development version, ‘r-devel’, are available as daily
tarballs and via access to the R Subversion repository.  (For the two
weeks prior to the release of a minor (3.x.0) version, ‘r-patched’
tarballs may refer to beta/release candidates of the upcoming release,
the patched version of the current release being available via
Subversion.)
 The tarballs are available from
https://stat.ethz.ch/R/daily.  Download
R-patched.tar.gz or R-devel.tar.gz (or the .tar.bz2
versions) and unpack as described in the previous section.  They are
built in exactly the same way as distributions of R releases.
 
Previous: Getting patched and development versions, Up: Getting patched and development versions   [Contents][Index] Sources are also available via https://svn.R-project.org/R/, the
R Subversion repository.  If you have a Subversion client (see
https://subversion.apache.org/), you can check out and update the
current ‘r-devel’ from
https://svn.r-project.org/R/trunk/ and the current
‘r-patched’ from
‘https://svn.r-project.org/R/branches/R-x-y-branch/’
(where x and y are the major and minor number of the current
released version of R).  E.g., use
 to check out ‘r-devel’ into directory path (which will be
created if necessary).  The alpha, beta and RC versions of an upcoming
x.y.0 release are available from
‘https://svn.r-project.org/R/branches/R-x-y-branch/’ in
the four-week period prior to the release.
 Note that ‘https:’ is required2,
and that the SSL certificate for the Subversion server of the R
project should be recognized as from a trusted source.
 Note that retrieving the sources by e.g. wget -r or
svn export from that URL will not work (and will give a error
early in the make process): the Subversion information is
needed to build R.
 The Subversion repository does not contain the current sources for the
recommended packages, which can be obtained by rsync or
downloaded from CRAN.  To use rsync to install the
appropriate sources for the recommended packages, run
./tools/rsync-recommended from the top-level directory of the
R sources.
 If downloading manually from CRAN, do ensure that you have the
correct versions of the recommended packages: if the number in the file
VERSION is ‘x.y.z’ you need to download
the contents of ‘https://CRAN.R-project.org/src/contrib/dir’,
where dir is ‘x.y.z/Recommended’ for
r-devel or x.y-patched/Recommended for r-patched,
respectively, to directory src/library/Recommended in the sources
you have unpacked.  After downloading manually you need to execute
tools/link-recommended from the top level of the sources to
make the requisite links in src/library/Recommended.  A suitable
incantation from the top level of the R sources using wget
might be (for the correct value of dir)
 
Next: Installing R under Windows, Previous: Obtaining R, Up: Top   [Contents][Index] R will configure and build under most common Unix and Unix-alike
platforms including ‘cpu-*-linux-gnu’ for the
‘alpha’, ‘arm’, ‘hppa’, ‘ix86’,
‘m68k’, ‘mips’, ‘mipsel’, ‘powerpc’,
‘s390’, ‘sparc’, and ‘x86_64’ CPUs,
‘x86_64-apple-darwin’, ‘i386-sun-solaris’ and
‘sparc-sun-solaris’ as well as
perhaps (it is tested less frequently on these platforms)
‘i386-apple-darwin’, ‘i386-*-freebsd’, ‘x86_64-*-freebsd’,
‘i386-*-netbsd’, ‘x86_64/*-openbsd’ and
‘powerpc-ibm-aix6*’
 In addition, binary distributions are available for some common Linux
distributions and for OS X (formerly Mac OS).  See the FAQ for
current details.  These are installed in platform-specific ways, so for
the rest of this chapter we consider only building from the sources.
 Cross-building is not possible: installing R builds a minimal version
of R and then runs many R scripts to complete the build.
 
Next: Help options, Previous: Installing R under Unix-alikes, Up: Installing R under Unix-alikes   [Contents][Index] First review the essential and useful tools and libraries in
Essential and useful other programs under a Unix-alike, and install
those you

want or need.  Ensure that the environment variable TMPDIR is
either unset (and /tmp exists and can be written in and scripts
can be executed from) or points to the absolute path to a valid
temporary directory (one from which execution of scripts is allowed)
which does not contain spaces.3
 Choose a directory to install the R tree (R is not just a binary, but
has additional data sets, help files, font metrics etc).  Let us call
this place R_HOME.  Untar the source code.  This should create
directories src, doc, and several more under a top-level
directory: change to that top-level directory (At this point North
American readers should consult Setting paper size.)  Issue the
following commands:
 (See Using make if your make is not called ‘make’.)  Users of
Debian-based 64-bit systems4 may need
 Then check the built system works correctly by
 Failures are not necessarily problems as they might be caused by missing
functionality, but you should look carefully at any reported
discrepancies.  (Some non-fatal errors are expected in locales that do
not support Latin-1, in particular in true C locales and
non-UTF-8 non-Western-European locales.)  A failure in
tests/ok-errors.R may indicate inadequate resource limits
(see Running R).
 More comprehensive testing can be done by
 or
 see file tests/README and Testing a Unix-alike Installation
for the possibilities of doing this in parallel.  Note that these checks
are only run completely if the recommended packages are installed.
 If the command configure and make commands execute
successfully, a shell-script front-end called R will be created
and copied to R_HOME/bin.  You can link or copy this script
to a place where users can invoke it, for example to
/usr/local/bin/R.  You could also copy the man page R.1 to
a place where your man reader finds it, such as
/usr/local/man/man1.  If you want to install the complete R
tree to, e.g., /usr/local/lib/R, see Installation.  Note:
you do not need to install R: you can run it from where it was
built.
 You do not necessarily have to build R in the top-level source
directory (say, TOP_SRCDIR).  To build in
BUILDDIR, run
 and so on, as described further below.  This has the advantage of always
keeping your source tree clean and is particularly recommended when you
work with a version of R from Subversion.  (You may need
GNU make to allow this, and you will need no spaces
in the path to the build directory.  It is unlikely to work if the
source directory has previously been used for a build.)
 Now rehash if necessary, type R, and read the R manuals
and the R FAQ (files FAQ or
doc/manual/R-FAQ.html, or
https://CRAN.R-project.org/doc/FAQ/R-FAQ.html which always
has the version for the latest release of R).
 Note: if you already have R installed, check that where you installed
R replaces or comes earlier in your path than the previous
installation.  Some systems are set up to have /usr/bin (the
standard place for a system installation) ahead of /usr/local/bin
(the default place for installation of R) in their default path, and
some do not have /usr/local/bin on the default path.
 
Next: Making the manuals, Previous: Simple compilation, Up: Installing R under Unix-alikes   [Contents][Index] By default HTML help pages are created when needed rather than being
built at install time.
 If you need to disable the server and want HTML help, there is the
option to build HTML pages when packages are installed
(including those installed with R).  This is enabled by the
configure option --enable-prebuilt-html.  Whether
R CMD INSTALL (and hence install.packages) pre-builds
HTML pages is determined by looking at the R installation and is
reported by R CMD INSTALL --help: it can be overridden by
specifying one of the INSTALL options --html or
--no-html.
 The server is disabled by setting the environment variable

R_DISABLE_HTTPD to a non-empty value, either before R is
started or within the R session before HTML help (including
help.start) is used.  It is also possible that system security
measures will prevent the server from being started, for example if the
loopback interface has been disabled.  See
?tools::startDynamicHelp for more details.
 
Next: Installation, Previous: Help options, Up: Installing R under Unix-alikes   [Contents][Index] There is a set of manuals that can be built from the sources,
 Printed versions of all the help pages for base and recommended packages
(around 3500 pages).
 Printed versions of the help pages for selected base packages (around
2000 pages)
 R FAQ
 “An Introduction to R”.
 “R Data Import/Export”.
 “R Installation and Administration”, this manual.
 “Writing R Extensions”.
 “The R Language Definition”.
 To make these (with ‘fullrefman’ rather than ‘refman’), use
 You will not be able to build any of these unless you have
texi2any version 5.1 or later installed, and for PDF you must
have texi2dvi and texinfo.tex installed (which are part
of the GNU texinfo distribution but are, especially
texinfo.tex, often made part of the TeX package in
re-distributions).  For historical reasons, the path to
texi2any can be set by macro ‘MAKEINFO’ in
config.site (makeinfo is nowadays a link to
texi2any).
 The PDF versions can be viewed using any recent PDF viewer: they have
hyperlinks that can be followed.  The info files are suitable for
reading online with Emacs or the standalone GNU info
program.  The PDF versions will be created using the paper size selected
at configuration (default ISO a4): this can be overridden by setting
R_PAPERSIZE

on the make command line, or setting R_PAPERSIZE in the
environment and using make -e.  (If re-making the manuals for
a different paper size, you should first delete the file
doc/manual/version.texi.  The usual value for North America would
be ‘letter’.)
 There are some issues with making the PDF reference manual,
fullrefman.pdf or refman.pdf.  The help files contain both
ISO Latin1 characters (e.g. in text.Rd) and upright quotes,
neither of which are contained in the standard LaTeX Computer Modern
fonts.  We have provided four alternatives:
 (The default.) Using standard PostScript fonts, Times Roman, Helvetica
and Courier.  This works well both for on-screen viewing and for
printing.  One disadvantage is that the Usage and Examples sections may
come out rather wide: this can be overcome by using in addition
either of the options inconsolata (on a Unix-alike only if found
by configure) or beramono, which replace the Courier
monospaced font by Inconsolata or Bera Sans mono respectively.  (You
will need a recent version of the appropriate LaTeX package
inconsolata5 or
bera installed.)
 Note that in most LaTeX installations this will not actually use the
standard fonts for PDF, but rather embed the URW clones NimbusRom,
NimbusSans and (for Courier, if used) NimbusMon.
 This needs LaTeX packages times, helvetic and (if used)
courier installed.
 Using the Latin Modern fonts.  These are not often installed as
part of a TeX distribution, but can obtained from
https://www.ctan.org/tex-archive/fonts/ps-type1/lm/ and
mirrors.  This uses fonts rather similar to Computer Modern, but is not
so good on-screen as times.
 Using type-1 versions of the Computer Modern fonts by Vladimir Volovich.
This is a large installation, obtainable from
https://www.ctan.org/tex-archive/fonts/ps-type1/cm-super/
and its mirrors.  These type-1 fonts have poor hinting and so are
nowhere near as readable on-screen as the other three options.
 A package to use composites of Computer Modern fonts.  This works well
most of the time, and its PDF is more readable on-screen than the
previous two options.  There are three fonts for which it will need to
use bitmapped fonts, tctt0900.600pk, tctt1000.600pk and
tcrm1000.600pk.  Unfortunately, if those files are not available,
Acrobat Reader will substitute completely incorrect glyphs so you need
to examine the logs carefully.
 The default can be overridden by setting the environment variable

R_RD4PDF.  (On Unix-alikes, this will be picked up at install time
and stored in etc/Renviron, but can still be overridden when the
manuals are built, using make -e.)  The usual6  default value for R_RD4PDF is
‘times,inconsolata,hyper’: omit ‘hyper’ if you do not want
hyperlinks (e.g. for printing the manual) or do not have LaTeX
package hyperref, and omit ‘inconsolata’ if you do not have
LaTeX package inconsolata installed.
 Further options, e.g for hyperref, can be included in a file
Rd.cfg somewhere on your LaTeX search path.  For example, if
you prefer the text and not the page number in the table of contents to
be hyperlinked use
 or 
 to hyperlink both text and page number.
 Ebook versions of most of the manuals in one or both of .epub and
.mobi formats can be made by running in doc/manual one of
 This requires ebook-convert from Calibre
(http://calibre-ebook.com/download), or from most Linux
distributions).  If necessary the path to ebook-convert can be
set as make macro EBOOK to by editing doc/manual/Makefile
(which contains a commented value suitable for OS X).
 
Next: Uninstallation, Previous: Making the manuals, Up: Installing R under Unix-alikes   [Contents][Index] To ensure that the installed tree is usable by the right group of users,
set umask appropriately (perhaps to ‘022’) before unpacking
the sources and throughout the build process.
 After
 (or, when building outside the source,
TOP_SRCDIR/configure, etc) have been completed
successfully, you can install the complete R tree to your system by
typing
 A parallel make can be used (but run make before make
install).  Those using GNU make 4.0 or later may want to use
make -j n -O to avoid interleaving of output.
 This will install to the following directories:
 the front-end shell script and other scripts and executables
 the man page
 all the rest (libraries, on-line help system, …).  Here
LIBnn is usually ‘lib’, but may be ‘lib64’ on some
64-bit Linux systems.  This is known as the R home directory.
 where prefix is determined during configuration (typically
/usr/local) and can be set by running configure with
the option --prefix, as in
 where the value should be an absolute path.  This causes make
install to install the R script to
/where/you/want/R/to/go/bin, and so on.  The prefix of the
installation directories can be seen in the status message that is
displayed at the end of configure.  The installation may need
to be done by the owner of prefix, often a root account.
 You can install into another directory tree by using
 at least with GNU or Solaris make (but not some
older Unix makes).
 More precise control is available at configure time via options: see
configure --help for details.  (However, most of the ‘Fine
tuning of the installation directories’ options are not used by R.)
 Configure options --bindir and --mandir are supported
and govern where a copy of the R script and the man
page are installed.
 The configure option --libdir controls where the main R
files are installed: the default is ‘eprefix/LIBnn’,
where eprefix is the prefix used for installing
architecture-dependent files, defaults to prefix, and can be set
via the configure option --exec-prefix.
 Each of bindir, mandir and libdir can also be
specified on the make install command line (at least for
GNU make).
 The configure or make variables rdocdir and
rsharedir can be used to install the system-independent
doc and share directories to somewhere other than
libdir.  The C header files can be installed to the value of
rincludedir: note that as the headers are not installed into a
subdirectory you probably want something like
rincludedir=/usr/local/include/R-3.3.1.
 If you want the R home to be something other than
libdir/R, use rhome: for example
 will use a version-specific R home on a non-Debian Linux 64-bit
system.
 If you have made R as a shared/static library you can install it in
your system’s library directory by
 where prefix is optional, and libdir will give more
precise control.7  However, you should not install
to a directory mentioned in LDPATHS (e.g.
/usr/local/lib64) if you intend to work with multiple versions of
R, since that directory may be given precedence over the lib
directory of other R installations.
 will install stripped executables, and on platforms where this is
supported, stripped libraries in directories lib and
modules and in the standard packages.
 Note that installing R into a directory whose path contains spaces is
not supported, and some aspects (such as installing source packages)
will not work.
 To install info and PDF versions of the manuals, use one or both of
 Once again, it is optional to specify prefix, libdir or
rhome (the PDF manuals are installed under the R home
directory).  (make install-info needs Perl installed
if there is no command install-info on the system.)
 More precise control is possible.  For info, the setting used is that of
infodir (default prefix/info, set by configure
option --infodir).  The PDF files are installed into the R
doc tree, set by the make variable rdocdir.
 A staged installation is possible, that it is installing R into a
temporary directory in order to move the installed tree to its final
destination.  In this case prefix (and so on) should reflect the

final destination, and DESTDIR should be used: see
https://www.gnu.org/prep/standards/html_node/DESTDIR.html.
 You can optionally install the run-time tests that are part of
make check-all by
 which populates a tests directory in the installation.
 
Next: Sub-architectures, Previous: Installation, Up: Installing R under Unix-alikes   [Contents][Index] You can uninstall R by
 optionally specifying prefix etc in the same way as specified for
installation.
 This will also uninstall any installed manuals.  There are specific
targets to uninstall info and PDF manuals in file
doc/manual/Makefile.
 Target uninstall-tests will uninstall any installed tests, as
well as removing the directory tests containing the test results.
 An installed shared/static libR can be uninstalled by
 
Next: Other Options, Previous: Uninstallation, Up: Installing R under Unix-alikes   [Contents][Index] Some platforms can support closely related builds of R which can
share all but the executables and dynamic objects.  Examples include
builds under Linux and Solaris for different CPUs or 32- and
64-bit builds.
 R supports the idea of architecture-specific builds, specified by
adding ‘r_arch=name’ to the configure line.  Here
name can be anything non-empty, and is used to name subdirectories
of lib, etc, include and the package libs
subdirectories.  Example names from other software are the use of
sparcv9 on Sparc Solaris and 32 by gcc on
‘x86_64’ Linux.
 If you have two or more such builds you can install them over each other
(and for 32/64-bit builds on one architecture, one build can be done
without ‘r_arch’).  The space savings can be considerable: on
‘x86_64’ Linux a basic install (without debugging symbols) took
74Mb, and adding a 32-bit build added 6Mb.  If you have installed
multiple builds you can select which build to run by
 and just running ‘R’ will run the last build that was installed.
 R CMD INSTALL will detect if more than one build is installed and
try to install packages with the appropriate library objects for each.
This will not be done if the package has an executable configure
script or a src/Makefile file.  In such cases you can install for
extra builds by
 If you want to mix sub-architectures compiled on different platforms
(for example ‘x86_64’ Linux and ‘i686’ Linux), it is
wise to use explicit names for each, and you may also need to set
libdir to ensure that they install into the same place.
 When sub-architectures are used the version of Rscript in
e.g. /usr/bin will be the last installed, but
architecture-specific versions will be available in e.g.
/usr/lib64/R/bin/exec${R_ARCH}.  Normally all installed
architectures will run on the platform so the architecture of
Rscript itself does not matter.  The executable
Rscript will run the R script, and at that time the

setting of the R_ARCH environment variable determines the
architecture which is run.
 When running post-install tests with sub-architectures, use
 to select a sub-architecture to check.
 Sub-architectures are also used on Windows, but by selecting executables
within the appropriate bin directory,
R_HOME/bin/i386 or R_HOME/bin/x64.  For
backwards compatibility there are executables
R_HOME/bin/R.exe and R_HOME/bin/Rscript.exe:
these will run an executable from one of the subdirectories, which one
being taken first from the

R_ARCH environment variable, then from the
--arch command-line option8 and finally from the
installation default (which is 32-bit for a combined 32/64 bit R
installation).
 
Previous: Sub-architectures, Up: Sub-architectures   [Contents][Index] For some Linux distributions9, there is an alternative mechanism for mixing
32-bit and 64-bit libraries known as multilib. If the Linux
distribution supports multilib, then parallel builds of R may be
installed in the sub-directories lib (32-bit) and lib64
(64-bit).  The build to be run may then be selected using the
setarch command. For example, a 32-bit build may be run by
 The setarch command is only operational if both 32-bit and
64-bit builds are installed. If there is only one installation of R,
then this will always be run regardless of the architecture specified
by the setarch command.
 There can be problems with installing packages on the non-native
architecture.  It is a good idea to run e.g. setarch i686 R for
sessions in which packages are to be installed, even if that is the only
version of R installed (since this tells the package installation
code the architecture needed).
 There is a potential problem with packages using Java, as the
post-install for a ‘i686’ RPM on ‘x86_64’ Linux
reconfigures Java and will find the ‘x86_64’ Java.  If you know
where a 32-bit Java is installed you may be able to run (as root)
 to get a suitable setting.
 When this mechanism is used, the version of Rscript in
e.g. /usr/bin will be the last installed, but an
architecture-specific version will be available in
e.g. /usr/lib64/R/bin.  Normally all installed architectures
will run on the platform so the architecture of Rscript does
not matter.
 
Next: Testing a Unix-alike Installation, Previous: Sub-architectures, Up: Installing R under Unix-alikes   [Contents][Index] There are many other installation options, most of which are listed by
configure --help.  Almost all of those not listed elsewhere in
this manual are either standard autoconf options not relevant
to R or intended for specialist uses by the R developers.
 One that may be useful when working on R itself is the option
--disable-byte-compiled-packages, which ensures that the base
and recommended packages are not byte-compiled.  (Alternatively the
(make or environment) variable R_NO_BASE_COMPILE can be set to a
non-empty value for the duration of the build.)
 Option --with-internal-tzcode makes use of R’s own code and
copy of the Olson database for managing timezones.  This will be
preferred where there are issues with the system implementation, usually
involving times after 2037 or before 1916.  An alternative time-zone
directory10 can be used, pointed
to by environment variable TZDIR: this should contain files such
as Europe/London.  On all tested OSes the system timezone was
deduced correctly, but if necessary it can be set as the value of
environment variable TZ.
 
Previous: Other Options, Up: Other Options   [Contents][Index] By default configure searches for suitable
options11 for OpenMP support for the C, C++98, FORTRAN 77 and
Fortran compilers.
 Only the C result is currently used for R itself, and only if
MAIN_LD/DYLIB_LD were not specified.  This can be
overridden by specifying
 Use for packages has similar restrictions (involving SHLIB_LD and
similar: note that as FORTRAN 77 code is normally linked by the C
compiler, both need to support OpenMP) and can be overridden by
specifying some of
 Setting to an empty value will disable OpenMP for that compiler (and
configuring with --disable-openmp will disable all detection of
OpenMP).  The configure detection test is to compile and link
a standalone OpenMP program, which is not the same as compiling a shared
object and loading it into the C program of R’s executable.  Note
that overridden values are not tested.
 
Previous: Other Options, Up: Installing R under Unix-alikes   [Contents][Index] Full post-installation testing is possible only if the test files have
been installed with
 which populates a tests directory in the installation.
 If this has been done, two testing routes are available.
The first is to move to the home directory of the R installation
(as given by R.home()) and run
 and other useful targets are test-BasePackages and
test-Recommended to the run tests of the standard and
recommended packages (if installed) respectively.
 This re-runs all the tests relevant to the installed R (including for
example code in the package vignettes), but not for example the ones
checking the example code in the manuals nor making the standalone Rmath
library.  This can occasionally be useful when the operating environment
has been changed, for example by OS updates or by substituting the
BLAS (see Shared BLAS).
 Parallel checking of packages may be possible: set the environment
variable TEST_MC_CORES to the maximum number of processes to be
run in parallel.  This affects both checking the package examples (part
of make check) and package sources (part of make
check-devel and make check-recommended).  It does require a
make command which supports the make -j n
option: most do but on Solaris you need to select GNU make or
dmake.  Where parallel checking of package sources is done, a log
file pngname.log is left in the tests directory for
inspection.
 Alternatively, the installed R can be run, preferably with
--vanilla.  Then

 runs the basic tests and then all the tests on the standard and
recommended packages.  These tests can be run from anywhere: the basic
tests write their results in the tests folder of the R home
directory and run fewer tests than the first approach: in particular
they do not test things which need Internet access—that can be tested
by
 These tests work best if diff (in Rtools*.exe for
Windows users) is in the path.
 It is possible to test the installed packages (but not their
package-specific tests) by testInstalledPackages even if
make install-tests was not run.
 Note that the results may depend on the language set for times and
messages: for maximal similarity to reference results you may want to
try setting (before starting the R session)
 and use a UTF-8 or Latin-1 locale.
 
Next: Installing R under OS X, Previous: Installing R under Unix-alikes, Up: Top   [Contents][Index] The bin/windows directory of a CRAN site contains
binaries for a base distribution and a large number of add-on packages
from CRAN to run on 32- or 64-bit Windows (XP or later) on
‘ix86’ and ‘x86_64’ CPUs.
 Your file system must allow long file names (as is likely except
perhaps for some network-mounted systems).  If it doesn’t also support
conversion to short name equivalents (a.k.a. DOS 8.3 names), then R
must be installed in a path that does not contain spaces.
 Installation is via the installer
R-3.3.1-win.exe.  Just double-click on the icon and
follow the instructions.  When installing on a 64-bit version of Windows
the options will include 32- or 64-bit versions of R (and the default is
to install both).  You can uninstall R from the Control Panel.
 Note that you will be asked to choose a language for installation, and
that choice applies to both installation and un-installation but not to
running R itself.
 See the R
Windows FAQ for more details on the binary installer.
 
Next: Testing a Windows Installation, Previous: Installing R under Windows, Up: Installing R under Windows   [Contents][Index] R can be built as either a 32-bit or 64-bit application on Windows:
to build the 64-bit application you need a 64-bit edition of Windows:
such an OS can also be used to build 32-bit R.
 The standard installer combines 32-bit and 64-bit builds into a single
executable which can then be installed into the same location and share
all the files except the .exe and .dll files and some
configuration files in the etc directory.
 Building is only tested in a 8-bit locale: using a multi-byte locale (as
used for CJK languages) is unsupported and may not work (the scripts do
try to select a ‘C’ locale; Windows may not honour this).
 NB: The build process is currently being changed to require
external binary distributions of third-party software.  Their location
is set using macro EXT_LIBS with default setting
$(LOCAL_SOFT); the $(LOCAL_SOFT) macro defaults to
$(R_HOME)/extsoft.  This directory can be populated using
make rsync-extsoft. The location can be overridden by
setting EXT_LIBS to a different path in
src/gnuwin32/MkRules.local. A suitable collection of files can
also be obtained from 
https://CRAN.R-project.org/bin/windows/extsoft or
https://www.stats.ox.ac.uk/pub/Rtools/libs.html.
 
Next: Getting the source files, Previous: Building from source, Up: Building from source   [Contents][Index] If you want to build R from the sources, you will first need to
collect, install and test an extensive set of tools.  See The Windows toolset (and perhaps updates in
https://CRAN.R-project.org/bin/windows/Rtools/) for details.
 The Rtools*.exe executable installer described in The Windows toolset also includes some source files in addition to the R
source as noted below.  You should run it first, to obtain a working
tar and other necessities.  Choose a “Full installation”, and
install the extra files into your intended R source directory, e.g.
C:/R. The directory name should not contain spaces. We
will call this directory R_HOME below.
 
Next: Building the core files, Previous: Getting the tools, Up: Building from source   [Contents][Index] You need to collect the following sets of files:
 to create the source tree in R_HOME.  Beware: do use
tar to extract the sources rather than tools such as WinZip.
If you are using an account with administrative privileges you may get a
lot of messages which can be suppressed by
 
or perhaps better, set the environment variable TAR_OPTIONS to the
value ‘--no-same-owner --no-same-permissions’.
 It is also possible to obtain the source code using Subversion; see 
Obtaining R for details.
 (or a more recent version if appropriate), create an empty directory, 
say c:/R/extsoft, and unpack it in
that directory by e.g.
 and edit MkRules.local, uncommenting EXT_LIBS and setting
it to the appropriate path (in our example c:/R/extsoft).
 Look through the file MkRules.local and make any other changes
needed: in particular, this is where a 64-bit build is selected and the
locations are set of external software for ICU collation and the
cairo-based devices.
 The following additional item is normally installed by
Rtools*.exe.  If instead you choose to do a completely manual
build you will also need
 
Next: Building the cairo devices files, Previous: Getting the source files, Up: Building from source   [Contents][Index] Set the environment variable TMPDIR to the absolute path to a
writable directory, with a path specified with forward slashes and no
spaces.  (The default is /tmp, which may not be useful on
Windows.)
 You may need to compile under a case-honouring file system: we found
that a samba-mounted file system (which maps all file names to
lower case) did not work.
 Open a command window at R_HOME/src/gnuwin32, then run
 and sit back and wait while the basic compile takes place.
 Notes:
 but this is only likely to be worthwhile on a multi-core machine with
ample memory, and is not 100% reliable.
 
Next: Using ICU for collation, Previous: Building the core files, Up: Building from source   [Contents][Index] The devices based on cairographics (svg, cairo_pdf,
cairo_ps and the type = "cairo" versions of png,
jpeg, tiff and bmp) are implemented in a separate
DLL winCairo.dll which is loaded when one of these devices is
first used.  It is not built by default, and needs to be built (after
make all) by make cairodevices.
 To enable the building of these devices you need to install the static
cairographics libraries built by Simon Urbanek at
https://www.rforge.net/Cairo/files/cairo-current-win.tar.gz.  Set
the macro ‘CAIRO_HOME’ in MkRules.local.  (Note that this
tarball unpacks with a top-level directory src/:
‘CAIRO_HOME’ needs to include that directory in its path.)
 
Next: Support for libcurl, Previous: Building the cairo devices files, Up: Building from source   [Contents][Index] It is recommended to build R to support ICU (International Components
for Unicode, http://site.icu-project.org/) for collation, as is
commonly done on Unix-alikes.
 Two settings are needed in MkRules.local, 
 The first should be uncommented and the second set to the top-level
directory of a suitably packaged binary build of ICU, for example that
at https://www.stats.ox.ac.uk/pub/Rtools/goodies/ICU_531.zip.
Depending on the build, it may be necessary to edit the macro
ICU_LIBS.
 Unlike on a Unix-alike, it is normally necessary to call
icuSetCollate to set a locale before ICU is actually used for
collation, or set the environment variable R_ICU_LOCALE.
 
Next: Checking the build, Previous: Using ICU for collation, Up: Building from source   [Contents][Index] libcurl version 7.28.0 or later can be used to support
curlGetHeaders and the "libcurl" methods of
download.file and url.
 A suitable distribution can be found via
https://www.stats.ox.ac.uk/pub/Rtools/libs.html and its unpacked
location should be specified in file MkRules.local.
 For secure use of e.g. ‘https://’ URLs Windows users may need to
specify the path to up-to-date CA root certificates: see
?download.file.
 
Next: Building the manuals, Previous: Support for libcurl, Up: Building from source   [Contents][Index] You can test a build by running
 The recommended packages can be checked by
 Other levels of checking are
 for a more thorough check of the R functionality, and
 for both check-devel and check-recommended.
 If a test fails, there will almost always be a .Rout.fail file in
the directory being checked (often tests/Examples or
tests): examine the file to help pinpoint the problem.
 Parallel checking of package sources (part of make check-devel
and make check-recommended) is possible: see the environment
variable TEST_MC_CORES to the maximum number of processes to be
run in parallel.
 
Next: Building the Inno Setup installer, Previous: Checking the build, Up: Building from source   [Contents][Index] The PDF manuals require texinfo 5.1 or later, and can be made by
 If you want to make the info versions (not including the Reference
Manual), use
 (all assuming you have pdftex/pdflatex installed and
in your path).
 See the Making the manuals section in the Unix-alike section for setting
options such as the paper size and the fonts used.
 By default it is assumed that texinfo is not installed, and the
manuals will not be built.  The comments in file MkRules.dist
describe settings to build them.  (Copy that file to
MkRules.local and edit it.)  The texinfo 5.x package for
use on Windows is available at
https://www.stats.ox.ac.uk/pub/Rtools/: you will also need to
install Perl12
 
Next: Building the MSI installer, Previous: Building the manuals, Up: Building from source   [Contents][Index] You need to have the files for a complete R build, including bitmap and
Tcl/Tk support and the manuals (which requires texinfo installed),
as well as the recommended packages and Inno Setup (see The Inno Setup installer).
 Once everything is set up
 will make all the pieces and the installer and put them in the
gnuwin32/cran subdirectory, then check the build.  This works by
building all the parts in the sequence:
 The parts can be made individually if a full build is not needed, but
earlier parts must be built before later ones.  (The Makefile
doesn’t enforce this dependency—some build targets force a lot of
computation even if all files are up to date.)  The first four targets
are the default build if just make (or make all) is
run.
 Parallel make is not supported and likely to fail.
 If you want to customize the installation by adding extra packages,
replace make rinstaller by something like
 An alternative way to customize the installer starting with a binary
distribution is to first make an installation of R from the standard
installer, then add packages and make other customizations to that
installation.  Then (after having customized file MkRules,
possibly via MkRules.local, and having made R in the
source tree) in src/gnuwin32/installer run
 where rootdir is the path to the root of the customized
installation (in double quotes if it contains spaces or backslashes).
 Both methods create an executable with a standard name such as
R-3.3.1-win.exe, so please rename it to indicate that
it is customized.  If you intend to distribute a customized
installer please do check that license requirements are met – note that
the installer will state that the contents are distributed under GPL
and this has a requirement for you to supply the complete sources
(including the R sources even if you started with a binary distribution
of R, and also the sources of any extra packages (including their
external software) which are included).
 The defaults for the startup parameters may also be customized.  For example
 will create an installer that defaults to installing R to run in SDI
mode.  See src/gnuwin32/installer/Makefile for the names and
values that can be set.
 The standard CRAN distribution of a 32/64-bit installer is
made by first building 32-bit R (just
 is needed), and then (in a separate directory) building 64-bit R with
the macro HOME32 set in file MkRules.local to the
top-level directory of the 32-bit build.  Then the make
rinstaller step copies the files that differ between architectures from
the 32-bit build as it builds the installer image.
 
Next: 64-bit Windows builds, Previous: Building the Inno Setup installer, Up: Building from source   [Contents][Index] It is also possible to build an installer for use with Microsoft
Installer.  This is intended for use by sysadmins doing automated
installs, and is not recommended for casual use.
 It makes use of the Windows Installer XML (WiX) toolkit version
3.5 (or perhaps later, untested) available from
http://wixtoolset.org/.  Once WiX is installed, set the path to
its home directory in MkRules.local.
 You need to have the files for a complete R build, including bitmap and
Tcl/Tk support and the manuals, as well as the recommended packages.
There is no option in the installer to customize startup options, so
edit etc/Rconsole and etc/Rprofile.site to set these as
required.  Then
 which will result in a file with a name like
R-3.3.1-win32.msi.  This can be double-clicked to be
installed, but those who need it will know what to do with it (usually
by running msiexec /i with additional options).  Properties
that users might want to set from the msiexec command line
include ‘ALLUSERS’, ‘INSTALLDIR’ (something like
c:\Program Files\R\R-3.3.1) and ‘RMENU’ (the path
to the ‘R’ folder on the start menu) and ‘STARTDIR’ (the
starting directory for R shortcuts, defaulting to something like
c:\Users\name\Documents\R).
 The MSI installer can be built both from a 32-bit build of R
(R-3.3.1-win32.msi) and from a 64-bit build of R
(R-3.3.1-win64.msi, optionally including 32-bit files
by setting the macro HOME32, when the name is
R-3.3.1-win.msi).  Unlike the main installer, a 64-bit
MSI installer can only be run on 64-bit Windows.
 Thanks to David del Campo (Dept of Statistics, University of Oxford)
for suggesting WiX and building a prototype installer.
 
Previous: Building the MSI installer, Up: Building from source   [Contents][Index] To build a 64-bit version of R you need a 64-bit toolchain: the only one
discussed here is based on the work of the MinGW-w64 project
(http://sourceforge.net/projects/mingw-w64/, but commercial
compilers such as those from Intel and PGI could be used (and have been
by R redistributors).
 Support for MinGW-w64 was developed in the R sources over the period
2008–10 and was first released as part of R 2.11.0.  The assistance
of Yu Gong at a crucial step in porting R to MinGW-w64 is gratefully
acknowledged, as well as help from Kai Tietz, the lead developer of the
MinGW-w64 project.
 Windows 64-bit is now completely integrated into the R and package
build systems: a 64-bit build is selected in file MkRules.local.
 
Previous: Building from source, Up: Installing R under Windows   [Contents][Index] The Windows installer contains a set of test files used when building
R.
 The Rtools are not needed to run these tests. but more
comprehensive analysis of errors will be given if diff is in
the path (and errorsAreFatal = FALSE is then not needed below).
 Launch either Rgui or Rterm, preferably with
--vanilla.  Then run
 runs the basic tests and then all the tests on the standard and
recommended packages.  These tests can be run from anywhere: they write
some of their results in the tests folder of the R home
directory (as given by R.home()), and hence may need to be run
under the account used to install R.
 The results of example(md5sums) when testing tools will
differ from the reference output as some files are installed with
Windows’ CRLF line endings.
 
Next: Running R, Previous: Installing R under Windows, Up: Top   [Contents][Index] The front page of a CRAN site has a link ‘Download R for OS
X’. Click on that, then download the file R-3.3.1.pkg
and install it. This runs on OS X 10.9 and later (Mavericks, Yosemite,
El Capitan, …).
 Installers for R-patched and R-devel are usually available from
https://r.research.att.com.
 For some older versions of the OS you can in principle (it is little
tested) install R from the sources.
 It is important that if you use a binary installer package that your OS
is fully updated: look at ‘Updates’ from the ‘App Store’ to be sure.
(If using XQuartz, check that is current.)
 To install, just double-click on the icon of the file you downloaded.
At the ‘Installation Type’ stage, note the option to ‘Customize’.  This
currently shows four components: everyone will need the ‘R Framework’
component: the remaining components are optional. (The ‘Tcl/Tk’ component
is needed to use package tcltk. The ‘Texinfo’ component is only
needed by those installing source packages.)
 This is an Apple Installer package. If you encounter any problem during
the installation, please check the Installer log by clicking on the
“Window” menu and item “Installer Log”. The full output (select
“Show All Log”) is useful for tracking down problems.  Note the the
installer is clever enough to try to upgrade the last-installed version
of the application where you installed it (which may not be where you
want this time …).
 Various parts of the build require XQuartz to be installed: : see
https://xquartz.macosforge.org/.  These include the tcltk
package and the X11 device: attempting to use these without
XQuartz will remind you.
 If you update your OS X version, you should re-install R (and perhaps
XQuartz): the installer tailors the installation to the current version
of the OS.
 For building R from source, see OS X.
 
Next: Uninstalling under OS X, Previous: Installing R under OS X, Up: Installing R under OS X   [Contents][Index] There are two ways to run R on OS X from a CRAN binary
distribution.
 There is a GUI console normally installed with the R icon in
/Applications which you can run by double-clicking (e.g. from
Launchpad or Finder).  (If you cannot find it there it was possibly
installed elsewhere so try searching for it in Spotlight.) This is
usually referred to as R.APP to distinguish it from command-line R:
its user manual is currently part of the OS X FAQ at
https://cran.r-project.org/bin/macosx/RMacOSX-FAQ.html and
can be viewed from R.APP’s ‘Help’ menu.
 You can run command-line R and Rscript from a
Terminal13 so these can be
typed as commands like any other Unix-alike: see the next chapter of
this manual.  There are some small differences which may surprise users
of R on other platforms, notably the default location of the personal
library directory (under ~/Library/R,
e.g. ~/Library/R/3.3/library), and that warnings, messages and
other output to stderr are highlighted in bold.
 It has been reported that running R.APP under Yosemite may fail if no
preferences are stored, so if it fails when launched for the very first
time, try it again (the first attempt will store some preferences).
 Users of R.APP need to be aware of the ‘App Nap’ feature
(https://developer.apple.com/library/mac/releasenotes/MacOSX/WhatsNewInOSX/Articles/MacOSX10_9.html)
which can cause R tasks to appear to run very slowly when not
producing output in the console.  Here are ways to avoid it:
 (see https://developer.apple.com/library/mac/releasenotes/MacOSX/WhatsNewInOSX/Articles/MacOSX10_9.html).
 Using the X11 device or the X11-based versions of View()
and edit() for data frames and matrices (the latter are the
default for command-line R but not R.APP) requires an X sub-system
to be installed: see OS X.  (As do the tcltk package and
some third-party packages.)
 
Next: Multiple versions, Previous: Running R under OS X, Up: Installing R under OS X   [Contents][Index] R for OS X consists of two parts: the GUI (R.APP) and the R
framework. The un-installation is as simple as removing those folders
(e.g. by dragging them into the Trash). The typical installation will
install the GUI into the /Applications/R.app folder and the R
framework into the /Library/Frameworks/R.framework folder.  The
links to R and Rscript in /usr/bin or
/usr/local/bin should also be removed.
 If you want to get rid of R more completely using a Terminal, simply
run (use /usr/local/bin as from El Capitan):
 The installation consists of four Apple packages:
org.r-project.R.x86_64.fw.pkg,
org.r-project.R.x86_64.GUI.pkg,
org.r-project.x86_64.tcltk.x11 and
org.r-project.x86_64.texinfo (not all of which need be
installed). You can use pkgutil --forget if you want the Apple
Installer to forget about the package without deleting its files (useful
for the R framework when installing multiple R versions in parallel),
or after you have deleted the files.
 Uninstalling the Tcl/Tk or Texinfo components (which are installed under
/usr/local) is not as simple.  You can list the files they installed
in a Terminal by
 These are paths relative to /, the root of the file system.
 
Previous: Uninstalling under OS X, Up: Installing R under OS X   [Contents][Index] The installer will remove any previous version of the R framework
which it finds installed.  This can be avoided by using pkgutil
--forget (see the previous section).  However, note that different
versions are installed under
/Library/Frameworks/R.framework/Versions as 3.2,
3.3 and so on, so it is not possible to have different
‘3.x.y’ versions installed for the same ‘x’.
 A version of R can be run directly from the command-line as e.g.
 However, R.APP will always run the ‘current’ version, that is the last
installed version.  A small utility, Rswitch.app (available at
https://r.research.att.com/#other), can be used to change the
‘current’ version.  This is of limited use as R.APP is compiled
against a particular version of R and will likely crash if switched
to an earlier version.  This may allow you to install a development
version of R (de-selecting R.APP) and then switch back to the
release version.
 
Next: Add-on packages, Previous: Installing R under OS X, Up: Top   [Contents][Index] How to start R and what command-line options are available is discussed
in Invoking R in An Introduction to R.
 You should ensure that the shell has set adequate resource limits: R
expects a stack size of at least 8MB and to be able to open at least 256
file descriptors.  (Any modern OS should have default limits at least as
large as these, but apparently NetBSD may not.  Use the shell command
ulimit (sh/bash) or limit
(csh/tcsh) to check.)
 R makes use of a number of environment variables, the default values
of many of which are set in file R_HOME/etc/Renviron (there
are none set by default on Windows and hence no such file).  These are
set at configure time, and you would not normally want to

change them – a possible exception is R_PAPERSIZE (see Setting paper size).  The paper size will be deduced from the ‘LC_PAPER’
locale category if it exists and R_PAPERSIZE is unset, and this
will normally produce the right choice from ‘a4’ and ‘letter’
on modern Unix-alikes (but can always be overridden by setting
R_PAPERSIZE).
 Various environment variables can be set to determine where R creates
its per-session temporary directory.  The environment variables



TMPDIR, TMP and TEMP are searched in turn and the
first one which is set and points to a writable area is used.  If none
do, the final default is /tmp on Unix-alikes and the value of

R_USER on Windows.  The path should be an absolute path not
containing spaces (and it is best to avoid non-alphanumeric characters
such as +).
 Some Unix-alike systems are set up to remove files and directories
periodically from /tmp, for example by a cron job

running tmpwatch.  Set TMPDIR to another directory
before starting long-running jobs on such a system.
 Note that TMPDIR will be used to execute configure
scripts when installing packages, so if /tmp has been mounted as
‘noexec’, TMPDIR needs to be set to a directory from which
execution is allowed.
 
Next: Internationalization, Previous: Running R, Up: Top   [Contents][Index] It is helpful to use the correct terminology.  A package is
loaded from a library by the function library().  Thus a
library is a directory containing installed packages; the main library
is R_HOME/library, but others can be used, for example by

setting the environment variable R_LIBS or using the R function
.libPaths().
 
Next: Managing libraries, Previous: Add-on packages, Up: Add-on packages   [Contents][Index] The set of packages loaded on startup is by default
 (plus, of course, base) and this can be changed by setting the
option in startup code (e.g. in ~/.Rprofile).  It is initially

set to the value of the environment variable R_DEFAULT_PACKAGES if
set (as a comma-separated list).  Setting R_DEFAULT_PACKAGES=NULL
ensures that only package base is loaded.
 Changing the set of default packages is normally used to reduce the set
for speed when scripting: in particular not using methods will
reduce the start-up time by a factor of up to two (and this is done by
Rscript).  But it can also be used to customize R, e.g.
for class use.
 
Next: Installing packages, Previous: Default packages, Up: Add-on packages   [Contents][Index] R packages are installed into libraries, which are
directories in the file system containing a subdirectory for each
package installed there.
 R comes with a single library, R_HOME/library which is
the value of the R object ‘.Library’ containing the standard and
recommended14  packages.
Both sites and users can create others and make use of them (or not) in
an R session.  At the lowest level ‘.libPaths()’ can be used to
add paths to the collection of libraries or to report the current
collection.
 R will automatically make use of a site-specific library
R_HOME/site-library if this exists (it does not in a
vanilla R installation).  This location can be overridden by
setting15 ‘.Library.site’ in
R_HOME/etc/Rprofile.site, or (not recommended) by setting
the

environment variable R_LIBS_SITE.  Like ‘.Library’, the
site libraries are always included by ‘.libPaths()’.
 Users can have one or more libraries, normally specified by the
environment variable R_LIBS_USER.  This has a default value (to
see it, use ‘Sys.getenv("R_LIBS_USER")’ within an R session),
but that is only used if the corresponding directory actually exists
(which by default it will not).
 Both R_LIBS_USER and R_LIBS_SITE can specify multiple
library paths, separated by colons (semicolons on Windows).
 
Next: Updating packages, Previous: Managing libraries, Up: Add-on packages   [Contents][Index] Packages may be distributed in source form or compiled binary form.
Installing source packages which contain C/C++/Fortran code requires
that compilers and related tools be installed.  Binary packages are
platform-specific and generally need no special tools to install, but
see the documentation for your platform for details.
 Note that you may need to specify implicitly or explicitly the library to
which the package is to be installed.  This is only an issue if you have
more than one library, of course.
 Ensure that the environment variable TMPDIR is either unset (and
/tmp exists and can be written in and executed from) or is the
absolute path to a valid temporary directory, not containing spaces.
 For most users it suffices to call
‘install.packages(pkgname)’ or its GUI equivalent if the
intention is to install a CRAN package and internet access is
available.16  On most systems ‘install.packages()’
will allow packages to be selected from a list box (typically with
several thousand items).
 To install packages from source on a Unix-alike use in a terminal
 The part ‘-l /path/to/library’ can be omitted, in which case the
first library of a normal R session is used (that shown by
.libPaths()[1]).
 There are a number of options available: use R CMD INSTALL --help
to see the current list.
 Alternatively, packages can be downloaded and installed from within
R.  First choose your nearest CRAN mirror using
chooseCRANmirror().  Then download and install packages
pkg1 and pkg2 by
 The essential dependencies of the specified packages will also be fetched.
Unless the library is specified (argument lib) the first library
in the library search path is used: if this is not writable, R will
ask the user (in an interactive session) if the default personal library
should be created, and if allowed to will install the packages there.
 If you want to fetch a package and all those it depends on (in any way)
that are not already installed, use e.g.
 install.packages can install a source package from a local
.tar.gz file (or a URL to such a file) by setting argument
repos to NULL: this will be selected automatically if the
name given is a single .tar.gz file.
 install.packages can look in several repositories, specified as a
character vector by the argument repos: these can include a
CRAN mirror, Bioconductor, R-forge, rforge.net,
local archives, local files, …).  Function
setRepositories() can select amongst those repositories that the
R installation is aware of.
 Naive users sometimes forget that as well as installing a package, they
have to use library to make its functionality available.
 
Next: OS X packages, Previous: Installing packages, Up: Installing packages   [Contents][Index] What install.packages does by default is different on Unix-alikes
(except OS X) and Windows.  On Unix-alikes it consults the list of
available source packages on CRAN (or other
repository/ies), downloads the latest version of the package sources,
and installs them (via R CMD INSTALL).  On Windows it looks (by
default) first at the list of binary versions of packages
available for your version of R and downloads the latest versions (if
any).  If no binary version is available or the source version is newer,
it will install the source versions of packages without compiled
C/C++/Fortran code, and offer to do so for those with, if make
is available (and this can be tuned by option
"install.packages.compile.from.source").
 On Windows install.packages can also install a binary package
from a local zip file (or the URL of such a file) by setting
argument repos to NULL.  Rgui.exe has a menu
Packages with a GUI interface to install.packages,
update.packages and library.
 Windows binary packages for R are distributed as a single binary
containing either or both architectures (32- and 64-bit).
 A few of the binary packages need other software to be installed on your
system: see for example
https://CRAN.R-project.org/bin/windows/contrib/3.2/@ReadMe.
Packages using Gtk+ (Cairo, RGtk2,
cairoDevice and those that depend on them) need the bin
directory of a bundled distribution of Gtk2 from
http://ftp.gnome.org/pub/gnome/binaries/win32/gtk+ or
http://ftp.gnome.org/pub/gnome/binaries/win64/gtk+ in
the path: it should work to have both 32- and 64-bit Gtk+ bin
directories in the path on a 64-bit version of R.
 R CMD INSTALL works in Windows to install source packages.
No additional tools are needed if the package does not contain
compiled code, and install.packages(type="source") will work
for such packages (and for those with compiled code if the tools (see
The Windows toolset) are on the path, and the variables
BINPREF and BINPREF64 are set properly; see the
discussion below).  We have seen occasional permission problems after
unpacking source packages on some systems: these have been
circumvented by setting the environment variable R_INSTALL_TAR
to ‘tar.exe’.

 If you have only a source package that is known to work with current
R and just want a binary Windows build of it, you could make use of
the building service offered at
http://win-builder.r-project.org/.
 For almost all packages R CMD INSTALL will attempt to install
both 32- and 64-bit builds of a package if run from a 32/64-bit install
of R.  It will report success if the installation of the architecture
of the running R succeeded, whether or not the other
architecture was successfully installed.  The exceptions are packages
with a non-empty configure.win script or which make use of
src/Makefile.win.  If configure.win does something
appropriate to both architectures use17 option
--force-biarch: otherwise R CMD INSTALL
--merge-multiarch can be applied to a source tarball to merge separate
32- and 64-bit installs.  (This can only be applied to a tarball, and
will only succeed if both installs succeed.)
 If you have a package without compiled code and no Windows-specific
help, you can zip up an installation on another OS and install from that
zip file on Windows.  However, such a package can be installed from the
sources on Windows without any additional tools.
 Packages with compiled code may need to have paths to the compilers
set explicitly, and there is provision to make use of a system-wide
library of installed external software. The compiler paths are set
using the make variables BINPREF (and in some cases
BINPREF64).  The library location is set using make
variable LOCAL_SOFT, to give an equivalent of /usr/local
on a Unix-alike.  All of these can be set in
src/gnuwin32/MkRules.local when R is built from sources (see
the comments in src/gnuwin32/MkRules.dist), or in
file18
etc/i386/Makeconf or etc/x64/Makeconf for an installed
version of R.  In the latter case only BINPREF is used, with
the 64 bit path used in etc/x64/Makeconf. The version used by
CRAN can be installed as described in Building from source.
 
Next: Customizing package compilation, Previous: Windows packages, Up: Installing packages   [Contents][Index] On OS X install.packages works as it does on other Unix-alike
systems, but there are additional types starting with mac.binary
(available for the CRAN distribution but not when compiling
from source: mac.binary.mavericks for a ‘Mavericks’ build with
"default" a synonym for the appropriate variant) which can be
passed to install.packages in order to download and install
binary packages from a suitable repository.  These OS X binary package
files have the extension ‘.tgz’.  The R.APP GUI provides menus
for installation of either binary or source packages, from
CRAN or local files.
 On R builds using binary packages, the default is type both:
this looks first at the list of binary packages available for your
version of R and installs the latest versions (if any).  If no binary
version is available or the source version is newer, it will install the
source versions of packages without compiled C/C++/Fortran code and offer
to do so for those with, if make is available.
 Note that most binary packages including compiled code are tied to a
particular series (e.g. R 3.2.x or 3.3.x) of R.
 Installing source packages which do not contain compiled code should
work with no additional tools. For others you will need the
‘Command Line Tools’ for Xcode and compilers which match those
used to build R: see OS X.
 Package rJava and those which depend on it need a Java runtime
installed and several packages need X11 installed, including those using
Tk.  See OS X and Java (OS X).
 Tcl/Tk extensions BWidget and Tktable are part of the
Tcl/Tk contained in the R installer.  These are required by a number
of CRAN and Bioconductor packages.
 A few of the binary packages need other software to be installed on your
system.  In particular packages using Gtk+ (RGtk2,
cairoDevice and those that depend on them) need the GTK
framework installed from https://r.research.att.com/libs/: the
appropriate version at the time of writing was
https://r.research.att.com/libs/GTK_2.24.17-X11.pkg
 The default compilers specified in
/Library/Frameworks/R.framework/Resources/etc/Makeconf depend on
the version of OS X under which R was installed, and are appropriate
for the latest version of the ‘Command Line Tools’ for that version of
OS X and the recommended version of Fortran (see OS X).  The
settings can be changed, either by editing that file or in a file such
as ~/.R/Makevars (see the next section).  Entries which may need
to be changed include ‘CC’, ‘CXX’, ‘FC’, ‘F77’,
‘FLIBS’ and the corresponding flags, and perhaps ‘CXXCPP’,
‘DYLIB_LD’, ‘MAIN_LD’, ‘SHLIB_CXXLD’, ‘SHLIB_FCLD’
and ‘SHLIB_LD’.
 So for example you could select clang for both C and C++ with
extensive checking by having in ~/.R/Makevars
 
Next: Multiple sub-architectures, Previous: OS X packages, Up: Installing packages   [Contents][Index] The R system and package-specific compilation flags can be overridden or
added to by setting the appropriate Make variables in the personal file
HOME/.R/Makevars-R_PLATFORM (but
HOME/.R/Makevars.win or HOME/.R/Makevars.win64
on Windows), or if that does not exist, HOME/.R/Makevars,
where ‘R_PLATFORM’ is the platform for which R was built, as
available in the platform component of the R variable
R.version.  An alternative personal file can be specified
via the environment variable R_MAKEVARS_USER.
 Package developers are encouraged to use this mechanism to enable a
reasonable amount of diagnostic messaging (“warnings”) when compiling,
such as e.g. -Wall -pedantic for tools from GCC, the Gnu
Compiler Collection.
 Note that this mechanism can also be used when it necessary to change
the optimization level for a particular package.  For example
 Another use is to override the settings in a binary installation of R.
For example, to use a different Fortran compiler on OS X
 (line split for legibility here).
 There is also provision for a site-wide Makevars.site file under
R_HOME/etc (in a sub-architecture-specific directory if
appropriate).  This is read immediately after Makeconf, and an
alternative file can be specified by environment variable
R_MAKEVARS_SITE.
 Note that these mechanisms do not work with packages which fail to pass
settings down to sub-makes, perhaps reading etc/Makeconf in
makefiles in subdirectories.  Fortunately such packages are unusual.
 
Next: Byte-compilation, Previous: Customizing package compilation, Up: Installing packages   [Contents][Index] When installing packages from their sources, there are some extra
considerations on installations which use sub-architectures.  These are
commonly used on Windows but can in principle be used on other
platforms.
 When a source package is installed by a build of R which supports
multiple sub-architectures, the normal installation process installs the
packages for all sub-architectures.  The exceptions are
 where there is an configure script, or a file src/Makefile.
 where there is a non-empty configure.win script, or a file
src/Makefile.win (with some exceptions where the package is known
to have an architecture-independent configure.win, or if
--force-biarch or field ‘Biarch’ in the DESCRIPTION
file is used to assert so).
 In those cases only the current architecture is installed.  Further
sub-architectures can be installed by
 using the path to R or R --arch to select the
additional sub-architecture.  There is also R CMD INSTALL
--merge-multiarch to build and merge the two architectures, starting
with a source tarball.
 
Next: External software, Previous: Multiple sub-architectures, Up: Installing packages   [Contents][Index] The base and recommended packages are byte-compiled by default.  Other
packages can be byte-compiled on installation by using R CMD
INSTALL with option --byte-compile or by
install.packages(type = "source", INSTALL_opts =
"--byte-compile").
 Not all contributed packages work correctly when byte-compiled.  For
most packages (especially those which make extensive use of compiled
code) the speed-up is small.  Unless a package is used frequently the
time spent in byte-compilation can outweigh the time saved in execution:
also byte-compilation can add substantially to the installed size of the
package.
 Byte-compilation can be controlled on a per-package basis by the
‘ByteCompile’ field in the DESCRIPTION file.
 
Previous: Byte-compilation, Up: Installing packages   [Contents][Index] Some R packages contain compiled code which links to external
software libraries.  Unless the external library is statically linked
(which is done as much as possible for binary packages on Windows and OS
X), the libraries have to be found when the package is loaded and not
just when it is installed.  How this should be done depends on the OS
(and in some cases the version).
 For Unix-alikes except OS X the primary mechanism is the ld.so
cache controlled by ldconfig: external dynamic libraries
recorded in that cache will be found.  Standard library locations will
be covered by the cache, and well-designed software will add its
locations (as for example openmpi does on Fedora).  The secondary
mechanism is to consult the environment variable LD_LIBRARY_PATH.
Now the R script controls that variable, and sets it to the
concatenation of R_LD_LIBRARY_PATH, R_JAVA_LD_LIBRARY_PATH
and the environment value of LD_LIBRARY_PATH.  The first two have
defaults which are normally set when R is installed (but can be
overridden in the environment) so LD_LIBRARY_PATH is the best
choice for a user to set.
 On OS X the primary mechanism is to embed the absolute path to dependent
dynamic libraries into an object when it is compiled.  Few R packages
arrange to do so, but it can be edited19 via install_name_tool —
that only deals with direct dependencies and those would also need to be
compiled to include the absolute paths of their dependencies.  If the
choice of absolute path is to be deferred to load time, how they are
resolved is described in man dyld: the role of
LD_LIBRARY_PATH is replaced on OS X by DYLD_LIBRARY_PATH and
latterly DYLD_FALLBACK_LIBRARY_PATH.  Running R CMD otool
-L on the package shared object will show where (if anywhere) its
dependencies are resolved. DYLD_FALLBACK_LIBRARY_PATH is preferred
(and it is that which is manipulated by the R script), but as from
10.11 (‘El Capitan’) the default behaviour had been changed for security
reasons to discard these environment variables when invoking a shell
script (and R is a shell script).  That makes the only portable
option to set R_LD_LIBRARY_PATH in the environment, something like
 The precise rules for where Windows looks for DLLs are complex and
depend on the version of Windows.  But for present purposes the main
solution is to put the directories containing the DLLs the package
links to (and any those DLLs link to) on the PATH.  64-bit
versions of Windows will ignore 32-bit DLLs from 64-bit R and
vice versa.
 The danger with any of the methods which involve setting environment
variables is of inadvertently masking a system library.  This is less
for DYLD_FALLBACK_LIBRARY_PATH and for appending to
PATH on Windows (as it should already contain the system library
paths).
 
Next: Removing packages, Previous: Installing packages, Up: Add-on packages   [Contents][Index] The command update.packages() is the simplest way to ensure that
all the packages on your system are up to date.  It downloads the list
of available packages and their current versions, compares it with those
installed and offers to fetch and install any that have later versions
on the repositories.
 An alternative interface to keeping packages up-to-date is provided by
the command packageStatus(), which returns an object with
information on all installed packages and packages available at multiple
repositories.  The print and summary methods give an
overview of installed and available packages, the upgrade method
offers to fetch and install the latest versions of outdated packages.
 One sometimes-useful additional piece of information that
packageStatus() returns is the status of a package, as
"ok", "upgrade" or "unavailable" (in the currently
selected repositories).  For example
 
Next: Setting up a package repository, Previous: Updating packages, Up: Add-on packages   [Contents][Index] Packages can be removed in a number of ways.  From a command prompt they
can be removed by
 From a running R process they can be removed by
 Finally, one can just remove the package directory from the library.
 
Next: Checking installed source packages, Previous: Removing packages, Up: Add-on packages   [Contents][Index] Utilities such as install.packages can be pointed at any
CRAN-style repository, and R users may want to set up their
own.  The ‘base’ of a repository is a URL such as
http://www.stats.ox.ac.uk/pub/RWin: this must be an URL scheme
that download.packages supports (which also includes
‘ftp://’ and ‘file://’ and (from R 3.3.0 and perhaps
earlier) ‘https://’).  Under that base URL there should be
directory trees for one or more of the following types of package
distributions:
 Each terminal directory must also contain a PACKAGES file.  This
can be a concatenation of the DESCRIPTION files of the packages
separated by blank lines, but only a few of the fields are needed.  The
simplest way to set up such a file is to use function
write_PACKAGES in the tools package, and its help explains
which fields are needed.  Optionally there can also be a
PACKAGES.gz file, a gzip-compressed version of
PACKAGES—as this will be downloaded in preference to
PACKAGES it should be included for large repositories.  (If you
have a mis-configured server that does not report correctly non-existent
files you may need PACKAGES.gz.)
 To add your repository to the list offered by setRepositories(),
see the help file for that function.
 Incomplete repositories are better specified via a
contriburl argument than via being set as a repository.
 A repository can contain subdirectories, when the descriptions in the
PACKAGES file of packages in subdirectories must include a line
of the form
 —once again write_PACKAGES is the simplest way to set this up.
 
Previous: Setting up a package repository, Up: Add-on packages   [Contents][Index] It can be convenient to run R CMD check on an installed
package, particularly on a platform which uses sub-architectures.  The
outline of how to do this is, with the source package in directory
pkg (or a tarball filename):
 Where sub-architectures are in use the R CMD check line can be
repeated with additional architectures by
 where --extra-arch selects only those checks which depend on
the installed code and not those which analyse the sources.  (If
multiple sub-architectures fail only because they need different
settings, e.g. environment variables, --no-multiarch may need
to be added to the INSTALL lines.)  On Unix-alikes the
architecture to run is selected by --arch: this can also be
used on Windows with R_HOME/bin/R.exe, but it is more usual
to select the path to the Rcmd.exe of the desired
architecture.
 So on Windows to install, check and package for distribution a source
package from a tarball which has been tested on another platform one
might use
 where one might want to run the second and third lines in a different
shell with different settings for environment variables and the path (to
find external software, notably for Gtk+).
 R CMD INSTALL can do a i386 install and then add the
x64 DLL from a single command by
 and --build can be added to zip up the installation.
 
Next: Choosing between 32- and 64-bit builds, Previous: Add-on packages, Up: Top   [Contents][Index] Internationalization refers to the process of enabling support
for many human languages, and localization to adapting to a
specific country and language.
 Current builds of R support all the character sets that the
underlying OS can handle.  These are interpreted according to the

current locale, a sufficiently complicated topic to merit a
separate section.  Note though that R has no built-in support for
right-to-left languages and bidirectional output, relying on the OS
services.  For example, how character vectors in UTF-8 containing both
English digits and Hebrew characters are printed is OS-dependent (and
perhaps locale-dependent).
 The other aspect of the internationalization is support for the
translation of messages.  This is enabled in almost all builds of R.
 
Next: Localization of messages, Previous: Internationalization, Up: Internationalization   [Contents][Index] A locale is a description of the local environment of the user,
including the preferred language, the encoding of characters, the
currency used and its conventions, and so on.  Aspects of the locale are
accessed by the R functions Sys.getlocale and
Sys.localeconv.
 The system of naming locales is OS-specific.  There is quite wide
agreement on schemes, but not on the details of their implementation.  A
locale needs to specify
 R is principally concerned with the first (for translations) and
third.  Note that the charset may be deducible from the language, as
some OSes offer only one charset per language.
 
Next: Locales under Windows, Previous: Locales, Up: Locales   [Contents][Index] Modern Linux uses the XPG20 locale specifications which have the form
‘en_GB’, ‘en_GB.UTF-8’, ‘aa_ER.UTF-8@saaho’,
‘de_AT.iso885915@euro’, the components being in the order listed
above.  (See man locale and locale -a for more
details.)  Similar schemes are used by most Unix-alikes: some (including
some distributions of Linux) use ‘.utf8’ rather than ‘.UTF-8’.
 Note that whereas UTF-8 locales are nowadays almost universally used,
locales such as ‘en_GB’ use 8-bit encodings for backwards
compatibility.
 
Next: Locales under OS X, Previous: Locales under Unix-alikes, Up: Locales   [Contents][Index] Windows also uses locales, but specified in a rather less concise way.
Most users will encounter locales only via drop-down menus, but more
information and lists can be found at
https://msdn.microsoft.com/en-us/library/hzz3tw78(v=vs.80)
(or if Microsoft moves it yet again, search for ‘Windows language
country strings’).
 It offers only one encoding per language.
 Some care is needed with Windows’ locale names.  For example,
chinese is Traditional Chinese and not Simplified Chinese as used
in most of the Chinese-speaking world.
 
Previous: Locales under Windows, Up: Locales   [Contents][Index] OS X supports locales in its own particular way, but the R GUI tries to
make this easier for users. See
https://developer.apple.com/documentation/MacOSX/Conceptual/BPInternational/
for how users can set their locales.  As with Windows, end users will
generally only see lists of languages/territories.  Users of R in a
terminal may need to set the locale to something like ‘en_GB.UTF-8’
if it defaults to ‘C’ (as it sometimes does when logging in
remotely and for batch jobs: note whether Terminal sets the
LANG environment variable is an (advanced) preference, but does so
by default).
 Internally OS X uses a form similar to Linux: the main difference from
other Unix-alikes is that where a character set is not specified it is
assumed to be UTF-8.
 
Previous: Locales, Up: Internationalization   [Contents][Index] The preferred language for messages is by default taken from the locale.
This can be overridden first by the setting of the environment variable




LANGUAGE and then21
by the environment variables LC_ALL, LC_MESSAGES and
LANG. (The last three are normally used to set the locale and so
should not be needed, but the first is only used to select the language
for messages.)  The code tries hard to map locales to languages, but on
some systems (notably Windows) the locale names needed for the
environment variable LC_ALL do not all correspond to XPG language
names and so LANGUAGE may need to be set.  (One example is
‘LC_ALL=es’ on Windows which sets the locale to Estonian and the
language to Spanish.)
 It is usually possible to change the language once R is running
via (not Windows) Sys.setlocale("LC_MESSAGES",
"new_locale"), or by setting an environment variable such as
LANGUAGE, provided22 the language you are changing to can be output in
the current character set.  But this is OS-specific, and has been known
to stop working on an OS upgrade.
 Messages are divided into domains, and translations may be
available for some or all messages in a domain.  R makes use of the
following domains.
 Dividing up the messages in this way allows R to be extensible: as
packages are loaded, their message translation catalogues can be loaded
too.
 R can be built without support for translations, but it is enabled by
default.
 R-level and C-level domains are subtly different, for example in the way
strings are canonicalized before being passed for translation. 
 Translations are looked for by domain according to the currently
specified language, as specifically as possible, so for example an
Austrian (‘de_AT’) translation catalogue will be used in preference
to a generic German one (‘de’) for an Austrian user.  However, if a
specific translation catalogue exists but does not contain a
translation, the less specific catalogues are consulted.  For example,
R has catalogues for ‘en_GB’ that translate the Americanisms
(e.g., ‘gray’) in the standard messages into English.23  Two other examples: there are catalogues
for ‘es’, which is Spanish as written in Spain and these will by
default also be used in Spanish-speaking Latin American countries, and
also for ‘pt_BR’, which are used for Brazilian locales but not for
locales specifying Portugal.
 Translations in the right language but the wrong charset are made use of

by on-the-fly re-encoding.  The LANGUAGE variable (only) can be a
colon-separated list, for example ‘se:de’, giving a set of
languages in decreasing order of preference.  One special value is
‘en@quot’, which can be used in a UTF-8 locale to have American
error messages with pairs of single quotes translated to Unicode directional
quotes.
 If no suitable translation catalogue is found or a particular message is
not translated in any suitable catalogue, ‘English’24 is used.
 See https://developer.r-project.org/Translations30.html for how to
prepare and install translation catalogues.
 
Next: The standalone Rmath library, Previous: Internationalization, Up: Top   [Contents][Index] Almost all current CPUs have both 32- and 64-bit sets of
instructions.  Most OSes running on such CPUs offer the choice
of building a 32-bit or a 64-bit version of R (and details are given
below under specific OSes).  For most a 32-bit version is the default,
but for some (e.g., ‘x86_64’ Linux and OS X >= 10.6)
64-bit is.
 All current versions of R use 32-bit integers and
ISO/IEC 6055925 double-precision reals, and so compute to
the same precision26 and with the same limits on the sizes of
numerical quantities.  The principal difference is in the size of the
pointers.
 64-bit builds have both advantages and disadvantages:
 R allocates memory for large objects as needed, and removes any
unused ones at garbage collection.  When the sizes of objects become an
appreciable fraction of the address limit, fragmentation of the address
space becomes an issue and there may be no hole available that is the
size requested.  This can cause more frequent garbage collection or the
inability to allocate large objects.  As a guide, this will become an
issue for 32-bit builds with objects more than 10% of the size of the
address space (around 300Mb) or when the total size of objects in use is
around one third (around 1Gb).
 So, for speed you may want to use a 32-bit build (especially on a
laptop), but to handle large datasets (and perhaps large files) a 64-bit
build.  You can often build both and install them in the same place:
See Sub-architectures.  (This is done for the Windows binary
distributions.)
 Even on 64-bit builds of R there are limits on the size of R
objects (see help("Memory-limits"), some of which stem from the
use of 32-bit integers (especially in FORTRAN code).  For example, the
dimensions of an array are limited to 2^{31} - 1.
 
Next: Essential and useful other programs under a Unix-alike, Previous: Choosing between 32- and 64-bit builds, Up: Top   [Contents][Index] The routines supporting the distribution and
special27 functions in R
and a few others are declared in C header file Rmath.h.  These
can be compiled into a standalone library for linking to other
applications.  (Note that they are not a separate library when R is
built, and the standalone version differs in several ways.)
 The makefiles and other sources needed are in directory
src/nmath/standalone, so the following instructions assume that
is the current working directory (in the build directory tree on a
Unix-alike if that is separate from the sources).
 Rmath.h contains ‘R_VERSION_STRING’, which is a character
string containing the current R version, for example "3.3.0".
 There is full access to R’s handling of NaN, Inf and
-Inf via special versions of the macros and functions
 and (extern) constants R_PosInf, R_NegInf and NA_REAL.
 There is no support for R’s notion of missing values, in particular
not for NA_INTEGER nor the distinction between NA and
NaN for doubles.
 A little care is needed to use the random-number routines. You will
need to supply the uniform random number generator 
 or use the one supplied (and with a shared library or DLL you may
have to use the one supplied, which is the Marsaglia-multicarry with
an entry point
 to set its seeds).
 The facilities to change the normal random number generator are
available through the constant N01_kind. This takes values
from the enumeration type
 (and ‘USER_NORM’ is not available).
 
Next: Windows standalone, Previous: The standalone Rmath library, Up: The standalone Rmath library   [Contents][Index] If R has not already been made in the directory tree,
configure must be run as described in the main build
instructions.
 Then (in src/nmath/standalone)
 will make standalone libraries libRmath.a and libRmath.so
(libRmath.dylib on OS X): ‘make static’ and ‘make
shared’ will create just one of them.
 To use the routines in your own C or C++ programs, include
 and link against ‘-lRmath’ (and ‘-lm’ if needed on your OS).
The example file test.c does nothing useful, but is provided to
test the process (via make test).  Note that you will probably
not be able to run it unless you add the directory containing

libRmath.so to the LD_LIBRARY_PATH environment variable
(libRmath.dylib, DYLD_FALLBACK_LIBRARY_PATH on OS X).
 The targets
 will (un)install the header Rmath.h and shared and static

libraries (if built).  Both prefix= and DESTDIR are
supported, together with more precise control as described for the main
build.
 ‘make install’ installs a file for pkg-config to use by
e.g.
 On some systems ‘make install-strip’ will install a stripped shared
library.
 
Previous: Unix-alike standalone, Up: The standalone Rmath library   [Contents][Index] You need to set up28 almost all the
tools to make R and then run (in a Unix-like shell)
 Alternatively, in a cmd.exe shell use
 This creates a static library libRmath.a and a DLL
Rmath.dll.  If you want an import library libRmath.dll.a
(you don’t need one), use
 To use the routines in your own C or C++ programs using MinGW-w64, include
 and link against ‘-lRmath’.  This will use the first found of
libRmath.dll.a, libRmath.a and Rmath.dll in that
order, so the result depends on which files are present.  You should be
able to force static or dynamic linking  via
 or by linking to explicit files (as in the ‘test’ target in
Makefile.win: this makes two executables, test.exe which
is dynamically linked, and test-static.exe, which is statically
linked).
 It is possible to link to Rmath.dll using other compilers, either
directly or via an import library: if you make a MinGW-w64 import library as
above, you will create a file Rmath.def which can be used
(possibly after editing) to create an import library for other systems
such as Visual C++.
 If you make use of dynamic linking you should use
 to ensure that the constants like NA_REAL are linked correctly.
(Auto-import will probably work with MinGW-w64, but it is better to be
sure. This is likely to also work with VC++, Borland and similar
compilers.)
 
Next: Configuration on a Unix-alike, Previous: The standalone Rmath library, Up: Top   [Contents][Index] This appendix gives details of programs you will need to build R on
Unix-like platforms, or which will be used by R if found by
configure.
 Remember that some package management systems (such as RPM and
Debian/Ubuntu’s) make a distinction between the user version of a
package and the development version.  The latter usually has the same
name but with the extension ‘-devel’ or ‘-dev’: you need both
versions installed.
 
Next: Useful libraries and programs, Previous: Essential and useful other programs under a Unix-alike, Up: Essential and useful other programs under a Unix-alike   [Contents][Index] You need a means of compiling C and FORTRAN 90 (see Using FORTRAN).  Your C compiler should be
ISO/IEC 6005929, POSIX 1003.1 and C99-compliant.30  R tries to choose suitable flags for
the C compilers it knows about, but you may have to set CC or
CFLAGS suitably.  For many versions of gcc with
glibc this means including
-std=gnu9931.  (Note that options essential to
run the compiler even for linking, such as those to set the
architecture, should be specified as part of CC rather than in
CFLAGS.)
 Unless you do not want to view graphs on-screen (or use OS X) you need
‘X11’ installed, including its headers and client libraries. For
recent Fedora/RedHat distributions it means (at least) RPMs
‘libX11’, ‘libX11-devel’, ‘libXt’ and ‘libXt-devel’.
On Debian/Ubuntu we recommend the meta-package ‘xorg-dev’.  If you
really do not want these you will need to explicitly configure R
without X11, using --with-x=no.
 The command-line editing (and command completion) depends on the
GNU readline library (including its headers): version
4.2 or later is needed for all the features to be enabled.  Otherwise
you will need to configure with --with-readline=no (or
equivalent).
 A suitably comprehensive iconv function is essential.  The R
usage requires iconv to be able to translate between
"latin1" and "UTF-8", to recognize "" (as the
current encoding) and "ASCII", and to translate to and from the
Unicode wide-character formats "UCS-[24][BL]E" — this is true
by default for glibc32 but not of most commercial Unixes.  However, you
can make use of GNU libiconv (as used on OS X: see
https://www.gnu.org/software/libiconv/).
 The OS needs to have enough support33 for wide-character
types: this is checked at configuration.  A small number of POSIX
functions34 are essential, and others35 will be used if available.
 Installations of zlib (version 1.2.5 or later), libbz2
(version 1.0.6 or later: called bzip2-libs/bzip2-devel or
libbz2-1.0/libbz2-dev by some Linux distributions),
liblzma36 version 5.0.3 or
later are required.
 PCRE37 (version 8.32 or later, although versions 8.10–8.31 will
be accepted with a deprecation warning) is required (or just its library
and headers if packaged separately).  PCRE must be built with UTF-8
support (not the default, and checked by configure) and
support for Unicode properties is assumed by some R packages.  JIT
support is desirable for the best performance: support for this and
Unicode properties can be checked at run-time by calling
pcre_config().  If building PCRE for use with R a suitable
configure command might be
 The --enable-jit flag is supported for most common CPUs.
 Library libcurl (version 7.28.0 or later) is required.
Information on libcurl is found from the curl-config
script: if that is missing or needs to be overridden38 there are macros to do so described in file
config.site.
 A tar program is needed to unpack the sources and packages
(including the recommended packages).  A version39 that can
automagically detect compressed archives is preferred for use with
untar(): the configure script looks for gtar and
gnutar before

tar – use environment variable TAR to override this.
 There need to be suitable versions of the tools grep and
sed: the problems are usually with old AT&T and BSD variants.
configure will try to find suitable versions (including
looking in /usr/xpg4/bin which is used on some commercial
Unixes).
 You will not be able to build most of the manuals unless you have
texi2any version 5.1 or later installed, and if not most of
the HTML manuals will be linked to a version on CRAN. To
make PDF versions of the manuals you will also need file
texinfo.tex installed (which is part of the GNU
texinfo distribution but is often made part of the TeX package
in re-distributions) as well as
texi2dvi.40
Further, the versions of texi2dvi and texinfo.tex need
to be compatible: we have seen problems with older TeX distributions.
 If you want to build from the R Subversion repository then
texi2any is highly recommended as it is used to create files
which are in the tarball but not stored in the Subversion repository.
 The PDF documentation (including doc/NEWS.pdf) and building
vignettes needs pdftex and pdflatex.  We require
LaTeX version 2005/12/01 or later (for UTF-8 support).
Building PDF package manuals (including the R reference manual) and
vignettes is sensitive to the version of the LaTeX package
hyperref and we recommend that the TeX distribution used is
kept up-to-date.  A number of standard LaTeX packages are required
(including url and some of the font packages such as times,
helvetic, ec and cm-super) and others such as
hyperref and inconsolata are desirable (and without them you
may need to change R’s defaults: see Making the manuals).  Note
that package hyperref (currently) requires packages
kvoptions, ltxcmds and refcount.  For distributions
based on TeX Live the simplest approach may be to install collections
collection-latex, collection-fontsrecommended,
collection-latexrecommended, collection-fontsextra and
collection-latexextra (assuming they are not installed by
default): Fedora uses names like texlive-collection-fontsextra and
Debian/Ubuntu like texlive-fonts-extra.
 The essential programs should be in your PATH at the time
configure is run: this will capture the full paths.
 
Next: Linear algebra, Previous: Essential programs and libraries, Up: Essential and useful other programs under a Unix-alike   [Contents][Index] The ability to use translated messages makes use of gettext and
most likely needs GNU gettext: you do need this to work
with new translations, but otherwise the version contained in the R
sources will be used if no suitable external gettext is found.
 The ‘modern’ version of the X11(), jpeg(), png()
and tiff() graphics devices uses the cairo and
(optionally) Pango libraries.  Cairo version 1.2.0 or later is
required.  Pango needs to be at least version 1.10, and 1.12 is the
earliest version we have tested.  (For Fedora users we believe the
pango-devel RPM and its dependencies suffice.)  R checks for
pkg-config, and uses that to check first that the
‘pangocairo’ package is installed (and if not, ‘cairo’) and if
additional flags are needed for the ‘cairo-xlib’ package, then if
suitable code can be compiled.  These tests will fail if
pkg-config is not installed41, and are likely to fail if cairo was built
statically (unusual).  Most systems with Gtk+ 2.8 or later
installed will have suitable libraries
 For the best font experience with these devices you need suitable fonts
installed: Linux users will want the urw-fonts package.  On
platforms which have it available, the msttcorefonts
package42 provides
TrueType versions of Monotype fonts such as Arial and Times New Roman.
Another useful set of fonts is the ‘liberation’ TrueType fonts available
at
https://fedorahosted.org/liberation-fonts/,43 which cover the Latin, Greek and Cyrillic alphabets
plus a fair range of signs.  These share metrics with Arial, Times New
Roman and Courier New, and contain fonts rather similar to the first two
(https://en.wikipedia.org/wiki/Liberation_fonts).  Then there
is the ‘Free UCS Outline Fonts’ project
(https://www.gnu.org/software/freefont/) which are
OpenType/TrueType fonts based on the URW fonts but with extended Unicode
coverage.  See the R help on X11 on selecting such fonts.
 The bitmapped graphics devices jpeg(), png() and
tiff() need the appropriate headers and libraries installed:
jpeg (version 6b or later, or libjpeg-turbo) or
libpng (version 1.2.7 or later) and zlib or libtiff
(any recent version – 3.9.[4567] and 4.0.[23] have been tested)
respectively.  They also need support for either X11 or
cairo (see above).  Should support for these devices not
be required or broken system libraries need to be avoided there are
configure options --without-libpng,
--without-jpeglib and --without-libtiff.  For most
system installations the TIFF libraries will require JPEG libraries to
be present and perhaps linked explicitly, so --without-jpeglib
may also disable the tiff() device.  The tiff() devices
only require a basic build of libtiff (not even JPEG support is
needed).  Recent versions allow several other libraries to be linked
into libtiff such as lzma, jbig and jpeg12,
and these may need also to be present.
 Option --with-system-tre is also available: it needs a recent
version of TRE. (The current sources are in the git repository
at https://github.com/laurikari/tre/, but at the time of writing
the resulting build will not pass its checks.).
 Library liblzma from xz-utils version 5.0.3 or later
(including 5.2.x) will be used if installed: the version in the R
sources can be selected instead by configuring with
--without-system-xz.  Systems differ in what they call the
package including this: e.g. on Fedora the library is in
‘xz-libs’ and the headers in ‘xz-devel’.
 An implementation of XDR is required, and the R sources
contain one which is likely to suffice (although a system version may
have higher performance).  XDR is part of RPC and
historically has been part of libc on a Unix-alike.  However some
builds of glibc hide it with the intention that the
TI-RPC library be used instead, in which case libtirpc
(and its development version) needs to be installed, and its headers
need to be on the C include path or in /usr/include/tirpc.
 Use of the X11 clipboard selection requires the Xmu headers and
libraries.  These are normally part of an X11 installation (e.g. the
Debian meta-package ‘xorg-dev’), but some distributions have split
this into smaller parts, so for example recent versions of Fedora
require the ‘libXmu’ and ‘libXmu-devel’ RPMs.
 Some systems (notably OS X and at least some FreeBSD systems) have
inadequate support for collation in multibyte locales.  It is possible
to replace the OS’s collation support by that from ICU (International
Components for Unicode, http://site.icu-project.org/), and this
provides much more precise control over collation on all systems.  ICU
is available as sources and as binary distributions for (at least) most
Linux distributions, Solaris, FreeBSD and AIX, usually as libicu
or icu4c.  It will be used by default where available: should a
very old or broken version of ICU be found this can be suppressed by
--without-ICU.
 The bitmap and dev2bitmap devices and function
embedFonts() use ghostscript
(http://www.ghostscript.com/).  This should either be in your
path when the command is run, or its full path specified by the
environment variable R_GSCMD at that time.

 
Next: Java support, Previous: Useful libraries and programs, Up: Useful libraries and programs   [Contents][Index] The tcltk package needs Tcl/Tk >= 8.4 installed: the sources are
available at https://www.tcl.tk/.  To specify the locations of the
Tcl/Tk files you may need the configuration options
 use Tcl/Tk, or specify its library directory
 specify location of tclConfig.sh
 specify location of tkConfig.sh
 or use the configure variables TCLTK_LIBS and
TCLTK_CPPFLAGS to specify the flags needed for linking against
the Tcl and Tk libraries and for finding the tcl.h and
tk.h headers, respectively.  If you have both 32- and 64-bit
versions of Tcl/Tk installed, specifying the paths to the correct config
files may be necessary to avoid confusion between them.
 Versions of Tcl/Tk up to 8.5.19 and 8.6.4 have been tested (including
most versions of 8.4.x, but not recently).
 Note that the tk.h header includes44 X11 headers, so you will need X11 and its
development files installed.
 
Next: Other compiled languages, Previous: Tcl/Tk, Up: Useful libraries and programs   [Contents][Index] The build process looks for Java support on the host system, and if it
finds it sets some settings which are useful for Java-using packages
(such as rJava and JavaGD).  This check can be
suppressed by configure option --disable-java.

Configure variable JAVA_HOME can be set to point to a specific
JRE/JDK, on the configure command line or in the environment.
 Principal amongst these settings are some library paths to the Java
libraries and JVM, which are stored in environment variable

R_JAVA_LD_LIBRARY_PATH in file R_HOME/etc/ldpaths (or
a sub-architecture-specific version).  A typical setting for
‘x86_64’ Linux is
 Unfortunately this depends on the exact version of the JRE/JDK
installed, and so may need updating if the Java installation is updated.
This can be done by running R CMD javareconf which updates
settings in both R_HOME/etc/Makeconf and
R_HOME/etc/ldpaths. See R CMD javareconf --help for
details: note that this needs to be done by the account owning the R
installation.
 Another way of overriding those settings is to set the environment variable

R_JAVA_LD_LIBRARY_PATH (before R is started, hence not in
~/.Renviron), which suffices to run already-installed
Java-using packages.  For example
 It may be possible to avoid this by specifying an invariant link as the
path when configuring. For example, on that system any of
 worked.
 
Previous: Java support, Up: Useful libraries and programs   [Contents][Index] Some add-on packages need a C++ compiler.  This is specified by the
configure variables CXX, CXXFLAGS and similar.
configure will normally find a suitable compiler.  However, in
most cases this will be a C++98 compiler, and as from R 3.1.0 it is
possible to specify an alternative compiler for use with C++11 by the
configure variables CXX1X, CXX1XSTD, CXX1XFLAGS and
similar.  Again, configure will normally find a suitable value
for CXX1XSTD if the compiler given by CXX is capable of
compiling C++11 code, but it is possible that a completely different
compiler will be needed.
 Other packages need full Fortran 90 (or later) support.  For source
files with extension .f90 or .f95, the compiler defined by
the macro FC is used by R CMD INSTALL.  This is found
when R is configured and is often the same as F77: note that
it is detected by the name of the command without a test that it can
actually compile Fortran 90 code.  Set the configure variable FC
to override this if necessary: variables FCFLAGS,
FCPICFLAGS, FCLIBS, SHLIB_FCLD and
SHLIB_FCLDFLAGS might also need to be set.
 See file config.site in the R source for more details about
these variables.
 
Previous: Useful libraries and programs, Up: Essential and useful other programs under a Unix-alike   [Contents][Index] 
Next: LAPACK, Previous: Linear algebra, Up: Linear algebra   [Contents][Index] The linear algebra routines in R can make use of enhanced
BLAS (Basic Linear Algebra Subprograms,
http://www.netlib.org/blas/faq.html) routines.  However,
these have to be explicitly requested at configure time: R provides
an internal BLAS which is well-tested and will be adequate for
most uses of R.
 You can specify a particular BLAS library via a value
for the configuration option --with-blas and not to use an
external BLAS library by --without-blas (the
default).  If --with-blas is given with no =, its value
is taken from the

environment variable BLAS_LIBS, set for example in
config.site.  If neither the option nor the environment variable
supply a value, a search is made for a suitable BLAS.  If the
value is not obviously a linker command (starting with a dash or giving
the path to a library), it is prefixed by ‘-l’, so
 is an instruction to link against ‘-lfoo’ to find an external
BLAS (which needs to be found both at link time and run time).
 The configure code checks that the external BLAS is complete
(it must include all double precision and double complex routines, as
well as LSAME), and appears to be usable.  However, an external
BLAS has to be usable from a shared object (so must contain
position-independent code), and that is not checked.
 Some enhanced BLASes are compiler-system-specific
(sunperf on Solaris45, libessl on IBM,
Accelerate on OS X).  The correct incantation for these is often
found via --with-blas with no value on the appropriate
platforms.
 Some of the external BLASes are multi-threaded.  One issue is
that R profiling (which uses the SIGPROF signal) may cause
problems, and you may want to disable profiling if you use a
multi-threaded BLAS.  Note that using a multi-threaded
BLAS can result in taking more CPU time and even
more elapsed time (occasionally dramatically so) than using a similar
single-threaded BLAS.  On a machine running other tasks, there
can be contention for CPU caches that reduces the effectiveness of the
optimization of cache use by a BLAS implementation.
 Note that under Unix (but not under Windows) if R is compiled against
a non-default BLAS and --enable-BLAS-shlib is
not used, then all BLAS-using packages must also be.
So if R is re-built to use an enhanced BLAS then packages
such as quantreg will need to be re-installed.
 R relies on ISO/IEC 60559 compliance of an
external BLAS.  This can be broken if for example the code
assumes that terms with a zero factor are always zero and do not need to
be computed—whereas x*0 can be NaN.  This is checked in
the test suite.
 External BLAS implementations often make less use of
extended-precision floating-point registers and will almost certainly
re-order computations.  This can result in less accuracy than using the
internal BLAS, and may result in different solutions, e.g.
different signs in SVD and eigendecompositions.
 The URIs for several of these BLAS are subject to frequent gratuitous
changes, so you will need to search for their current locations.
 
Next: ACML, Previous: BLAS, Up: BLAS   [Contents][Index] ATLAS (http://math-atlas.sourceforge.net/) is a “tuned”
BLAS that runs on a wide range of Unix-alike platforms.
Unfortunately it is built by default as a static library that on some
platforms cannot be used with shared objects such as are used in R
packages.  Be careful when using pre-built versions of ATLAS (they seem
to work on ‘ix86’ platforms, but not always on ‘x86_64’
ones).
 The usual way to specify ATLAS will be via
 if the libraries are in the library path, otherwise by
 For example, ‘x86_64’ Fedora needs
 For systems with multiple CPU cores it is possible to use a
multi-threaded version of ATLAS, by specifying
 Consult its installation guide for how to build ATLAS with
position-independent code, and as a shared library.
 
Next: Goto and OpenBLAS, Previous: ATLAS, Up: BLAS   [Contents][Index] For ‘x86_64’ processors46  under Linux there is the AMD Core Math Library (ACML).
For the gcc version we could use
 if the appropriate library directory (such as

/opt/acml5.1.0/gfortran64/lib) is in the LD_LIBRARY_PATH.
For other compilers, see the ACML documentation.  There is a
multithreaded Linux version of ACML available for recent versions of
gfortran.  To make use of this you will need something like
 (and you may need to arrange for the directory to be in ld.so
cache).
 See see Shared BLAS for an alternative (and in many ways preferable)
way to use ACML.
 The version last tested (5.1.0) failed the reg-BLAS.R test in its
handling of NAs.
 
Next: MKL, Previous: ACML, Up: BLAS   [Contents][Index] Dr Kazushige Goto wrote a tuned BLAS for several processors and
OSes,  which was  frozen in  mid-2010.  The  final version  is  known as
GotoBLAS2, and was re-released under a much less restrictive licence.
Once it is built and installed, it can be used by configuring R with
 See see Shared BLAS for an alternative (and in many ways preferable)
way to use it.
 OpenBLAS (http://www.openblas.net/) is a descendant
project with support for some later CPUs (e.g. Intel Sandy Bridge).
Once installed it can be used by something like
 or as a shared BLAS.
 
Next: Shared BLAS, Previous: Goto and OpenBLAS, Up: BLAS   [Contents][Index] For Intel processors (and perhaps others) and some distributions of
Linux, there is Intel’s Math Kernel Library.  You are strongly
encouraged to read the MKL User’s Guide, which is installed with the
library, before attempting to link to MKL.  This includes a ‘link line
advisor’ which will suggest appropriate incantations: its use is
recommended.  Or see
https://software.intel.com/en-us/articles/intel-mkl-link-line-advisor.
 There are also versions of MKL for OS X and Windows, but at the time
these were tried they did not work with the standard compilers used for
R on those platforms.
 The MKL interface has changed several times and may change again: the
following examples have been used with versions 10.3 to 11.3, for GCC
compilers on ‘x86_64’.
 To a sequential version of MKL we used
 The option --with-lapack is used since MKL contains a tuned
copy of LAPACK as well as BLAS (see LAPACK), although this
can be omitted.
 Threaded MKL may be used by replacing the line defining the variable
MKL by
 The default number of threads will be chosen by the OpenMP software, but
can be controlled by setting OMP_NUM_THREADS or
MKL_NUM_THREADS, and in recent versions seems to default to a
sensible value for sole use of the machine.
 It has been reported that
 worked with the Intel 2015.3 compilers on Centos 6.
 
Previous: MKL, Up: BLAS   [Contents][Index] The BLAS library will be used for many of the add-on packages
as well as for R itself.  This means that it is better to use a
shared/dynamic BLAS library, as most of a static library will
be compiled into the R executable and each BLAS-using
package.
 R offers the option of compiling the BLAS into a dynamic
library libRblas stored in R_HOME/lib and linking
both R itself and all the add-on packages against that library.
 This is the default on all platforms except AIX unless an external
BLAS is specified and found: for the latter it can be used by
specifying the option --enable-BLAS-shlib, and it can always be
disabled via --disable-BLAS-shlib.
 This has both advantages and disadvantages.
 Another option to change the BLAS in use is to symlink a
dynamic BLAS library (such as ACML or Goto’s) to
R_HOME/lib/libRblas.so.  For example, just
 will change the BLAS in use to multithreaded ACML.  A similar
link works for some versions of Goto BLAS, OpenBLAS and MKL
(provided the appropriate lib directory is in the run-time
library path or ld.so cache).
 
Next: Caveats, Previous: BLAS, Up: Linear algebra   [Contents][Index] Provision is made for using an external LAPACK library, principally to
cope with BLAS libraries which contain a copy of LAPACK (such
as sunperf on Solaris, Accelerate on OS X and ACML and MKL
on ‘ix86’/‘x86_64’ Linux).  At least LAPACK version 3.2
is required.  This can only be done if --with-blas has been used.
 However, the likely performance gains are thought to be small (and may
be negative), and the default is not to search for a suitable LAPACK
library, and this is definitely not recommended.  You can
specify a specific LAPACK library or a search for a generic library by
the configuration option --with-lapack.  The default for
--with-lapack is to check the BLAS library and then
look for an external library ‘-llapack’.  Sites searching for the
fastest possible linear algebra may want to build a LAPACK library using
the ATLAS-optimized subset of LAPACK.  To do so specify something like
 since the ATLAS subset of LAPACK depends on libcblas.  A value
for --with-lapack can be set via the environment
variable

LAPACK_LIBS, but this will only be used if --with-lapack
is specified (as the default value is no) and the BLAS library
does not contain LAPACK.
 Since ACML contains a full LAPACK, if selected as the BLAS it
can be used as the LAPACK via --with-lapack.
 If you do use --with-lapack, be aware of potential problems
with bugs in the LAPACK sources (or in the posted corrections to those
sources).  In particular, bugs in DGEEV and DGESDD have
resulted in error messages such as
 .  Other potential problems are incomplete versions of the libraries,
seen several times in Linux distributions over the years.
 Please do bear in mind that using --with-lapack is
‘definitely not recommended’: it is provided only
because it is necessary on some platforms and because some users want to
experiment with claimed performance improvements.  Reporting problems
where it is used unnecessarily will simply irritate the R helpers.
 Note too the comments about ISO/IEC 60559
compliance in the section of external BLAS: these apply
equally to an external LAPACK, and for example the Intel MKL
documentation says
 LAPACK routines assume that input matrices do not contain IEEE 754
special values such as INF or NaN values. Using these special values may
cause LAPACK to return unexpected results or become unstable.
 We rely on limited support in LAPACK for matrices with 2^{31} or
more elements: it is quite possible that an external LAPACK will not
have that support.
 If you have a pure FORTRAN 77 compiler which cannot compile LAPACK it
may be possible to use CLAPACK from
http://www.netlib.org/clapack/ by something like
 provided these were built with position-independent code and the calling
conventions for double complex function return values match those in the
BLAS used, so it may be simpler to use CLAPACK built to use CBLAS and
 
Previous: LAPACK, Up: Linear algebra   [Contents][Index] As with all libraries, you need to ensure that they and R were
compiled with compatible compilers and flags.  For example, this has
meant that on Sun Sparc using the native compilers the flag
-dalign is needed if sunperf is to be used.
 On some systems it has been necessary that an external
BLAS/LAPACK was built with the same FORTRAN compiler used to
build R.
 
Next: Platform notes, Previous: Essential and useful other programs under a Unix-alike, Up: Top   [Contents][Index] 
Next: Internationalization support, Previous: Configuration on a Unix-alike, Up: Configuration on a Unix-alike   [Contents][Index] configure has many options: running
 will give a list.  Probably the most important ones not covered
elsewhere are (defaults in brackets)
 use the X Window System [yes]
 X include files are in DIR
 X library files are in DIR
 use readline library (if available) [yes]
 attempt to compile support for Rprof() [yes]
 attempt to compile support for Rprofmem() and tracemem() [no]
 build R as a shared/dynamic library [no]
 build the BLAS as a shared/dynamic library [yes, except on AIX]
 You can use --without-foo or --disable-foo for the
negatives.
 You will want to use --disable-R-profiling if you are building
a profiled executable of R (e.g. with ‘-pg)’.
 Flag --enable-R-shlib causes the make process to build R as
a dynamic (shared) library, typically called libR.so, and link
the main R executable R.bin against that library.  This can
only be done if all the code (including system libraries) can be
compiled into a dynamic library, and there may be a
performance47 penalty.  So you probably
only want this if you will be using an application which embeds R.
Note that C code in packages installed on an R system linked with
--enable-R-shlib is linked against the dynamic library and so
such packages cannot be used from an R system built in the default
way.  Also, because packages are linked against R they are on some
OSes also linked against the dynamic libraries R itself is linked
against, and this can lead to symbol conflicts.
 For maximally effective use of valgrind, R should be
compiled with valgrind instrumentation. The configure option
is --with-valgrind-instrumentation=level, where
level is 0, 1 or 2.  (Level 0 is the default and does not add
anything.)  The system headers for valgrind can be requested
by option --with-system-valgrind-headers: they will be used if
present (on Linux they may be in a separate package such as
valgrind-devel).  Note though that there is no guarantee that the
code in R will be compatible with very old48 or future valgrind
headers.
 If you need to re-configure R with different options you may need to run
make clean or even make distclean before doing so.
 The configure script has other generic options added by
autoconf and which are not supported for R: in particular
building for one architecture on a different host is not possible.
 
Next: Configuration variables, Previous: Configuration options, Up: Configuration on a Unix-alike   [Contents][Index] Translation of messages is supported via GNU gettext
unless disabled by the configure option --disable-nls.
The configure report will show NLS as one of the
‘Additional capabilities’ if support has been compiled in, and running
in an English locale (but not the C locale) will include
 in the greeting on starting R.
 
Next: Setting the shell, Previous: Internationalization support, Up: Configuration on a Unix-alike   [Contents][Index] If you need or want to set certain configure variables to something
other than their default, you can do that by either editing the file
config.site (which documents many of the variables you might want
to set: others can be seen in file etc/Renviron.in) or on the
command line as
 If you are building in a directory different from the sources, there can
be copies of config.site in the source and the build directories,
and both will be read (in that order).  In addition, if there is a file
~/.R/config, it is read between the config.site files in
the source and the build directories.
 There is also a general autoconf mechanism for
config.site files, which are read before any of those mentioned
in the previous paragraph.  This looks first at a file specified by the

environment variable CONFIG_SITE, and if not is set at files such
as /usr/local/share/config.site and
/usr/local/etc/config.site in the area (exemplified by
/usr/local) where R would be installed.
 These variables are precious, implying that they do not have to
be exported to the environment, are kept in the cache even if not
specified on the command line, checked for consistency between two
configure runs (provided that caching is used), and are kept during
automatic reconfiguration as if having been passed as command line
arguments, even if no cache is used.
 See the variable output section of configure --help for a list of
all these variables.
 If you find you need to alter configure variables, it is worth noting
that some settings may be cached in the file config.cache, and it
is a good idea to remove that file (if it exists) before re-configuring.
Note that caching is turned off by default: use the command line
option --config-cache (or -C) to enable caching.
 
Next: Setting the browsers, Previous: Configuration variables, Up: Configuration variables   [Contents][Index] One common variable to change is R_PAPERSIZE, which defaults to
‘a4’, not ‘letter’.  (Valid values are ‘a4’,
‘letter’, ‘legal’ and ‘executive’.)
 This is used both when configuring R to set the default, and when
running R to override the default.  It is also used to set the
paper size when making PDF manuals.
 The configure default will most often be ‘a4’ if R_PAPERSIZE
is unset.  (If the (Debian Linux) program paperconf is found

or the environment variable PAPERSIZE is set, these are used to
produce the default.)
 
Next: Compilation flags, Previous: Setting paper size, Up: Configuration variables   [Contents][Index] Another precious variable is R_BROWSER, the default HTML
browser, which should take a value of an executable in the user’s path
or specify a full path.
 Its counterpart for PDF files is  R_PDFVIEWER.
 
Next: Making manuals, Previous: Setting the browsers, Up: Configuration variables   [Contents][Index] If you have libraries and header files, e.g., for GNU
readline, in non-system directories, use the variables LDFLAGS
(for libraries, using ‘-L’ flags to be passed to the linker) and
CPPFLAGS (for header files, using ‘-I’ flags to be passed to
the C/C++ preprocessors), respectively, to specify these locations.
These default to ‘-L/usr/local/lib’ (LDFLAGS,
‘-L/usr/local/lib64’ on most 64-bit Linux OSes) and
‘-I/usr/local/include’ (CPPFLAGS) to catch the most common
cases.  If libraries are still not found, then maybe your
compiler/linker does not support re-ordering of -L and
-l flags (this has been reported to be a problem on HP-UX with
the native cc).  In this case, use a different compiler (or a
front end shell script which does the re-ordering).
 These flags can also be used to build a faster-running version of R.
On most platforms using gcc, having ‘-O3’ in
CFLAGS and FFLAGS produces worthwhile performance gains
with gcc and gfortran, but may result in a less
reliable build (both segfaults and incorrect numeric computations have
been seen).  On systems using the GNU linker (especially those
using R as a shared library), it is likely that including
‘-Wl,-O1’ in LDFLAGS is worthwhile, and
‘'-Bdirect,--hash-style=both,-Wl,-O1'’ is recommended at
https://lwn.net/Articles/192624/.  Tuning compilation to a
specific CPU family (e.g. ‘-mtune=native’ for
gcc) can give worthwhile performance gains, especially on
older architectures such as ‘ix86’.
 
Previous: Compilation flags, Up: Configuration variables   [Contents][Index] The default settings for making the manuals are controlled by
R_RD4PDF and R_PAPERSIZE.
 
Next: Using make, Previous: Configuration variables, Up: Configuration on a Unix-alike   [Contents][Index] By default the shell scripts such as R will be ‘#!/bin/sh’
scripts (or using the SHELL chosen by configure).  This is
almost always satisfactory, but on a few systems /bin/sh is not a
Bourne shell or clone, and the shell to be used can be changed by
setting the configure variable R_SHELL to a suitable value (a full
path to a shell, e.g. /usr/local/bin/bash).
 
Next: Using FORTRAN, Previous: Setting the shell, Up: Configuration on a Unix-alike   [Contents][Index] To compile R, you will most likely find it easiest to use
GNU make, although the Sun make works on
Solaris.  The native make has been reported to fail on SGI
Irix 6.5 and Alpha/OSF1 (aka Tru64).
 To build in a separate directory you need a make that supports
the VPATH variable, for example GNU make and
Sun make.
 dmake has also been used. e.g, on Solaris 10.
 If you want to use a make by another name, for example if your
GNU make is called ‘gmake’, you need to set the
variable MAKE at configure time, for example
 
Next: Compile and load flags, Previous: Using make, Up: Configuration on a Unix-alike   [Contents][Index] To compile R, you need a FORTRAN compiler.  The default
is to search for
f95, fort, xlf95,
ifort, ifc, efc, pgf95
lf95, gfortran, ftn, g95,
f90, xlf90, pghpf, pgf90,
epcf90,
g77, f77, xlf, frt,
pgf77, cf77, fort77, fl32,
af77 (in that order)49, and use whichever is found first; if none is found,
R cannot be compiled.
However, if CC is gcc, the matching FORTRAN compiler
(g77 for gcc 3 and gfortran for
gcc 4) is used if available.
 The search mechanism can be changed using the configure variable
F77 which specifies the command that runs the FORTRAN 77
compiler.  If your FORTRAN compiler is in a non-standard location, you

should set the environment variable PATH accordingly before
running configure, or use the configure variable F77 to
specify its full path.
 If your FORTRAN libraries are in slightly peculiar places, you should

also look at LD_LIBRARY_PATH or your system’s equivalent to make
sure that all libraries are on this path.
 Note that only FORTRAN compilers which convert identifiers to lower case
are supported.
 You must set whatever compilation flags (if any) are needed to ensure
that FORTRAN integer is equivalent to a C int pointer and
FORTRAN double precision is equivalent to a C double
pointer.  This is checked during the configuration process.
 Some of the FORTRAN code makes use of COMPLEX*16 variables, which
is a Fortran 90 extension.  This is checked for at configure
time50, but you may need to avoid
compiler flags asserting FORTRAN 77 compliance.
 Compiling the version of LAPACK in the R sources also requires some
Fortran 90 extensions, but these are not needed if an external LAPACK is
used.
 It might be possible to use f2c, the FORTRAN-to-C converter
(http://www.netlib.org/f2c), via a script.  (An example script
is given in scripts/f77_f2c: this can be customized by setting



the environment variables F2C, F2CLIBS, CC and

CPP.)  You will need to ensure that the FORTRAN type
integer is translated to the C type int.  Normally
f2c.h contains ‘typedef long int integer;’, which will work
on a 32-bit platform but needs to be changed to ‘typedef int
integer;’ on a 64-bit platform.  If your compiler is not gcc
you will need to set

FPICFLAGS appropriately.  Also, the included LAPACK sources
contain constructs that f2c is unlikely to be able to process,
so you would need to use an external LAPACK library (such as CLAPACK
from http://www.netlib.org/clapack/).
 
Next: Maintainer mode, Previous: Using FORTRAN, Up: Configuration on a Unix-alike   [Contents][Index] A wide range of flags can be set in the file config.site or as
configure variables on the command line.  We have already mentioned
 header file search directory (-I) and any other miscellaneous
options for the C and C++ preprocessors and compilers
 path (-L), stripping (-s) and any other miscellaneous
options for the linker
 and others include
 debugging and optimization flags, C
 ditto, for compiling the main program
 for shared objects
 debugging and optimization flags, FORTRAN
 ditto for source files which need exact floating point behaviour
 ditto, for compiling the main program
 for shared objects
 additional flags for the main link
 additional flags for linking the shared objects
 the primary library directory, lib or lib64
 special flags for compiling C code to be turned into a shared object
 special flags for compiling Fortran code to be turned into a shared object
 special flags for compiling C++ code to be turned into a shared object
 special flags for compiling Fortran 95 code to be turned into a shared object
 defines to be used when compiling C code in R itself
 Library paths specified as -L/lib/path in LDFLAGS are

collected together and prepended to LD_LIBRARY_PATH (or your
system’s equivalent), so there should be no need for -R or
-rpath flags.
 Variables such as CPICFLAGS are determined where possible by
configure.  Some systems allows two types of PIC flags, for
example ‘-fpic’ and ‘-fPIC’, and if they differ the first
allows only a limited number of symbols in a shared object.  Since R
as a shared library has about 6200 symbols, if in doubt use the larger
version.
 To compile a profiling version of R, one might for example want to
use ‘MAIN_CFLAGS=-pg’, ‘MAIN_FFLAGS=-pg’,
‘MAIN_LDFLAGS=-pg’ on platforms where ‘-pg’ cannot be used
with position-independent code.
 Beware: it may be necessary to set CFLAGS and
FFLAGS in ways compatible with the libraries to be used: one
possible issue is the alignment of doubles, another is the way
structures are passed.
 On some platforms configure will select additional flags for
CFLAGS, CPPFLAGS, FFLAGS, CXXFLAGS and
LIBS in R_XTRA_CFLAGS (and so on).  These are for options
which are always required, for example to force IEC 60559
compliance.
 
Previous: Compile and load flags, Up: Configuration on a Unix-alike   [Contents][Index] There are several files that are part of the R sources but can be
re-generated from their own sources by configuring with option
--enable-maintainer-mode and then running make in the
build directory.  This requires other tools to be installed, discussed
in the rest of this section.
 File configure is created from configure.ac and the files
under m4 by autoconf and aclocal.  There is a
formal version requirement on autoconf of 2.62 or later, but
it is unlikely that anything other than the most recent versions have
been thoroughly tested.
 File src/include/config.h is created by autoheader.
 Grammar files *.y are converted to C sources by an implementation
of yacc, usually bison -y: these are found in
src/main and src/library/tools/src.  It is known that
earlier versions of bison generate code which reads (and in
some cases writes) outside array bounds: bison 2.6.1 was found
to be satisfactory.
 The ultimate sources for package compiler are in its noweb
directory.  To re-create the sources from
src/library/compiler/noweb/compiler.nw, the command
notangle is required.  This is likely to need to be installed
from the sources at https://www.cs.tufts.edu/~nr/noweb/ (and can
also be found on CTAN).  The package sources are only re-created even in
maintainer mode if src/library/compiler/noweb/compiler.nw has
been updated.
 It is likely that in future creating configure will need the GNU
‘autoconf archive’ installed.  This can be found at
https://www.gnu.org/software/autoconf-archive/ and as a package
(usually called autoconf-archive) in most packaged distributions,
for example Debian, Fedora, OpenCSW, Homebrew and MacPorts.
 
Next: The Windows toolset, Previous: Configuration on a Unix-alike, Up: Top   [Contents][Index] This section provides some notes on building R on different Unix-alike
platforms.  These notes are based on tests run on one or two systems in
each case with particular sets of compilers and support libraries.
Success in building R depends on the proper installation and functioning
of support software; your results may differ if you have other versions
of compilers and support libraries.
 Older versions of this manual (for R < 2.10.0) contain notes on
platforms such as HP-UX, IRIX and Alpha/OSF1 for which we have had no
recent reports.
 C macros to select particular platforms can be tricky to track down
(there is a fair amount of misinformation on the Web).  The Wiki
(currently) at http://sourceforge.net/p/predef/wiki/Home/ can be
helpful.  The R sources currently use
 
Next: Linux, Previous: Platform notes, Up: Platform notes   [Contents][Index] The ‘X11()’ graphics device is the one started automatically on
Unix-alikes when plotting.  As its name implies, it displays on a (local
or remote) X server, and relies on the services provided by the X
server.
 The ‘modern’ version of the ‘X11()’ device is based on ‘cairo’
graphics and (in most implementations) uses ‘fontconfig’ to pick and
render fonts.  This is done on the server, and although there can be
selection issues, they are more amenable than the issues with
‘X11()’ discussed in the rest of this section.
 When X11 was designed, most displays were around 75dpi, whereas today
they are of the order of 100dpi or more.  If you find that X11()
is reporting51 missing font sizes, especially larger ones, it is likely
that you are not using scalable fonts and have not installed the 100dpi
versions of the X11 fonts.  The names and details differ by system, but
will likely have something like Fedora’s
 and you need to ensure that the ‘-100dpi’ versions are installed
and on the X11 font path (check via xset -q).  The
‘X11()’ device does try to set a pointsize and not a pixel size:
laptop users may find the default setting of 12 too large (although very
frequently laptop screens are set to a fictitious dpi to appear like a
scaled-down desktop screen).
 More complicated problems can occur in non-Western-European locales, so
if you are using one, the first thing to check is that things work in
the C locale.  The likely issues are a failure to find any fonts
or glyphs being rendered incorrectly (often as a pair of ASCII
characters).  X11 works by being asked for a font specification and
coming up with its idea of a close match.  For text (as distinct from
the symbols used by plotmath), the specification is the first element of
the option "X11fonts" which defaults to
 If you are using a single-byte encoding, for example ISO 8859-2 in
Eastern Europe or KOI8-R in Russian, use xlsfonts to find an
appropriate family of fonts in your encoding (the last field in the
listing).  If you find none, it is likely that you need to install
further font packages, such as ‘xorg-x11-fonts-ISO8859-2-75dpi’ and
‘xorg-x11-fonts-cyrillic’ shown in the listing above.
 Multi-byte encodings (most commonly UTF-8) are even more complicated.
There are few fonts in ‘iso10646-1’, the Unicode encoding, and they
only contain a subset of the available glyphs (and are often fixed-width
designed for use in terminals).  In such locales fontsets are
used, made up of fonts encoded in other encodings.  If the locale you
are using has an entry in the ‘XLC_LOCALE’ directory (typically
/usr/share/X11/locale, it is likely that all you need to do is to
pick a suitable font specification that has fonts in the encodings
specified there.  If not, you may have to get hold of a suitable locale
entry for X11.  This may mean that, for example, Japanese text can be
displayed when running in ‘ja_JP.UTF-8’ but not when running in
‘en_GB.UTF-8’ on the same machine (although on some systems many
UTF-8 X11 locales are aliased to ‘en_US.UTF-8’ which covers several
character sets, e.g. ISO 8859-1 (Western European), JISX0208 (Kanji),
KSC5601 (Korean), GB2312 (Chinese Han) and JISX0201 (Kana)).
 On some systems scalable fonts are available covering a wide range of
glyphs.  One source is TrueType/OpenType fonts, and these can provide
high coverage.  Another is Type 1 fonts: the URW set of Type 1 fonts
provides standard typefaces such as Helvetica with a larger coverage of
Unicode glyphs than the standard X11 bitmaps, including Cyrillic.  These
are generally not part of the default install, and the X server may need
to be configured to use them.  They might be under the X11 fonts
directory or elsewhere, for example,
 
Next: OS X, Previous: X11 issues, Up: Platform notes   [Contents][Index] Linux is the main development platform for R, so compilation from the
sources is normally straightforward with the standard compilers.
 Remember that some package management systems (such as RPM and
deb) make a distinction between the user version of a package and the
developer version.  The latter usually has the same name but with the
extension ‘-devel’ or ‘-dev’: you need both versions
installed.  So please check the configure output to see if the
expected features are detected: if for example ‘readline’ is
missing add the developer package.  (On most systems you will also need
‘ncurses’ and its developer package, although these should be
dependencies of the ‘readline’ package(s).)  You should expect to
see in the configure summary
 When R has been installed from a binary distribution there are
sometimes problems with missing components such as the FORTRAN
compiler.  Searching the ‘R-help’ archives will normally reveal
what is needed.
 It seems that ‘ix86’ Linux accepts non-PIC code in shared
libraries, but this is not necessarily so on other platforms, in
particular on 64-bit CPUs such as ‘x86_64’.  So care
can be needed with BLAS libraries and when building R as a
shared library to ensure that position-independent code is used in any
static libraries (such as the Tcl/Tk libraries, libpng,
libjpeg and zlib) which might be linked against.
Fortunately these are normally built as shared libraries with the
exception of the ATLAS BLAS libraries.
 The default optimization settings chosen for CFLAGS etc are
conservative.  It is likely that using -mtune will result in
significant performance improvements on recent CPUs (especially for
‘ix86’): one possibility is to add -mtune=native for
the best possible performance on the machine on which R is being
installed: if the compilation is for a site-wide installation, it may
still be desirable to use something like
-mtume=core2.52 It is also possible to increase the
optimization levels to -O3: however for many versions of the
compilers this has caused problems in at least one CRAN
package.
 For platforms with both 64- and 32-bit support, it is likely that
 is appropriate since most (but not all) software installs its 64-bit
libraries in /usr/local/lib64.  To build a 32-bit version of R
on ‘x86_64’ with Fedora 21 we used
 Note the use of ‘LIBnn’: ‘x86_64’ Fedora installs its
64-bit software in /usr/lib64 and 32-bit software in
/usr/lib.  Linking will skip over inappropriate binaries, but for
example the 32-bit Tcl/Tk configure scripts are in /usr/lib.  It
may also be necessary to set the pkg-config path, e.g. by
 64-bit versions of Linux are built with support for files > 2Gb, and
32-bit versions will be if possible unless --disable-largefile
is specified.
 To build a 64-bit version of R on ‘ppc64’ (also known as
‘powerpc64’) with gcc 4.1.1, Ei-ji Nakama used
 the additional flags being needed to resolve problems linking against
libnmath.a and when linking R as a shared library.
 
Next: Intel compilers, Previous: Linux, Up: Linux   [Contents][Index] R has been built with Linux ‘ix86’ and ‘x86_64’ C and
C++ compilers (http://clang.llvm.org) based on the Clang
front-ends, invoked by CC=clang CXX=clang++, together with
gfortran.  These take very similar options to the
corresponding GCC compilers.
 This has to be used in conjunction with a Fortran compiler: the
configure code will remove -lgcc from FLIBS,
which is needed for some versions of gfortran.
 The current default for clang++ is to use the C++ runtime from
the installed g++.  Using the runtime from the libc++
project (http://libcxx.llvm.org/) has also been tested: for some
R packages only the variant using libcxxabi was successful.
 Most builds of clang have no OpenMP support.  Builds of
version 3.7.0 or later may.53
 
Next: Oracle Solaris Studio compilers, Previous: Clang, Up: Linux   [Contents][Index] Intel compilers have been used under ‘ix86’ and ‘x86_64’
Linux.  Brian Ripley used version 9.0 of the compilers for
‘x86_64’ on Fedora Core 5 with
 configure will add ‘-c99’ to CC for
C99-compliance.  This causes warnings with icc 10 and later, so
use CC="icc -std=c99" there.  The flag -wd188 suppresses
a large number of warnings about the enumeration type ‘Rboolean’.
Because the Intel C compiler sets ‘__GNUC__’ without complete
emulation of gcc, we suggest adding CPPFLAGS=-no-gcc.
 To maintain correct IEC 60559 arithmetic you most likely
need add flags to CFLAGS, FFLAGS and CXXFLAGS such
as -mp (shown above) or -fp-model precise -fp-model
source, depending on the compiler version.
 Others have reported success with versions 10.x and 11.x.  
% https://stat.ethz.ch/pipermail/r-devel/2015-September/071717.html
BjĂ¸rn-Helge Mevik reported success with version 2015.3 of the compilers,
using (for a SandyBridge CPU on Centos 6.x)
 
Previous: Intel compilers, Up: Linux   [Contents][Index] Brian Ripley tested the Sun Studio 12 compilers, since renamed to Oracle
Solaris Studio.  On ‘x86_64’ Linux with
 -m64 could be added, but was the default.  Do not use
-fast: see the warnings under Solaris. (The C++ options are
also explained under Solaris.)
 Others have found on at least some versions of ‘ix86’ Linux that
the configure flag --disable-largefile was needed (since
glob.h on that platform presumed gcc was being used).
 
Next: Solaris, Previous: Linux, Up: Platform notes   [Contents][Index] The instructions here are for ‘x86_64’ builds on 10.9
(Mavericks) or later.  In principle54 R can be
built for 10.6 to 10.8 but these has not been tested recently.
 To build R you need Apple’s ‘Command Line Tools’: these can be
(re-)installed by xcode-select --install.  (If you have a
fresh OS installation, running e.g. make in a terminal will
offer the installation of the command-line tools.  If you have installed
Xcode, this provides the command-line tools.  The tools will need to be
reinstalled when OS X is upgraded, as upgrading partially removes them.)
 You need readline55 and a Fortran compiler.  Those and other binary
components are available from https://r.research.att.com/libs:
you are likely to need pcre and xz (recent OS X provides
libraries but not headers for these).
 An X sub-system is required unless configuring using
--without-x: see https://xquartz.macosforge.org/.
 To use the quartz() graphics device you need to configure with
--with-aqua (which is the default): quartz() then
becomes the default device when running R at the console and X11
would only be used for the command-line-R data editor/viewer and one
version of Tcl/Tk.  (This needs an Objective-C compiler56 which can compile the source code of
quartz().)
 Use --without-aqua if you want a standard Unix-alike build:
apart from disabling quartz() and the ability to use the build
with R.APP, it also changes the default location of the personal
library (see ?.libPaths).
 Various compilers can be used.  The current CRAN distribution
of R is built using
 with clang and clang++ from the ‘Command Line
Tools’ and the Fortran compiler from
https://r.research.att.com/libs/gfortran-4.8.2-darwin13.tar.bz2.57
 Other builds of gfortran are available: see
https://gcc.gnu.org/wiki/GFortranBinaries and
http://coudert.name/software.html.  To use one of these with a
binary distribution of R you will probably need to specify the name
or path in a personal or site Makevars file (see Customizing package compilation).
 More recent and complete distributions of clang are usually
available from http://llvm.org/releases/.  In particular, these
include support for the ‘Address Sanitizer’ (not included by Apple until
Xcode 7) and for OpenMP58 in version 3.7.0 and later.
 Pre-compiled versions of many of the Useful libraries and programs
are available from https://r.research.att.com/libs/.  You will
most likely want at least jpeg and tiff.
pkg-config is not provided by Apple and used for many packages:
it will also be used if present when configuring the X11() and
bitmap devices.
 Support for cairo (without Pango) can be enabled in two
ways: both need pkg-config available.  XQuartz ships cairo
and its version will be selected if its pkg-config files are
first on the configuration path: for example by setting
 or appending that variable to the configure command.
Otherwise the binary libraries at
https://r.research.att.com/libs/ can be used: cairo,
fontconfig, freetype, pixman and
pkgconfig-system-stubs-darwin13.tar.gz are needed, plus
libpng for PNG support.
 The Accelerate library can be used via the configuration options
 to provide potentially higher-performance versions of the BLAS
and LAPACK routines.59
 Looking at the top of
/Library/Frameworks/R.framework/Resources/etc/Makeconf
will show the compilers and configuration options used for the
CRAN binary package for R: at the time of writing the
non-default options
 were used.
 Configure option --with-internal-tzcode is the default on OS X,
as the system implementation of time zones does not work correctly for
times before 1902 or after 2037 (despite using a 64-bit time_t).
 The TeX implementation used by the developers is MacTeX
(https://www.tug.org/mactex/): the full installation is about
4GB, but a smaller version is available at
https://www.tug.org/mactex/morepackages.html: you will need to
add some packages, e.g. for the 2015 version we needed to add
cm-super, helvetic, inconsolata and texinfo
which brought this to about 410MB.  ‘TeX Live Utility’ (available
via the MacTeX front page) provides a graphical means to manage
TeX packages.
 One OS X quirk is that the default path has /usr/local/bin after
/usr/bin, contrary to common practice on Unix-alikes.  This means
that if you install tools from the sources they will by default be
installed under /usr/local and not supersede the system
versions.
 If you upgrade your OS you should re-install the ‘Command Line Tools’
and may need to re-install XQuartz and Java (this has been needed for
some upgrades but not others).
 
Next: Tcl/Tk headers and libraries, Previous: OS X, Up: OS X   [Contents][Index] There are problems resulting from the new-to-El-Capitan restriction that
only Apple is allowed to install software under /usr: this
affects inter alia MacTeX and XQuartz.  For
MacTeX it is necessary to include /Library/TeX/texbin in
your path rather than /usr/texbin.  Upgrading will move
disallowed files to under /Library/SystemMigration/usr: this
includes /usr/X11R6, /usr/texbin, /usr/bin/R,
/usr/bin/Rscript but not the link /usr/X11.
 configure can be told to look for X11 in
XQuartz’s main location of /opt/X11, e.g. by
 although the linked versions under /usr/X11 will be found (if the
link is present).
 
Next: Java (OS X), Previous: El Capitan, Up: OS X   [Contents][Index] If you plan to use the tcltk package for R, you need to
install a distribution of Tcl/Tk.  There are two alternatives.  If you
use R.APP you will want to use X11-based Tcl/Tk (as used on other
Unix-alikes), which is installed as part of the CRAN binary for R.
This may need
 or
 Note that this requires a matching XQuartz installation.
 There is also a native (‘Aqua’) version of Tcl/Tk which produces widgets
in the native OS X style: this will not work with R.APP because of
conflicts over the OS X menu, but for those only using command-line R
this provides a much more intuitive interface to Tk for experienced Mac
users.  Most versions of OS X come with Aqua Tcl/Tk libraries, but these
are not at all recent versions of Tcl/Tk (8.5.9 in El Capitan, which is
not even the latest patched version in that series).  It is better to
install Tcl/Tk 8.6.x from the sources or a binary distribution from
https://www.activestate.com/activetcl/downloads.  Configure R
with
 (for the versions bundled with OS X, use paths starting with
/System/Library).
 If you need to find out which distribution of Tk is in use at run time,
use
 
Next: Frameworks, Previous: Tcl/Tk headers and libraries, Up: OS X   [Contents][Index] The situation with Java support on OS X is messy.60
 OS X no longer comes with an installed Java runtime (JRE), and an OS X
upgrade removes one if already installed: it is intended to be installed
at first use.  Check if a JRE is installed by running java
-version in a Terminal window: if Java is not installed this
should prompt you to install it.  You can also install directly the
latest Java from Oracle (currently from
http://www.oracle.com/technetwork/java/javase/downloads/index.html).
 You may need to install what Apple calls ‘legacy Java’61 to suppress pop-up messages
even if you have a current version installed.
 To see what compatible versions of Java are currently installed, run
/usr/libexec/java_home -V -a x86_64.  If needed, set the
environment variable JAVA_HOME to choose between these, both when
R is built from the sources and when R CMD javareconf is
run.
 Configuring and building R both looks for a JRE and for support for
compiling JNI programs (used by packages rJava and
JavaGD); the latter requires a JDK (Java SDK) and not just a
JRE.
 The build process tries to fathom out what JRE/JDK to use, but it may
need some help, e.g. by setting JAVA_HOME.  An Apple JRE can be
specified explicitly by something like
 The Oracle JDK can be specified explicitly by something like
 in config.site.
 Note that it is necessary to set the environment variable NOAWT
to 1 to install many of the Java-using packages.
 
Next: Building R.app, Previous: Java (OS X), Up: OS X   [Contents][Index] The CRAN build of R is installed as a framework, which is
selected by the option
 (This is intended to be used with an Apple toolchain: other compilers may
not support frameworks correctly. It was the default prior to R 3.3.0.)
 It is only needed if you want to build R for use with the R.APP
console, and implies --enable-R-shlib to build R as a
dynamic library.  This option configures R to be built and installed
as a framework called R.framework.  The default installation path
for R.framework is /Library/Frameworks but this can be
changed at configure time by specifying the flag
--enable-R-framework[=DIR] (or --prefix) or at
install time via
 Note that installation as a framework is non-standard (especially to a
non-standard location) and Unix utilities may not support it (e.g. the
pkg-config file libR.pc will be put somewhere unknown
to pkg-config).
 
Previous: Frameworks, Up: OS X   [Contents][Index] Note that building the R.APP GUI console is a separate project, using
Xcode.  Before compiling R.APP make sure the current version of R
is installed in /Library/Frameworks/R.framework and working at
the command-line (this can be a binary install).
 The current sources can be checked out by
 and built by loading the R.xcodeproj project (select the
R target and a suitable configuration), or from the command-line
by e.g.
 See also the INSTALL file in the checkout or directly at
https://svn.r-project.org/R-packages/trunk/Mac-GUI/INSTALL.
 R.APP does not need to be installed in any specific way. Building
R.APP results in the R.APP bundle which appears as one R icon. This
application bundle can be run anywhere and it is customary to place it
in the /Applications folder.
 
Next: AIX, Previous: OS X, Up: Platform notes   [Contents][Index] R has been built successfully on Solaris 10 (both Sparc and
‘x86’) using the (zero cost) Oracle Solaris Studio compilers:
there has been some success with
gcc 4/gfortran.  (Recent Sun machines are AMD
Opterons or Intel Xeons (‘amd64’) rather than ‘x86’, but
32-bit ‘x86’ executables are the default.)
 There have been few reports on Solaris 11, with no known extra issues.
Solaris 9 and earlier are now so old that it is unlikely that R is
still used with them, and they will not be considered here.
 The Solaris versions of several of the tools needed to build R
(e.g. make, ar and ld) are in
/usr/ccs/bin, so if using those tools ensure this is in your
path.  A version of the preferred GNU tar is (if
installed) in /usr/sfw/bin.  It may be necessary to avoid the
tools in /usr/ucb: POSIX-compliant versions of some tools can be
found in /usr/xpg4/bin and /usr/xpg6/bin.
 A large selection of Open Source software can be installed from
https://www.opencsw.org, by default installed under
/opt/csw.  Solaris 10 ships with bzlib version 1.0.6
(sufficient) but zlib version 1.2.3 (too old): OpenCSW has 1.2.8.
 You will need GNU libiconv and readline: the
Solaris version of iconv is not sufficiently powerful.
 The native make suffices to build R but a small number of
packages require GNU make (some without good reason
and without declaring it as ‘SystemRequirements’ in the
DESCRIPTION file).
 Some people have reported that the Solaris libintl needs to be
avoided, for example by using --disable-nls or
--with-included-gettext or using libintl from OpenCSW.
(On the other hand, there have been many successful installs which
automatically detected libintl from OpenCSW or selected the
included gettext.)
 The support for the C99 long double type on Sparc hardware uses
quad-precision arithmetic, and this is usually slow because it is done
by software emulation.  On such systems the configure option
--disable-long-double can be used for faster but less accurate
computations.
 The Solaris time-zone conversion services seem to be unreliable pre-1916
in Europe (when daylight-savings time was first introduced): most often
reporting in the non-existent DST variant.  Using configure
option --with-internal-tzcode is recommended, and required if
you find time-zone abbreviations being given odd values (as has been
seen on 64-bit builds without it).
 When using the Oracle compilers62 do not specify -fast, as this
disables IEEE arithmetic and make check will fail.
 It has been reported that some Solaris installations need
 on the configure command line or in file config.site;
however, there have been many successful installs without this.
 A little juggling of paths was needed to ensure GNU
libiconv (in /usr/local) was used rather than the Solaris
iconv:
 For a 64-bit target add -m64 to the compiler macros
and use something like LDFLAGS=-L/usr/local/lib/sparcv9 or
LDFLAGS=-L/usr/local/lib/amd64 as appropriate.
It will also be necessary to point pkg-config at the 64-bit
directories, e.g. one of
 and to specify a 64-bit Java VM by e.g.
 With Solaris Studio 12.[23] on Sparc, FCLIBS needs to be
 (and possibly other Fortran libraries, but this suffices for the
packages currently on CRAN).
 Currently ‘amd64’ and ‘sparcv9’ builds work
out-of-the-box with Sun Studio 12u1 but not Solaris Studio 12.2 and
12.3: libRblas.so and lapack.so are generated with code
that causes relocation errors (which is being linked in from the Fortran
libraries).  This means that building 64-bit R as a shared library
may be impossible with Solaris Studio >= 12.2.  For a standard build the
trick seems to be to manually set FLIBS to avoid the troublesome
libraries.  For example, on ‘amd64’ set in config.site
something like
 For 64-bit Sparc, set in config.site something like
 By default the Solaris Studio compilers do not by default conform to the C99
standard (appendix F 8.9) on the return values of functions such as
log: use -xlibmieee to ensure this.
 You can target specific Sparc architectures for (slightly) higher
performance: -xtarget=native (in CFLAGS etc) tunes the
compilation to the current machine.
 Using -xlibmil in CFLAGS and -xlibmil in
FFLAGS allows more system mathematical functions to be inlined.
 On ‘x86’ you will get marginally higher performance via
 but the use of -nofstore can be less numerically stable, and some
packages (notably mgcv on ‘x86’) failed to compile at
higher optimization levels with version 12.3.
 The Solaris Studio compilers provide several implementations of the
C++98 standard which select both the set of headers and a C++ runtime
library.  These are selected by the -library flag, which as it
is needed for both compiling and linking is best specified as part of
the compiler.  The examples above use ‘stlport4’, currently the
most modern of the options: the default (but still needed to be
specified as it is needed for linking) is ‘Cstd’: see
http://www.oracle.com/technetwork/server-storage/solaris/cmp-stlport-libcstd-142559.html.
Note though that most external Solaris C++ libraries will have been
built with ‘Cstd’ and so an R package using such libraries also
needs to be.  Occasionally the option -library=stlport4,Crun
has been needed.
 Several CRAN packages using C++ need the more liberal
interpretation given by adding
 The performance library sunperf is available for use with the
Solaris Studio compilers.  If selected as a BLAS, it must also
be selected as LAPACK via (for Solaris Studio 12.2 and later)
 This has often given test failures in the past, in several different
places.  At the time of writing it fails in tests/reg-BLAS.R, and on
some builds, including for ‘amd64’, it fails in
example(eigen).
 Parsing very complex R expressions needs a lot of stack space when
the Oracle compilers are used: several packages require the stack
increased to at least 20MB.
 
Previous: Solaris, Up: Solaris   [Contents][Index] If using gcc, ensure that the compiler was compiled for the
version of Solaris in use.  (This can be ascertained from gcc
-v.)  gcc makes modified versions of some header files, and
several reports of problems were due to using gcc compiled on
one version of Solaris on a later version.  
 The notes here are for gcc set up to use the Solaris linker:
it can also be set up to use GNU ld, but that has not been
tested.
 Compilation for a 32-bit Sparc target with gcc 4.9.2
needed
 and for a 64-bit Sparc target
 Note that paths such as /opt/csw/gcc4/lib/sparcv9 may need to
be in the

LD_LIBRARY_PATH during configuration.
 The compilation can be tuned to a particular cpu: the CRAN
check system uses -mtune=niagara2.
 Compilation for an ‘x86’ target with gcc 4.9.2
needed
 (-L/opt/csw/lib is needed since TeX Live was built using
32-bit gcc, and we need /opt/csw/lib in
R_LD_LIBRARY_PATH.)
 For an ‘amd64’ target with gcc 4.9.2
we used 
 
Next: FreeBSD, Previous: Solaris, Up: Platform notes   [Contents][Index] We no longer support AIX prior to 4.2, and configure will
throw an error on such systems.
 Ei-ji Nakama was able to build under AIX 5.2 on ‘powerpc’ with
GCC 4.0.3 in several configurations.  32-bit versions could be
configured with --without-iconv as well as
--enable-R-shlib.  For 64-bit versions he used
 and was also able to build with the IBM xlc and Hitachi
f90 compilers by
 Some systems have f95 as an IBM compiler that does not by
default accept FORTRAN 77.  It needs the flag -qfixed=72, or to
be invoked as xlf_r.
 The AIX native iconv does not support encodings ‘latin1’ nor
‘""’ and so cannot be used.  (As far as we know GNU
libiconv could be installed.)
 Fan Long reported success on AIX 5.3 using
 On one AIX 6.x system it was necessary to use R_SHELL to set the
default shell to be Bash rather than Zsh.
 Kurt Hornik and Stefan Theussl at WU (Wirtschaftsuniversität Wien)
successfully built R on a ‘powerpc’ (8-CPU Power6
system) running AIX 6.1, configuring with or without
--enable-R-shlib (Ei-ji Nakama’s support is gratefully
acknowledged).
 It helps to describe the WU build environment first.  A small part of
the software needed to build R and/or install packages is available
directly from the AIX Installation DVDs, e.g., Java 6 and  X11.
Additional open source software (OSS) is packaged for AIX in .rpm
files and available from both IBM’s “AIX Toolbox for Linux
Applications”
(http://www-03.ibm.com/systems/power/software/aix/linux/) and
http://www.oss4aix.org/download/.  The latter website typically
offers more recent versions of the available OSS.  All tools needed and
libraries downloaded from these repositories (e.g., GCC, Make,
libreadline, etc.) are typically installed to
/opt/freeware, hence corresponding executables are found in

/opt/freeware/bin which thus needs to be in PATH for using
these tools.  As on other Unix systems one needs GNU
libiconv as the AIX version of iconv is not sufficiently
powerful.  Additionally, for proper Unicode compatibility one should
install the corresponding package from the ICU project
(http://www.icu-project.org/download/), which offers pre-compiled
binaries for various platforms which in case of AIX can be installed via
unpacking the tarball to the root file system.  For full LaTeX
support one can install the TeX Live DVD distribution
(https://www.tug.org/texlive/): it is recommended to update the
distribution using the tlmgr update manager.  For 64-bit R builds
supporting Tcl/Tk this needs to installed from the sources as available
pre-compiled binaries supply only 32-bit shared objects.
 The recent WU testing was done using compilers from both the
GNU Compiler Collection (version 4.2.4) which is available
from one of the above OSS repositories, and the IBM C/C++ (XL C/C++
10.01) as well as FORTRAN (XL Fortran 12.01) compilers
(http://www14.software.ibm.com/webapp/download/byproduct.jsp#X).
 To compile for a 64-bit ‘powerpc’ (Power6 CPU) target
one can use
 for the GCC and
 for the IBM XL compilers.  For the latter, it is important to note that
the decision for generating 32-bit or 64-bit code is done by setting the

OBJECT_MODE environment variable appropriately (recommended) or
using an additional compiler flag (-q32 or -q64).  By
default the IBM XL compilers produce 32 bit code.  Thus, to build R with
64-bit support one needs to either export OBJECT_MODE=64 in the
environment or, alternatively, use the -q64 compiler options.
 It is strongly recommended to install Bash and use it as the configure
shell, e.g., via setting CONFIG_SHELL=/usr/bin/bash in the
environment, and to use GNU Make (e.g., via
(MAKE=/opt/freeware/bin/make).
 Further installation instructions to set up a proper R development
environment can be found in the “R on AIX” project on R-Forge
(https://R-Forge.R-project.org/projects/aix/).
 
Next: OpenBSD, Previous: AIX, Up: Platform notes   [Contents][Index] There have been few recent reports on FreeBSD.
 There is a ‘port’ at https://www.freebsd.org/ports/math.html, for
R 3.0.2 at the time of writing.  Davor Cubranic reported some success
on x86_64 FreeBSD 10.2 for R 3.2.2.
 Use of ICU for collation and the configure option
--with-internal-tzcode are desirable workarounds.
 
Next: Cygwin, Previous: FreeBSD, Up: Platform notes   [Contents][Index] Ingo Feinerer installed R version 3.2.2 on OpenBSD 5.8 arch
‘amd64’ (their name for ‘x86_64’).  Details of the build
(and patches applied) are at
http://cvsweb.openbsd.org/cgi-bin/cvsweb/ports/math/R/.
 
Next: New platforms, Previous: OpenBSD, Up: Platform notes   [Contents][Index] The 32-bit version has never worked well enough to pass R’s
make check, and residual support from earlier experiments was
removed in R 3.3.0.
 The 64-bit version is completely unsupported.
 
Previous: Cygwin, Up: Platform notes   [Contents][Index] There are a number of sources of problems when installing R on a new
hardware/OS platform.  These include
 Floating Point Arithmetic: R requires arithmetic compliant
with IEC 60559, also known as IEEE 754.
This mandates the use of plus and minus infinity and NaN (not a
number) as well as specific details of rounding.  Although almost all
current FPUs can support this, selecting such support can be a pain.
The problem is that there is no agreement on how to set the signalling
behaviour; Sun/Sparc, SGI/IRIX and ‘ix86’ Linux require no
special action, FreeBSD requires a call to (the macro)
fpsetmask(0) and OSF1 required that computation be done with a
-ieee_with_inexact flag etc.  On a new platform you must find
out the magic recipe and add some code to make it work.  This can often
be done via the file config.site which resides in the top level
directory.
 Beware of using high levels of optimization, at least initially.  On
many compilers these reduce the degree of compliance to the
IEEE model.  For example, using -fast on the Solaris
Studio compilers has caused R’s NaN to be set incorrectly, and
gcc’s -ffast-math and clang’s
-Ofast have given incorrect results.
 Shared Objects: There seems to be very little agreement
across platforms on what needs to be done to build shared objects.
there are many different combinations of flags for the compilers and
loaders.  GNU libtool cannot be used (yet), as it currently
does not fully support FORTRAN: one would need a shell wrapper for
this).  The technique we use is to first interrogate the X window system
about what it does (using xmkmf), and then override this in
situations where we know better (for tools from the GNU
Compiler Collection and/or platforms we know about).  This typically
works, but you may have to manually override the results.  Scanning the
manual entries for cc and ld usually reveals the
correct incantation.  Once you know the recipe you can modify the file
config.site (following the instructions therein) so that the
build will use these options.
 It seems that gcc 3.4.x and later on ‘ix86’ Linux
defeat attempts by the LAPACK code to avoid computations entirely in
extended-precision registers, so file src/modules/lapack/dlamc.f
may need to be compiled without optimization.  Set the configure
variable SAFE_FFLAGS to the flags to be used for this file.  If
configure detects GNU FORTRAN it adds flag
-ffloat-store to FFLAGS.  (Other settings are needed when
using icc on ‘ix86’ Linux, for example.  Using
-mpc64 is preferable on more recent GCC compilers.)
 If you do manage to get R running on a new platform please let us
know about it so we can modify the configuration procedures to include
that platform.
 If you are having trouble getting R to work on your platform please
feel free to use the ‘R-devel’ mailing list to ask questions.  We
have had a fair amount of practice at porting R to new platforms
...
 
Next: Function and variable index, Previous: Platform notes, Up: Top   [Contents][Index] If you want to build R or add-on packages from source in Windows, you
will need to collect, install and test an extensive set of tools.  See
https://CRAN.R-project.org/bin/windows/Rtools/ for the current
locations and other updates to these instructions.  (Most Windows users
will not need to build add-on packages from source; see Add-on packages for details.)
 We have found that the build process for R is quite sensitive to
the choice of tools: please follow our instructions exactly,
even to the choice of particular versions of the tools.63  The build process for add-on packages is somewhat more
forgiving, but we recommend using the exact toolset at first, and only
substituting other tools once you are familiar with the process.
 This appendix contains a lot of prescriptive comments.  They are
here as a result of bitter experience.  Please do not report problems to
the R mailing lists unless you have followed all the prescriptions.
 We have collected most of the necessary tools (unfortunately not all,
due to license or size limitations) into an executable installer named
Rtools*.exe, available from
https://CRAN.R-project.org/bin/windows/Rtools/. You should
download and run it, choosing the default “Package authoring
installation” to build add-on packages, or the “full installation” if
you intend to build R.
 You will need the following items to build R and packages.
See the subsections below for detailed descriptions.
 For installing simple source packages containing data or R source but
no compiled code, none of these are needed.
 A complete build of R including PDF manuals, and producing the
installer will also need the following:
 It is important to set your PATH properly.  The installer
Rtools*.exe optionally sets the path to components that it
installs.
 Your PATH may include . first, then the bin
directories of the tools, the compiler toolchain and LaTeX.  Do not
use filepaths containing spaces: you can always use the short forms
(found by dir /x at the Windows command line).  Network shares
(with paths starting \\) are not supported.
 For example for a 32-bit build, all on one line,
 It is essential that the directory containing the command line tools
comes first or second in the path: there are typically like-named
tools64 in other directories, and they will not
work. The ordering of the other directories is less important, but if in
doubt, use the order above.
 Our toolset contains copies of Cygwin DLLs that may conflict with other
ones on your system if both are in the path at once.  The normal
recommendation is to delete the older ones; however, at one time we
found our tools did not work with a newer version of the Cygwin DLLs, so
it may be safest not to have any other version of the Cygwin DLLs in your
path.
 
Next: The Inno Setup installer, Previous: The Windows toolset, Up: The Windows toolset   [Contents][Index] The ‘MiKTeX’ (http://www.miktex.org/) distribution of
LaTeX includes a suitable port of pdftex.  This can be set up
to install extra packages ‘on the fly’, which is the simplest way to use
it (and the default).  The ‘basic’ version of ‘MiKTeX’ almost
suffices: when last checked packages
 needed to be added (on the fly or via the ‘MiKTeX’ Package
Manager) to install R.  In any case ensure that the inconsolata
package is installed—you can check with the ‘MiKTeX’ Package
Manager.
 The Rtools*.exe installer does not include any version of
LaTeX.
 It is also possible to use the TeX Live distribution from
https://www.tug.org/texlive/.
 Please read Making the manuals about how to make fullrefman.pdf
and set the environment variable R_RD4PDF suitably; ensure you
have the required fonts installed or that ‘MiKTeX’ is set up to
install LaTeX packages on first use.
 
Next: The command line tools, Previous: LaTeX, Up: The Windows toolset   [Contents][Index] To make the installer package (R-3.3.1-win.exe) we
currently require the Unicode version of Inno Setup 5.3.7 or later from
http://jrsoftware.org/. This is not included in
Rtools*.exe.
 Copy file src/gnuwin32/MkRules.dist to
src/gnuwin32/MkRules.local and edit it to set ISDIR to the
location where Inno Setup was installed.
 
Next: The MinGW-w64 toolchain, Previous: The Inno Setup installer, Up: The Windows toolset   [Contents][Index] This item is installed by the Rtools*.exe installer.
 If you choose to install these yourself, you will need suitable versions
of at least basename, cat, cmp, comm,
cp, cut, date, diff, du, echo,
expr, gzip, ls, make, makeinfo,
mkdir, mv, rm, rsync, sed, sh,
sort, tar, texindex, touch and uniq;
we use those from the Cygwin distribution
(https://www.cygwin.com/) or compiled from the sources.  You will
also need zip and unzip from the Info-ZIP project
(http://www.info-zip.org/).  All of these tools are in
Rtools*.exe.
 Beware: ‘Native’ ports of make are not suitable
(including those called ‘MinGW make’ at the MinGW SourceForge site and
mingw32-make in some MinGW-w64 distributions).  There were
also problems with other versions of the Cygwin tools and DLLs.  To
avoid frustration, please use our tool set, and make sure it is at the
front of your path (including before the Windows system directories).
If you are using a Windows shell, type PATH at the prompt to find
out.
 You may need to set the environment variable CYGWIN to a value
including ‘nodosfilewarning’ to suppress messages about
Windows-style paths.
 
Next: Useful additional programs, Previous: The command line tools, Up: The Windows toolset   [Contents][Index] Technically you need more than just a compiler so the set of tools is
referred to as a ‘toolchain’.
 The preferred toolchain is part of Rtools*.exe: this uses a beta
version of gcc 4.6.3 and version 2.0.1 of the MinGW-w64
project’s runtime.
 This toolchain uses multilib: that is there is a single front-end
such as gcc.exe for each of the compilers and 32-bit (the
default) and 64-bit compilation are selected by the flags65  -m32 and -m64
respectively.  The tools are all 32-bit Windows executables and should
be able to run on any current version of Windows—however you do need a
64-bit version of Windows to build 64-bit R as the build process runs
R.
 To select a 32-bit or 64-bit build of R, set the options in
MkRules.local appropriately (following the comments in the file).
 Some external software libraries will need to be re-compiled under the
new toolchain: especially those providing a C++ interface.  Many of
those used by CRAN packages are available from
https://www.stats.ox.ac.uk/pub/Rtools/multilib/.  Users
developing packages with Rcpp need to ensure that they use a
version built with exactly the same toolchain as their package: the
recommendation is to build Rcpp from its sources yourself.
 There is support for OpenMP and pthreads in this toolchain.  As the
performance of OpenMP on Windows is poor for small tasks, it is not used
for R itself.
 
Previous: The MinGW-w64 toolchain, Up: The Windows toolset   [Contents][Index] The process of making the installer will make use of qpdf to
compact some of the package vignettes, if it is available.  Windows
binaries of qpdf are available from
http://sourceforge.net/projects/qpdf/files/.  Set the path
to the qpdf installation in file MkRules.local.
 Developers of packages will find some of the ‘goodies’ at
https://www.stats.ox.ac.uk/pub/Rtools/goodies useful.  
 There is a version of the file command that identifies the
type of files, and is used by Rcmd check if available.  The
binary distribution is included in Rtools*.exe.
 The file xzutils.zip contains the program xz which can
be used to (de)compress files with that form of compression.
 
Next: Concept index, Previous: The Windows toolset, Up: Top   [Contents][Index] 
Next: Environment variable index, Previous: Function and variable index, Up: Top   [Contents][Index] 
Previous: Concept index, Up: Top   [Contents][Index] e.g. GNU
tar version 1.15 or later, or that from the ‘libarchive’
(as used on OS X versions 10.6 and later) or ‘Heirloom Toolchest’
distributions. for some Subversion clients
‘http:’ may appear to work, but requires continual redirection. Most aspects will work with
paths containing spaces, but external software used by R, e.g.
texi2dvi version 4.8, may not. which use lib rather than
lib64 for their primary 64-bit library directories. Instructions on how to install the latest
version are at
https://www.ctan.org/tex-archive/fonts/inconsolata/. on a
Unix-alike, ‘inconsolata’ is omitted if not found by
configure. This will be needed if more than one
sub-architecture is to be installed. with possible values
‘i386’, ‘x64’, ‘32’ and ‘64’. mainly on RedHat and Fedora, whose
layout is described here. How to prepare such a directory is described in file
src/extra/tzone/Notes in the R sources. for example, -fopenmp, -xopenmp or
-qopenmp.  This includes for clang 3.7.x and the
Intel C compiler. Suitable distributions include
Strawberry Perl, http://strawberryperl.com/ and ActivePerl,
https://www.activestate.com/activeperl. The installer as puts links to R and
Rscript in /usr/bin (Mavericks, Yosemite) or
/usr/local/bin (El Capitan and later). If these are missing, you
can run directly the versions in
/Library/Frameworks/R.framework/Resources/. unless they were excluded in the build. its binding is locked once the startup files have been
read, so users cannot easily change it. If a proxy needs to be set, see
?download.file. for a small number of
CRAN packages where this is known to be safe and is needed by
the autobuilder this is the default.  Look at the source of
tools:::.install_packages for the list.  It can also be specified
in the package’s DESCRIPTION file. or by adding it in a file such as
etc/i386/Makevars.site, which does not exist by default. They need to have been
created using -headerpad_max_install_names, which is the
default for an R package. ‘X/Open Portability Guide’, which has
had several versions. On some systems setting
LC_ALL or LC_MESSAGES to ‘C’ disables LANGUAGE. If you try changing from French
to Russian except in a UTF-8 locale, you will most likely find messages
change to English. the
language written in England: some people living in the USA appropriate
this name for their language. with
Americanisms. also known as
IEEE 754 at least when storing quantities: the on-FPU
precision is allowed to vary e.g. Bessel, beta and gamma functions including copying MkRules.dist to
MkRule.local and selecting the architecture. also known as
IEEE 754 Note
that C11 compilers need not be C99-compliant: R requires support for
double complex and variable-length arrays which are optional in
C11 but is mandatory in C99. -std=c99 excludes POSIX
functionality, but config.h will turn on all GNU
extensions to include the POSIX functionality.  The default mode for GCC
5.1 and later is -std=gnu11. However, it is possible to break
the default behaviour of glibc by re-specifying the gconv
modules to be loaded. specifically, the C99
functionality of headers wchar.h and wctype.h, types
wctans_t and mbstate_t and functions mbrtowc,
mbstowcs, wcrtomb, wcscoll, wcstombs,
wctrans, wctype, and iswctype. including the opendir, readdir,
closedir, popen, stat, glob, access,
getcwd and chdir system calls, and either putenv or
setenv. such as
realpath, symlink. most often distributed as part of xz:
possible names in Linux distributions include
xz-devel/xz-libs and liblzma-dev. sometimes known as PCRE1, and not PCRE2 which started at
version 10.0. for
example to specify static linking with a build which has both shared and
static libraries. Such as
GNU tar 1.15 or later, bsdtar (from
https://github.com/libarchive/libarchive/, as used by FreeBSD and OS
X 10.6 and later) or tar from the Heirloom Toolchest
(http://heirloom.sourceforge.net/tools.html). texi2dvi is normally a shell
script.  Some versions (including that from texinfo 5.2 and 6.0)
need to be run under bash rather than a Bourne shell,
especially on Solaris. Some of the issues which have been observed with
broken versions of texi2dvi can be circumvented by setting the
environment variable R_TEXI2DVICMD to the value emulation. If necessary the path to
pkg-config can be specified by setting PKGCONF in
config.site, on the configure command line or in the
environment. also known as ttf-mscorefonts-installer in the
Debian/Ubuntu world: see also
https://en.wikipedia.org/wiki/Core_fonts_for_the_Web. ttf-liberation
in Debian/Ubuntu. This is true even for
the ‘Aqua’ version of Tk on OS X, but distributions of that include a
copy of the X11 files needed. Using the Oracle Solaris Studio
cc and f95 compilers and ‘i686’ for earlier
versions. We have measured 15–20% on ‘i686’ Linux
and around 10% on ‘x86_64’ Linux. We believe that
versions 3.4.0 to 3.10.1 are compatible. On HP-UX fort77 is the
POSIX compliant FORTRAN compiler, and comes after
g77. as well as its equivalence to the Rcomplex
structure defined in R_ext/Complex.h. for example, X11 font at size 14 could not
be loaded. or -mtune=corei7 for Intel Core
i3/15/17 with gcc >= 4.6.0. This also needs the OpenMP runtime,
which is usually distributed separately, e.g. at
http://llvm.org/releases. It will be necessary to
install later versions of software such as libcurl. Apple provides a partial emulation of
GNU readline 4.2 based on the NetBSD editline library.  That is
not recommended but for the time being R’s installation scripts will
make use of it. These
days that is defined by Apple’s implementation of clang, so it is
strongly recommended to use that. This
is a tarball which needs to be unpacked in the Terminal by e.g.
sudo tar -zxf gfortran-4.8.2-darwin13.tar.bz2 -C /. It does
not run on Core 2 Duo Macs. This also needs the OpenMP runtime,
which is distributed separately at that site. It is reported that for some non-Apple
toolchains CPPFLAGS needed to contain -D__ACCELERATE__. For more
details see http://www.macstrategy.com/article.php?3. e.g.
Java For OS X 2015-001 from
https://support.apple.com/kb/DL1572. including gcc for
Sparc from Oracle. For
example, the Cygwin version of make 3.81 fails to work
correctly. such as sort, find and perhaps
make. these
flags apply to the compilers: some of the tools use different flags.
32-bit builds are the default. How to contribute Edit this page With more than 10 years experience programming in R, I’ve had the luxury of being able to spend a lot of time trying to figure out and understand how the language works. This book is my attempt to pass on what I’ve learned so that you can quickly become an effective R programmer. Reading it will help you avoid the mistakes I’ve made and dead ends I’ve gone down, and will teach you useful tools, techniques, and idioms that can help you to attack many types of problems. In the process, I hope to show that, despite its frustrating quirks, R is, at its heart, an elegant and beautiful language, well tailored for data analysis and statistics. If you are new to R, you might wonder what makes learning such a quirky language worthwhile. To me, some of the best features are: It’s free, open source, and available on every major platform. As a result, if you do your analysis in R, anyone can easily replicate it. A massive set of packages for statistical modelling, machine learning, visualisation, and importing and manipulating data. Whatever model or graphic you’re trying to do, chances are that someone has already tried to do it. At a minimum, you can learn from their efforts. Cutting edge tools. Researchers in statistics and machine learning will often publish an R package to accompany their articles. This means immediate access to the very latest statistical techniques and implementations. Deep-seated language support for data analysis. This includes features likes missing values, data frames, and subsetting. A fantastic community. It is easy to get help from experts on the R-help mailing list, stackoverflow, or subject-specific mailing lists like R-SIG-mixed-models or ggplot2. You can also connect with other R learners via twitter, linkedin, and through many local user groups. Powerful tools for communicating your results. R packages make it easy to produce html or pdf reports, or create interactive websites. A strong foundation in functional programming. The ideas of functional programming are well suited to solving many of the challenges of data analysis. R provides a powerful and flexible toolkit which allows you to write concise yet descriptive code. An IDE tailored to the needs of interactive data analysis and statistical programming. Powerful metaprogramming facilities. R is not just a programming language, it is also an environment for interactive data analysis. Its metaprogramming capabilities allow you to write magically succinct and concise functions and provide an excellent environment for designing domain-specific languages. Designed to connect to high-performance programming languages like C, Fortran, and C++. Of course, R is not perfect. R’s biggest challenge is that most R users are not programmers. This means that: Much of the R code you’ll see in the wild is written in haste to solve a pressing problem. As a result, code is not very elegant, fast, or easy to understand. Most users do not revise their code to address these shortcomings. Compared to other programming languages, the R community tends to be more focussed on results instead of processes. Knowledge of software engineering best practices is patchy: for instance, not enough R programmers use source code control or automated testing. Metaprogramming is a double-edged sword. Too many R functions use tricks to reduce the amount of typing at the cost of making code that is hard to understand and that can fail in unexpected ways. Inconsistency is rife across contributed packages, even within base R. You are confronted with over 20 years of evolution every time you use R. Learning R can be tough because there are many special cases to remember. R is not a particularly fast programming language, and poorly written R code can be terribly slow. R is also a profligate user of memory. Personally, I think these challenges create a great opportunity for experienced programmers to have a profound positive impact on R and the R community. R users do care about writing high quality code, particularly for reproducible research, but they don’t yet have the skills to do so. I hope this book will not only help more R users to become R programmers but also encourage programmers from other languages to contribute to R. This book is aimed at two complementary audiences: Intermediate R programmers who want to dive deeper into R and learn new strategies for solving diverse problems. Programmers from other languages who are learning R and want to understand why R works the way it does. To get the most out of this book, you’ll need to have written a decent amount of code in R or another programming language. You might not know all the details, but you should be familiar with how functions work in R and although you may currently struggle to use them effectively, you should be familiar with the apply family (like apply() and lapply()). This book describes the skills I think an advanced R programmer should have: the ability to produce quality code that can be used in a wide variety of circumstances. After reading this book, you will: Be familiar with the fundamentals of R. You will understand complex data types and the best ways to perform operations on them. You will have a deep understanding of how functions work, and be able to recognise and use the four object systems in R. Understand what functional programming means, and why it is a useful tool for data analysis. You’ll be able to quickly learn how to use existing tools, and have the knowledge to create your own functional tools when needed. Appreciate the double-edged sword of metaprogramming. You’ll be able to create functions that use non-standard evaluation in a principled way, saving typing and creating elegant code to express important operations. You’ll also understand the dangers of metaprogramming and why you should be careful about its use. Have a good intuition for which operations in R are slow or use a lot of memory. You’ll know how to use profiling to pinpoint performance bottlenecks, and you’ll know enough C++ to convert slow R functions to fast C++ equivalents. Be comfortable reading and understanding the majority of R code. You’ll recognise common idioms (even if you wouldn’t use them yourself) and be able to critique others’ code. There are two meta-techniques that are tremendously helpful for improving your skills as an R programmer: reading source code and adopting a scientific mindset. Reading source code is important because it will help you write better code. A great place to start developing this skill is to look at the source code of the functions and packages you use most often. You’ll find things that are worth emulating in your own code and you’ll develop a sense of taste for what makes good R code. You will also see things that you don’t like, either because its virtues are not obvious or it offends your sensibilities. Such code is nonetheless valuable, because it helps make concrete your opinions on good and bad code. A scientific mindset is extremely helpful when learning R. If you don’t understand how something works, develop a hypothesis, design some experiments, run them, and record the results. This exercise is extremely useful since if you can’t figure something out and need to get help, you can easily show others what you tried. Also, when you learn the right answer, you’ll be mentally prepared to update your world view. When I clearly describe a problem to someone else (the art of creating a reproducible example), I often figure out the solution myself. R is still a relatively young language, and the resources to help you understand it are still maturing. In my personal journey to understand R, I’ve found it particularly helpful to use resources from other programming languages. R has aspects of both functional and object-oriented (OO) programming languages. Learning how these concepts are expressed in R will help you leverage your existing knowledge of other programming languages, and will help you identify areas where you can improve. To understand why R’s object systems work the way they do, I found The Structure and Interpretation of Computer Programs (SICP) by Harold Abelson and Gerald Jay Sussman, particularly helpful. It’s a concise but deep book. After reading it, I felt for the first time that I could actually design my own object-oriented system. The book was my first introduction to the generic function style of OO common in R. It helped me understand its strengths and weaknesses. SICP also talks a lot about functional programming, and how to create simple functions which become powerful when combined. To understand the trade-offs that R has made compared to other programming languages, I found Concepts, Techniques and Models of Computer Programming by Peter van Roy and Sef Haridi extremely helpful. It helped me understand that R’s copy-on-modify semantics make it substantially easier to reason about code, and that while its current implementation is not particularly efficient, it is a solvable problem. If you want to learn to be a better programmer, there’s no place better to turn than The Pragmatic Programmer by Andrew Hunt and David Thomas. This book is language agnostic, and provides great advice for how to be a better programmer. Currently, there are two main venues to get help when you’re stuck and can’t figure out what’s causing the problem: stackoverflow and the R-help mailing list. You can get fantastic help in both venues, but they do have their own cultures and expectations. It’s usually a good idea to spend a little time lurking, learning about community expectations, before you put up your first post.  Some good general advice: Make sure you have the latest version of R and of the package (or packages) you are having problems with. It may be that your problem is the result of a recently fixed bug. Spend some time creating a reproducible example. This is often a useful process in its own right, because in the course of making the problem reproducible you often figure out what’s causing the problem. Look for related problems before posting. If someone has already asked your question and it has been answered, it’s much faster for everyone if you use the existing answer. I would like to thank the tireless contributors to R-help and, more recently, stackoverflow. There are too many to name individually, but I’d particularly like to thank Luke Tierney, John Chambers, Dirk Eddelbuettel, JJ Allaire and Brian Ripley for generously giving their time and correcting my countless misunderstandings. This book was written in the open, and chapters were advertised on twitter when complete. It is truly a community effort: many people read drafts, fixed typos, suggested improvements, and contributed content. Without those contributors, the book wouldn’t be nearly as good as it is, and I’m deeply grateful for their help. Special thanks go to Peter Li, who read the book from cover-to-cover and provided many fixes. Other outstanding contributors were Aaron Schumacher, @crtahlin, Lingbing Feng, @juancentro, and @johnbaums.  Thanks go to all contributers in alphabetical order: Aaron Schumacher, @aaronwolen, Aaron Wolen, @absolutelyNoWarranty, Adam Hunt, @agrabovsky, @ajdm, @alexbbrown, @alko989, @allegretto, @AmeliaMN, @andrewla, Andy Teucher, Anthony Damico, Anton Antonov, @aranlunzer, @arilamstein, @avilella, @baptiste, @blindjesse, @blmoore, @bnjmn, Brandon Hurr, @BrianDiggs, @Bryce, @Carson, @cdrv, Ching Boon, @chiphogg, Christopher Brown, @christophergandrud, C. Jason Liang, Clay Ford, @cornelius1729, @cplouffe, Craig Citro, @crossfitAL, @crowding, @crtahlin, Crt Ahlin, @cscheid, @csgillespie, @cusanovich, @cwarden, @cwickham, Daniel Lee, @darrkj, @Dasonk, David Hajage, David LeBauer, @dchudz, dennis feehan, @dfeehan, Dirk Eddelbuettel, @dkahle, @dlebauer, @dlschweizer, @dmontaner, @dougmitarotonda, @dpatschke, @duncandonutz, @EdFineOKL, @EDiLD, @eipi10, @elegrand, @EmilRehnberg, Eric C. Anderson, @etb, @fabian-s, Facundo Muñoz, @flammy0530, @fpepin, Frank Farach, @freezby, @fyears, @garrettgman, Garrett Grolemund, @gavinsimpson, @gggtest, Gökçen Eraslan, Gregg Whitworth, @gregorp, @gsee, @gsk3, @gthb, @hassaad85, @i, Iain Dillingham, @ijlyttle, Ilan Man, @imanuelcostigan, @initdch, Jason Asher, @jasondavies, Jason Knight, @jastingo, @jcborras, Jeff Allen, @jeharmse, @jentjr, @JestonBlu, @JimInNashville, @jinlong25, JJ Allaire, Jochen Van de Velde, Johann Hibschman, @johnbaums, John Blischak, @johnjosephhorton, John Verzani, Joris Muller, Joseph Casillas, @juancentro, @kdauria, @kenahoo, @kent37, Kevin Markham, Kevin Ushey, @kforner, Kirill Müller, Kun Ren, Laurent Gatto, @Lawrence-Liu, @ldfmrails, @lgatto, @liangcj, Lingbing Feng, @lynaghk, Maarten Kruijver, Mamoun Benghezal, @mannyishere, @mattbaggott, Matthew Grogan, @mattmalin, Matt Pettis, @michaelbach, Michael Kane, @mjsduncan, @Mullefa, @myqlarson, Nacho Caballero, Nick Carchedi, @nstjhp, @ogennadi, Oliver Keyes, @otepoti, Parker Abercrombie, @patperu, Patrick Miller, @pdb61, @pengyu, Peter F Schulam, Peter Lindbrook, Peter Meilstrup, @philchalmers, @picasa, @piccolbo, @pierreroudier, @pooryorick, @ramnathv, Ramnath Vaidyanathan, @Rappster, Ricardo Pietrobon, Richard Cotton, @richardreeve, R. Mark Sharp, @rmflight, @rmsharp, Robert M Flight, @RobertZK, @robiRagan, Romain François, @rrunner, @rubenfcasal, @sailingwave, @sarunasmerkliopas, @sbgraves237, @scottko, @scottl, Scott Ritchie, Sean Anderson, Sean Carmody, Sean Wilkinson, @sebastian-c, Sebastien Vigneau, @shabbychef, Shannon Rush, Simon O’Hanlon, Simon Potter, @SplashDance, @ste-fan, Stefan Widgren, @stephens999, Steven Pav, @strongh, @stuttungur, @surmann, @swnydick, @taekyunk, @talgalili, Tal Galili, @tdenes, @Thomas, @thomasherbig, @thomaszumbrunn, Tim Cole, @tjmahr, Tom Buckley, Tom Crockett, @ttriche, @twjacobs, @tyhenkaline, @tylerritchie, @ulrichatz, @varun729, @victorkryukov, @vijaybarve, @vzemlys, @wchi144, @wibeasley, @WilCrofter, William Doane, Winston Chang, @wmc3, @wordnerd, Yoni Ben-Meshulam, @zackham, @zerokarmaleft, Zhongpeng Lin. Throughout this book I use f() to refer to functions, g to refer to variables and function parameters, and h/ to paths. Larger code blocks intermingle input and output. Output is commented so that if you have an electronic version of the book, e.g., http://adv-r.had.co.nz, you can easily copy and paste examples into R. Output comments look like #> to distinguish them from regular comments.  This book was written in Rmarkdown inside Rstudio. knitr and pandoc converted the raw Rmarkdown to html and pdf. The website was made with jekyll, styled with bootstrap, and automatically published to Amazon’s S3 by travis-ci. The complete source is available from github. Code is set in inconsolata. © Hadley Wickham. Powered by jekyll,
          knitr, and
          pandoc. Source
          available on github.
         How to contribute Edit this page The environment is the data structure that powers scoping. This chapter dives deep into environments, describing their structure in depth, and using them to improve your understanding of the four scoping rules described in lexical scoping.  Environments can also be useful data structures in their own right because they have reference semantics. When you modify a binding in an environment, the environment is not copied; it’s modified in place. Reference semantics are not often needed, but can be extremely useful. If you can answer the following questions correctly, you already know the most important topics in this chapter. You can find the answers at the end of the chapter in answers. List at least three ways that an environment is different to a list. What is the parent of the global environment? What is the only environment that doesn’t have a parent? What is the enclosing environment of a function? Why is it important? How do you determine the environment from which a function was called? How are <- and <<- different? Environment basics introduces you to the basic properties of an environment and shows you how to create your own. Recursing over environments provides a function template for computing with environments, illustrating the idea with a useful function. Function environments revises R’s scoping rules in more depth, showing how they correspond to four types of environment associated with each function. Binding names to values describes the rules that names must follow (and how to bend them), and shows some variations on binding a name to a value. Explicit environments discusses three problems where environments are useful data structures in their own right, independent of the role they place in scoping. This chapter uses many functions from the pryr package to pry open R and look inside at the messy details. You can install pryr by running install.packages("pryr") The job of an environment is to associate, or bind, a set of names to a set of values. You can think of an environment as a bag of names:    Each name points to an object stored elsewhere in memory:  The objects don’t live in the environment so multiple names can point to the same object:  Confusingly they can also point to different objects that have the same value:  If an object has no names pointing to it, it gets automatically deleted by the garbage collector. This process is described in more detail in gc. Every environment has a parent, another environment. In diagrams, I’ll represent the pointer to parent with a small black circle. The parent is used to implement lexical scoping: if a name is not found in an environment, then R will look in its parent (and so on). Only one environment doesn’t have a parent: the empty environment.   We use the metaphor of a family to refer to environments. The grandparent of an environment is the parent’s parent, and the ancestors include all parent environments up to the empty environment. It’s rare to talk about the children of an environment because there are no back links: given an environment we have no way to find its children. Generally, an environment is similar to a list, with four important exceptions: Every object in an environment has a unique name. The objects in an environment are not ordered (i.e., it doesn’t make sense to ask what the first object in an environment is). An environment has a parent. Environments have reference semantics. More technically, an environment is made up of two components, the frame, which contains the name-object bindings (and behaves much like a named list), and the parent environment. Unfortunately “frame” is used inconsistently in R. For example, parent.frame() doesn’t give you the parent frame of an environment. Instead, it gives you the calling environment. This is discussed in more detail in calling environments.   There are four special environments: The globalenv(), or global environment, is the interactive workspace. This is the environment in which you normally work. The parent of the global environment is the last package that you attached with library() or require(). The baseenv(), or base environment, is the environment of the base package. Its parent is the empty environment. The emptyenv(), or empty environment, is the ultimate ancestor of all environments, and the only environment without a parent. The environment() is the current environment. search() lists all parents of the global environment. This is called the search path because objects in these environments can be found from the top-level interactive workspace. It contains one environment for each attached package and any other objects that you’ve attach()ed. It also contains a special environment called Autoloads which is used to save memory by only loading package objects (like big datasets) when needed.  You can access any environment on the search list using as.environment(). globalenv(), baseenv(), the environments on the search path, and emptyenv() are connected as shown below. Each time you load a new package with library() it is inserted between the global environment and the package that was previously at the top of the search path.  To create an environment manually, use new.env(). You can list the bindings in the environment’s frame with ls() and see its parent with parent.env().  The easiest way to modify the bindings in an environment is to treat it like a list: By default, ls() only shows names that don’t begin with .. Use all.names = TRUE to show all bindings in an environment:  Another useful way to view an environment is ls.str(). It is more useful than str() because it shows each object in the environment. Like ls(), it also has an all.names argument. Given a name, you can extract the value to which it is bound with $, [[, or get(): $ and [[ look only in one environment and return NULL if there is no binding associated with the name.   get() uses the regular scoping rules and throws an error if the binding is not found.  Deleting objects from environments works a little differently from lists. With a list you can remove an entry by setting it to NULL. In environments, that will create a new binding to NULL. Instead, use rm() to remove the binding.   You can determine if a binding exists in an environment with exists(). Like get(), its default behaviour is to follow the regular scoping rules and look in parent environments. If you don’t want this behavior, use inherits = FALSE: To compare environments, you must use identical() not ==: List three ways in which an environment differs from a list. If you don’t supply an explicit environment, where do ls() and rm() look? Where does <- make bindings? Using parent.env() and a loop (or a recursive function), verify that the ancestors of globalenv() include baseenv() and emptyenv(). Use the same basic idea to implement your own version of search(). Environments form a tree, so it’s often convenient to write a recursive function. This section shows you how by applying your new knowledge of environments to understand the helpful pryr::where(). Given a name, where() finds the environment where that name is defined, using R’s regular scoping rules:  The definition of where() is straightforward. It has two arguments: the name to look for (as a string), and the environment in which to start the search. (We’ll learn later why parent.frame() is a good default in calling environments.) There are three cases: The base case: we’ve reached the empty environment and haven’t found the binding. We can’t go any further, so we throw an error. The successful case: the name exists in this environment, so we return the environment. The recursive case: the name was not found in this environment, so try the parent. It’s easier to see what’s going on with an example. Imagine you have two environments as in the following diagram:  If you’re looking for a, where() will find it in the first environment. If you’re looking for b, it’s not in the first environment, so where() will look in its parent and find it there. If you’re looking for c, it’s not in the first environment, or the second environment, so where() reaches the empty environment and throws an error. It’s natural to work with environments recursively, so where() provides a useful template. Removing the specifics of where() shows the structure more clearly: It’s possible to use a loop instead of recursion. This might run slightly faster (because we eliminate some function calls), but I think it’s harder to understand. I include it because you might find it easier to see what’s happening if you’re less familiar with recursive functions. Modify where() to find all environments that contain a binding for name. Write your own version of get() using a function written in the style of where(). Write a function called fget() that finds only function objects. It should have two arguments, name and env, and should obey the regular scoping rules for functions: if there’s an object with a matching name that’s not a function, look in the parent. For an added challenge, also add an inherits argument which controls whether the function recurses up the parents or only looks in one environment. Write your own version of exists(inherits = FALSE) (Hint: use ls().) Write a recursive version that behaves like exists(inherits = TRUE). Most environments are not created by you with new.env() but are created as a consequence of using functions. This section discusses the four types of environments associated with a function: enclosing, binding, execution, and calling.  The enclosing environment is the environment where the function was created. Every function has one and only one enclosing environment. For the three other types of environment, there may be 0, 1, or many environments associated with each function: Binding a function to a name with <- defines a binding environment. Calling a function creates an ephemeral execution environment that stores variables created during execution. Every execution environment is associated with a calling environment, which tells you where the function was called. The following sections will explain why each of these environments is important, how to access them, and how you might use them. When a function is created, it gains a reference to the environment where it was made. This is the enclosing environment and is used for lexical scoping. You can determine the enclosing environment of a function by calling environment() with a function as its first argument:  In diagrams, I’ll depict functions as rounded rectangles. The enclosing environment of a function is given by a small black circle:  The previous diagram is too simple because functions don’t have names. Instead, the name of a function is defined by a binding. The binding environments of a function are all the environments which have a binding to it. The following diagram better reflects this relationship because the enclosing environment contains a binding from f to the function:   In this case the enclosing and binding environments are the same. They will be different if you assign a function into a different environment:  The enclosing environment belongs to the function, and never changes, even if the function is moved to a different environment. The enclosing environment determines how the function finds values; the binding environments determine how we find the function. The distinction between the binding environment and the enclosing environment is important for package namespaces. Package namespaces keep packages independent. For example, if package A uses the base mean() function, what happens if package B creates its own mean() function? Namespaces ensure that package A continues to use the base mean() function, and that package A is not affected by package B (unless explicitly asked for).  Namespaces are implemented using environments, taking advantage of the fact that functions don’t have to live in their enclosing environments. For example, take the base function sd(). It’s binding and enclosing environments are different: The definition of sd() uses var(), but if we make our own version of var() it doesn’t affect sd(): This works because every package has two environments associated with it: the package environment and the namespace environment. The package environment contains every publicly accessible function, and is placed on the search path. The namespace environment contains all functions (including internal functions), and its parent environment is a special imports environment that contains bindings to all the functions that the package needs. Every exported function in a package is bound into the package environment, but enclosed by the namespace environment. This complicated relationship is illustrated by the following diagram:  When we type var into the console, it’s found first in the global environment. When sd() looks for var() it finds it first in its namespace environment so never looks in the globalenv(). What will the following function return the first time it’s run? What about the second?  This function returns the same value every time it is called because of the fresh start principle, described in a fresh start. Each time a function is called, a new environment is created to host execution. The parent of the execution environment is the enclosing environment of the function. Once the function has completed, this environment is thrown away. Let’s depict that graphically with a simpler function. I draw execution environments around the function they belong to with a dotted border.  When you create a function inside another function, the enclosing environment of the child function is the execution environment of the parent, and the execution environment is no longer ephemeral. The following example illustrates that idea with a function factory, plus(). We use that factory to create a function called plus_one(). The enclosing environment of plus_one() is the execution environment of plus() where x is bound to the value 1.   You’ll learn more about function factories in functional programming. Look at the following code. What do you expect i() to return when the code is run?  The top-level x (bound to 20) is a red herring: using the regular scoping rules, h() looks first where it is defined and finds that the value associated with x is 10. However, it’s still meaningful to ask what value x is associated within the environment where i() is called: x is 10 in the environment where h() is defined, but it is 20 in the environment where h() is called. We can access this environment using the unfortunately named parent.frame(). This function returns the environment where the function was called. We can also use this function to look up the value of names in that environment: In more complicated scenarios, there’s not just one parent call, but a sequence of calls which lead all the way back to the initiating function, called from the top-level. The following code generates a call stack three levels deep. The open-ended arrows represent the calling environment of each execution environment.  Note that each execution environment has two parents: a calling environment and an enclosing environment. R’s regular scoping rules only use the enclosing parent; parent.frame() allows you to access the calling parent. Looking up variables in the calling environment rather than in the enclosing environment is called dynamic scoping. Few languages implement dynamic scoping (Emacs Lisp is a notable exception.) This is because dynamic scoping makes it much harder to reason about how a function operates: not only do you need to know how it was defined, you also need to know in what context it was called. Dynamic scoping is primarily useful for developing functions that aid interactive data analysis. It is one of the topics discussed in non-standard evaluation.   List the four environments associated with a function. What does each one do? Why is the distinction between enclosing and binding environments particularly important? Draw a diagram that shows the enclosing environments of this function: Expand your previous diagram to show function bindings. Expand it again to show the execution and calling environments. Write an enhanced version of str() that provides more information about functions. Show where the function was found and what environment it was defined in. Assignment is the act of binding (or rebinding) a name to a value in an environment. It is the counterpart to scoping, the set of rules that determines how to find the value associated with a name. Compared to most languages, R has extremely flexible tools for binding names to values. In fact, you can not only bind values to names, but you can also bind expressions (promises) or even functions, so that every time you access the value associated with a name, you get something different!  You’ve probably used regular assignment in R thousands of times. Regular assignment creates a binding between a name and an object in the current environment. Names usually consist of letters, digits, . and _, and can’t begin with _. If you try to use a name that doesn’t follow these rules, you get an error: Reserved words (like TRUE, NULL, if, and function) follow the rules but are reserved by R for other purposes: A complete list of reserved words can be found in ?Reserved.    It’s possible to override the usual rules and use a name with any sequence of characters by surrounding the name with backticks: You can also create non-syntactic bindings using single and double quotes instead of backticks, but I don’t recommend it. The ability to use strings on the left hand side of the assignment arrow is a historical artefact, used before R supported backticks. The regular assignment arrow, <-, always creates a variable in the current environment. The deep assignment arrow, <<-, never creates a variable in the current environment, but instead modifies an existing variable found by walking up the parent environments. You can also do deep binding with assign(): name <<- value is equivalent to assign("name", value, inherits = TRUE). If <<- doesn’t find an existing variable, it will create one in the global environment. This is usually undesirable, because global variables introduce non-obvious dependencies between functions. <<- is most often used in conjunction with a closure, as described in Closures. There are two other special types of binding, delayed and active: Rather than assigning the result of an expression immediately, a delayed binding creates and stores a promise to evaluate the expression when needed. We can create delayed bindings with the special assignment operator %<d-%, provided by the pryr package. %<d-% is a wrapper around the base delayedAssign() function, which you may need to use directly if you need more control. Delayed bindings are used to implement autoload(), which makes R behave as if the package data is in memory, even though it’s only loaded from disk when you ask for it.  Active are not bound to a constant object. Instead, they’re re-computed every time they’re accessed: %<a-% is a wrapper for the base function makeActiveBinding(). You may want to use this function directly if you want more control. Active bindings are used to implement reference class fields.  What does this function do? How does it differ from <<- and why might you prefer it? Create a version of assign() that will only bind new names, never re-bind old names. Some programming languages only do this, and are known as single assignment languages. Write an assignment function that can do active, delayed, and locked bindings. What might you call it? What arguments should it take? Can you guess which sort of assignment it should do based on the input? As well as powering scoping, environments are also useful data structures in their own right because they have reference semantics. Unlike most objects in R, when you modify an environment, it does not make a copy. For example, look at this modify() function.   If you apply it to a list, the original list is not changed because modifying a list actually creates and modifies a copy. However, if you apply it to an environment, the original environment is modified: Just as you can use a list to pass data between functions, you can also use an environment. When creating your own environment, note that you should set its parent environment to be the empty environment. This ensures you don’t accidentally inherit objects from somewhere else: Environments are data structures useful for solving three common problems: These are described in turn below. Since environments have reference semantics, you’ll never accidentally create a copy. This makes it a useful vessel for large objects. It’s a common technique for bioconductor packages which often have to manage large genomic objects. Changes to R 3.1.0 have made this use substantially less important because modifying a list no longer makes a deep copy. Previously, modifying a single element of a list would cause every element to be copied, an expensive operation if some elements are large. Now, modifying a list efficiently reuses existing vectors, saving much time. Explicit environments are useful in packages because they allow you to maintain state across function calls. Normally, objects in a package are locked, so you can’t modify them directly. Instead, you can do something like this: Returning the old value from setter functions is a good pattern because it makes it easier to reset the previous value in conjunction with on.exit() (see more in on exit). A hashmap is a data structure that takes constant, O(1), time to find an object based on its name. Environments provide this behaviour by default, so can be used to simulate a hashmap. See the CRAN package hash for a complete development of this idea.   There are four ways: every object in an environment must have a name; order doesn’t matter; environments have parents; environments have reference semantics. The parent of the global environment is the last package that you loaded. The only environment that doesn’t have a parent is the empty environment. The enclosing environment of a function is the environment where it was created. It determines where a function looks for variables. Use parent.frame(). <- always creates a binding in the current environment; <<- rebinds an existing name in a parent of the current environment. © Hadley Wickham. Powered by jekyll,
          knitr, and
          pandoc. Source
          available on github.
         How to contribute Edit this page “To become significantly more reliable, code must become more transparent. In particular, nested conditions and loops must be viewed with great suspicion. Complicated control flows confuse programmers. Messy code often hides bugs.” — Bjarne Stroustrup A higher-order function is a function that takes a function as an input or returns a function as output. We’ve already seen one type of higher order function: closures, functions returned by another function. The complement to a closure is a functional, a function that takes a function as an input and returns a vector as output. Here’s a simple functional: it calls the function provided as input with 1000 random uniform numbers.  The chances are that you’ve already used a functional: the three most frequently used are lapply(), apply(), and tapply(). All three take a function as input (among other things) and return a vector as output. A common use of functionals is as an alternative to for loops. For loops have a bad rap in R. They have a reputation for being slow (although that reputation is only partly true, see modification in place for more details). But the real downside of for loops is that they’re not very expressive. A for loop conveys that it’s iterating over something, but doesn’t clearly convey a high level goal. Instead of using a for loop, it’s better to use a functional. Each functional is tailored for a specific task, so when you recognise the functional you know immediately why it’s being used. Functionals play other roles as well as replacements for for-loops. They are useful for encapsulating common data manipulation tasks like split-apply-combine, for thinking “functionally”, and for working with mathematical functions.  Functionals reduce bugs in your code by better communicating intent. Functionals implemented in base R are well tested (i.e., bug-free) and efficient, because they’re used by so many people. Many are written in C, and use special tricks to enhance performance. That said, using functionals will not always produce the fastest code. Instead, it helps you clearly communicate and build tools that solve a wide range of problems. It’s a mistake to focus on speed until you know it’ll be a problem. Once you have clear, correct code you can make it fast using the techniques you’ll learn in improving the speed of your code. My first functional: lapply() introduces your first functional: lapply(). For loop functionals shows you variants of lapply() that produce different outputs, take different inputs, and distribute computation in different ways. Data structure functionals discusses functionals that work with more complex data structures like matrices and arrays. Functional programming teaches you about the powerful Reduce() and Filter() functions which are useful for working with lists. Mathematical functionals discusses functionals that you might be familiar with from mathematics, like root finding, integration, and optimisation. Loops that shouldn’t be converted to functions provides some important caveats about when you shouldn’t attempt to convert a loop into a functional. A family of functions finishes off the chapter by showing you how functionals can take a simple building block and use it to create a set of powerful and consistent tools. You’ll use closures frequently used in conjunction with functionals. If you need a refresher, review closures. The simplest functional is lapply(), which you may already be familiar with. lapply() takes a function, applies it to each element in a list, and returns the results in the form of a list. lapply() is the building block for many other functionals, so it’s important to understand how it works. Here’s a pictorial representation:   lapply() is written in C for performance, but we can create a simple R implementation that does the same thing: From this code, you can see that lapply() is a wrapper for a common for loop pattern: create a container for output, apply f() to each component of a list, and fill the container with the results. All other for loop functionals are variations on this theme: they simply use different types of input or output. lapply() makes it easier to work with lists by eliminating much of the boilerplate associated with looping. This allows you to focus on the function that you’re applying: (I’m using unlist() to convert the output from a list to a vector to make it more compact. We’ll see other ways of making the output a vector shortly.) Since data frames are also lists, lapply() is also useful when you want to do something to each column of a data frame:  The pieces of x are always supplied as the first argument to f. If you want to vary a different argument, you can use an anonymous function. The following example varies the amount of trimming applied when computing the mean of a fixed x. It’s useful to remember that there are three basic ways to loop over a vector:  The first form is usually not a good choice for a for loop because it leads to inefficient ways of saving output. With this form it’s very natural to save the output by extending a datastructure, like in this example: This is slow because each time you extend the vector, R has to copy all of the existing elements. Avoid copies discusses this problem in more depth. Instead, it’s much better to create the space you’ll need for the output and then fill it in. This is easiest with the second form:  Just as there are three basic ways to use a for loop, there are three basic ways to use lapply(): Typically you’d use the first form because lapply() takes care of saving the output for you. However, if you need to know the position or name of the element you’re working with, you should use the second or third form. Both give you an element’s position (i, nm) and value (xs[[i]], xs[[nm]]). If you’re struggling to solve a problem using one form, you might find it easier with another. Why are the following two invocations of lapply() equivalent? The function below scales a vector so it falls in the range [0, 1]. How would you apply it to every column of a data frame? How would you apply it to every numeric column in a data frame? Use both for loops and lapply() to fit linear models to the mtcars using the formulas stored in this list: Fit the model mpg ~ disp to each of the bootstrap replicates of mtcars in the list below by using a for loop and lapply(). Can you do it without an anonymous function? For each model in the previous two exercises, extract R2 using the function below. The key to using functionals in place of for loops is recognising that common looping patterns are already implemented in existing base functionals. Once you’ve mastered these existing functionals, the next step is to start writing your own: if you discover you’re duplicating the same looping pattern in many places, you should extract it out into its own function. The following sections build on lapply() and discuss: sapply() and vapply(), variants of lapply() that produce vectors, matrices, and arrays as output, instead of lists. Map() and mapply() which iterate over multiple input data structures in parallel. mclapply() and mcMap(), parallel versions of lapply() and Map(). Writing a new function, rollapply(), to solve a new problem. sapply() and vapply() are very similar to lapply() except they simplify their output to produce an atomic vector. While sapply() guesses, vapply() takes an additional argument specifying the output type. sapply() is great for interactive use because it saves typing, but if you use it inside your functions you’ll get weird errors if you supply the wrong type of input. vapply() is more verbose, but gives more informative error messages and never fails silently. It is better suited for use inside other functions.   The following example illustrates these differences. When given a data frame, sapply() and vapply() return the same results. When given an empty list, sapply() returns another empty list instead of the more correct zero-length logical vector. If the function returns results of different types or lengths, sapply() will silently return a list, while vapply() will throw an error. sapply() is fine for interactive use because you’ll normally notice if something goes wrong, but it’s dangerous when writing functions. The following example illustrates a possible problem when extracting the class of columns in a data frame: if you falsely assume that class only has one value and use sapply(), you won’t find out about the problem until some future function is given a list instead of a character vector. sapply() is a thin wrapper around lapply() that transforms a list into a vector in the final step. vapply() is an implementation of lapply() that assigns results to a vector (or matrix) of appropriate type instead of as a list. The following code shows a pure R implementation of the essence of sapply() and vapply() (the real functions have better error handling and preserve names, among other things).  vapply() and sapply() have different outputs from lapply(). The following section discusses Map(), which has different inputs. With lapply(), only one argument to the function varies; the others are fixed. This makes it poorly suited for some problems. For example, how would you find a weighted mean when you have two lists, one of observations and the other of weights?  It’s easy to use lapply() to compute the unweighted means: But how could we supply the weights to weighted.mean()? lapply(x, means, w) won’t work because the additional arguments to lapply() are passed to every call. We could change looping forms: This works, but it’s a little clumsy. A cleaner alternative is to use Map, a variant of lapply(), where all arguments can vary. This lets us write: Note that the order of arguments is a little different: function is the first argument for Map() and the second for lapply(). This is equivalent to: There’s a natural equivalence between Map() and lapply() because you can always convert a Map() to an lapply() that iterates over indices. But using Map() is more concise, and more clearly indicates what you’re trying to do. Map is useful whenever you have two (or more) lists (or data frames) that you need to process in parallel. For example, another way of standardising columns is to first compute the means and then divide by them. We could do this with lapply(), but if we do it in two steps, we can more easily check the results at each step, which is particularly important if the first step is more complicated. If some of the arguments should be fixed and constant, use an anonymous function: We’ll see a more compact way to express the same idea in the next chapter. You may be more familiar with mapply() than Map(). I prefer Map() because: It’s equivalent to mapply with simplify = FALSE, which is almost always what you want. Instead of using an anonymous function to provide constant inputs, mapply has the MoreArgs argument that takes a list of extra arguments that will be supplied, as is, to each call. This breaks R’s usual lazy evaluation semantics, and is inconsistent with other functions. In brief, mapply() adds more complication for little gain.  What if you need a for loop replacement that doesn’t exist in base R? You can often create your own by recognising common looping structures and implementing your own wrapper. For example, you might be interested in smoothing your data using a rolling (or running) mean function:   But if the noise was more variable (i.e., it has a longer tail), you might worry that your rolling mean was too sensitive to outliers. Instead, you might want to compute a rolling median.  To change rollmean() to rollmedian(), all you need to do is replace mean with median inside the loop. But instead of copying and pasting to create a new function, we could extract the idea of computing a rolling summary into its own function:   You might notice that the internal loop looks pretty similar to a vapply() loop, so we could rewrite the function as: This is effectively the same as the implementation in zoo::rollapply(), which provides many more features and much more error checking. One interesting thing about the implementation of lapply() is that because each iteration is isolated from all others, the order in which they are computed doesn’t matter. For example, lapply3() scrambles the order of computation, but the results are always the same:   This has a very important consequence: since we can compute each element in any order, it’s easy to dispatch the tasks to different cores, and compute them in parallel. This is what parallel::mclapply() (and parallel::mcMap()) does. (These functions are not available in Windows, but you can use the similar parLapply() with a bit more work. See parallelise for more details.)  In this case, mclapply() is actually slower than lapply(). This is because the cost of the individual computations is low, and additional work is needed to send the computation to the different cores and to collect the results. If we take a more realistic example, generating bootstrap replicates of a linear model for example, the advantages are clearer:  While increasing the number of cores will not always lead to linear improvement, switching from lapply() or Map() to its parallelised forms can dramatically improve computational performance. Use vapply() to: Compute the standard deviation of every column in a numeric data frame. Compute the standard deviation of every numeric column in a mixed data frame. (Hint: you’ll need to use vapply() twice.) Why is using sapply() to get the class() of each element in a data frame dangerous? The following code simulates the performance of a t-test for non-normal data. Use sapply() and an anonymous function to extract the p-value from every trial. Extra challenge: get rid of the anonymous function by using [[ directly. What does replicate() do? What sort of for loop does it eliminate? Why do its arguments differ from lapply() and friends? Implement a version of lapply() that supplies FUN with both the name and the value of each component. Implement a combination of Map() and vapply() to create an lapply() variant that iterates in parallel over all of its inputs and stores its outputs in a vector (or a matrix). What arguments should the function take? Implement mcsapply(), a multicore version of sapply(). Can you implement mcvapply(), a parallel version of vapply()? Why or why not? Functionals can also be used to eliminate loops in common data manipulation tasks. In this section, we’ll give a brief overview of the available options, hint at how they can help you, and point you in the right direction to learn more. We’ll cover three categories of data structure functionals: apply(), sweep(), and outer() work with matrices. tapply() summarises a vector by groups defined by another vector. the plyr package, which generalises tapply() to make it easy to work with data frames, lists, or arrays as inputs, and data frames, lists, or arrays as outputs. So far, all the functionals we’ve seen work with 1d input structures. The three functionals in this section provide useful tools for working with higher-dimensional data structures. apply() is a variant of sapply() that works with matrices and arrays. You can think of it as an operation that summarises a matrix or array by collapsing each row or column to a single number. It has four arguments:  A typical example of apply() looks like this There are a few caveats to using apply(). It doesn’t have a simplify argument, so you can never be completely sure what type of output you’ll get. This means that apply() is not safe to use inside a function unless you carefully check the inputs. apply() is also not idempotent in the sense that if the summary function is the identity operator, the output is not always the same as the input: (You can put high-dimensional arrays back in the right order using aperm(), or use plyr::aaply(), which is idempotent.) sweep() allows you to “sweep” out the values of a summary statistic. It is often used with apply() to standardise arrays. The following example scales the rows of a matrix so that all values lie between 0 and 1.  The final matrix functional is outer(). It’s a little different in that it takes multiple vector inputs and creates a matrix or array output where the input function is run over every combination of the inputs:  Good places to learn more about apply() and friends are: “Using apply, sapply, lapply in R” by Peter Werner. “The infamous apply function” by Slawa Rokicki. “The R apply function - a tutorial with examples” by axiomOfChoice. The stackoverflow question “R Grouping functions: sapply vs. lapply vs. apply vs. tapply vs. by vs. aggregate”. You can think about tapply() as a generalisation to apply() that allows for “ragged” arrays, arrays where each row can have a different number of columns. This is often needed when you’re trying to summarise a data set. For example, imagine you’ve collected pulse rate data from a medical trial, and you want to compare the two groups:  tapply() works by creating a “ragged” data structure from a set of inputs, and then applying a function to the individual elements of that structure. The first task is actually what the split() function does. It takes two inputs and returns a list which groups elements together from the first vector according to elements, or categories, from the second vector: Then tapply() is just the combination of split() and sapply(): Being able to rewrite tapply() as a combination of split() and sapply() is a good indication that we’ve identified some useful building blocks.  One challenge with using the base functionals is that they have grown organically over time, and have been written by multiple authors. This means that they are not very consistent:  With tapply() and sapply(), the simplify argument is called simplify. With mapply(), it’s called SIMPLIFY. With apply(), the argument is absent. vapply() is a variant of sapply() that allows you to describe what the output should be, but there are no corresponding variants for tapply(), apply(), or Map(). The first argument of most base functionals is a vector, but the first argument in Map() is a function. This makes learning these operators challenging, as you have to memorise all of the variations. Additionally, if you think about the possible combinations of input and output types, base R only covers a partial set of cases: This was one of the driving motivations behind the creation of the plyr package. It provides consistently named functions with consistently named arguments and covers all combinations of input and output data structures: Each of these functions splits up the input, applies a function to each piece, and then combines the results. Overall, this process is called “split-apply-combine”. You can read more about it and plyr in “The Split-Apply-Combine Strategy for Data Analysis”, an open-access article published in the Journal of Statistical Software.  How does apply() arrange the output? Read the documentation and perform some experiments. There’s no equivalent to split() + vapply(). Should there be? When would it be useful? Implement one yourself. Implement a pure R version of split(). (Hint: use unique() and subsetting.) Can you do it without a for loop? What other types of input and output are missing? Brainstorm before you look up some answers in the plyr paper. Another way of thinking about functionals is as a set of general tools for altering, subsetting, and collapsing lists. Every functional programming language has three tools for this: Map(), Reduce(), and Filter(). We’ve seen Map() already, and the following sections describe Reduce(), a powerful tool for extending two-argument functions, and Filter(), a member of an important class of functionals that work with predicates, functions that return a single TRUE or FALSE. Reduce() reduces a vector, x, to a single value by recursively calling a function, f, two arguments at a time. It combines the first two elements with f, then combines the result of that call with the third element, and so on. Calling Reduce(f, 1:3) is equivalent to f(f(1, 2), 3). Reduce is also known as fold, because it folds together adjacent elements in the list.   The following two examples show what Reduce does with an infix and prefix function: The essence of Reduce() can be described by a simple for loop: The real Reduce() is more complicated because it includes arguments to control whether the values are reduced from the left or from the right (right), an optional initial value (init), and an option to output intermediate results (accumulate). Reduce() is an elegant way of extending a function that works with two inputs into a function that can deal with any number of inputs. It’s useful for implementing many types of recursive operations, like merges and intersections. (We’ll see another use in the final case study.) Imagine you have a list of numeric vectors, and you want to find the values that occur in every element: You could do that by intersecting each element in turn: That’s hard to read. With Reduce(), the equivalent is: A predicate is a function that returns a single TRUE or FALSE, like is.character, all, or is.NULL. A predicate functional applies a predicate to each element of a list or data frame. There are three useful predicate functionals in base R: Filter(), Find(), and Position().   Filter() selects only those elements which match the predicate.  Find() returns the first element which matches the predicate (or the last element if right = TRUE).  Position() returns the position of the first element that matches the predicate (or the last element if right = TRUE).  Another useful predicate functional is where(), a custom functional that generates a logical vector from a list (or a data frame) and a predicate:  The following example shows how you might use these functionals with a data frame: Why isn’t is.na() a predicate function? What base R function is closest to being a predicate version of is.na()? Use Filter() and vapply() to create a function that applies a summary statistic to every numeric column in a data frame. What’s the relationship between which() and Position()? What’s the relationship between where() and Filter()? Implement Any(), a function that takes a list and a predicate function, and returns TRUE if the predicate function returns TRUE for any of the inputs. Implement All() similarly. Implement the span() function from Haskell: given a list x and a predicate function f, span returns the location of the longest sequential run of elements where the predicate is true. (Hint: you might find rle() helpful.) Functionals are very common in mathematics. The limit, the maximum, the roots (the set of points where f(x) = 0), and the definite integral are all functionals: given a function, they return a single number (or vector of numbers). At first glance, these functions don’t seem to fit in with the theme of eliminating loops, but if you dig deeper you’ll find out that they are all implemented using an algorithm that involves iteration. In this section we’ll use some of R’s built-in mathematical functionals. There are three functionals that work with functions to return single numeric values:    Let’s explore how these are used with a simple function, sin(): In statistics, optimisation is often used for maximum likelihood estimation (MLE). In MLE, we have two sets of parameters: the data, which is fixed for a given problem, and the parameters, which vary as we try to find the maximum. These two sets of parameters make the problem well suited for closures. Combining closures with optimisation gives rise to the following approach to solving MLE problems.  The following example shows how we might find the maximum likelihood estimate for λ, if our data come from a Poisson distribution. First, we create a function factory that, given a dataset, returns a function that computes the negative log likelihood (NLL) for parameter lambda. In R, it’s common to work with the negative since optimise() defaults to finding the minimum.  Note how the closure allows us to precompute values that are constant with respect to the data. We can use this function factory to generate specific NLL functions for input data. Then optimise() allows us to find the best values (the maximum likelihood estimates), given a generous starting range. We can check that these values are correct by comparing them to the analytic solution: in this case, it’s just the mean of the data, 32.1 and 5.4666667. Another important mathematical functional is optim(). It is a generalisation of optimise() that works with more than one dimension. If you’re interested in how it works, you might want to explore the Rvmmin package, which provides a pure-R implementation of optim(). Interestingly Rvmmin is no slower than optim(), even though it is written in R, not C. For this problem, the bottleneck lies not in controlling the optimisation but with having to evaluate the function multiple times.  Implement arg_max(). It should take a function and a vector of inputs, and return the elements of the input where the function returns the highest value. For example, arg_max(-10:5, function(x) x ^ 2) should return -10. arg_max(-5:5, function(x) x ^ 2) should return c(-5, 5). Also implement the matching arg_min() function. Challenge: read about the fixed point algorithm. Complete the exercises using R. Some loops have no natural functional equivalent. In this section you’ll learn about three common cases:  It’s possible to torture these problems to use a functional, but it’s not a good idea. You’ll create code that is harder to understand, eliminating the main reason for using functionals in the first case. If you need to modify part of an existing data frame, it’s often better to use a for loop. For example, the following code performs a variable-by-variable transformation by matching the names of a list of functions to the names of variables in a data frame. We wouldn’t normally use lapply() to replace this loop directly, but it is possible. Just replace the loop with lapply() by using <<-:  The for loop is gone, but the code is longer and much harder to understand. The reader needs to understand <<- and how x[[y]] <<- z works (it’s not simple!). In short, we’ve taken a simple, easily understood for loop, and turned it into something few people will understand: not a good idea! It’s hard to convert a for loop into a functional when the relationship between elements is not independent, or is defined recursively. For example, exponential smoothing works by taking a weighted average of the current and previous data points. The exps() function below implements exponential smoothing with a for loop.  We can’t eliminate the for loop because none of the functionals we’ve seen allow the output at position i to depend on both the input and output at position i - 1. One way to eliminate the for loop in this case is to solve the recurrence relation by removing the recursion and replacing it with explicit references. This requires a new set of mathematical tools, and is challenging, but it can pay off by producing a simpler function. Another type of looping construct in R is the while loop. It keeps running until some condition is met. while loops are more general than for loops: you can rewrite every for loop as a while loop, but you can’t do the reverse. For example, we could turn this for loop:   into this while loop: Not every while loop can be turned into a for loop because many while loops don’t know in advance how many times they will be run: This is a common problem when you’re writing simulations. In this case we can remove the loop by recognising a special feature of the problem. Here we’re counting the number of successes before Bernoulli trial with p = 0.1 fails. This is a geometric random variable, so you could replace the code with i <- rgeom(1, 0.1). Reformulating the problem in this way is hard to do in general, but you’ll benefit greatly if you can do it for your problem. To finish off the chapter, this case study shows how you can use functionals to take a simple building block and make it powerful and general. I’ll start with a simple idea, adding two numbers together, and use functionals to extend it to summing multiple numbers, computing parallel and cumulative sums, and summing across array dimensions. We’ll start by defining a very simple addition function, one which takes two scalar arguments: (We’re using R’s existing addition operator here, which does much more, but the focus here is on how we can take very simple building blocks and extend them to do more.) I’ll also add an na.rm argument. A helper function will make this a bit easier: if x is missing it should return y, if y is missing it should return x, and if both x and y are missing then it should return another argument to the function: identity. This function is probably a bit more general than what we need now, but it’s useful if we implement other binary operators. This allows us to write a version of add() that can deal with missing values if needed: Why did we pick an identity of 0? Why should add(NA, NA, na.rm = TRUE) return 0? Well, for every other input it returns a number, so even if both arguments are NA, it should still do that. What number should it return? We can figure it out because additional is associative, which means that the order of additional doesn’t matter. That means that the following two function calls should return the same value: This implies that add(NA, NA, na.rm = TRUE) must be 0, and hence identity = 0 is the correct default. Now that we have the basics working, we can extend the function to deal with more complicated inputs. One obvious generalisation is to add more than two numbers. We can do this by iteratively adding two numbers: if the input is c(1, 2, 3) we compute add(add(1, 2), 3). This is a simple application of Reduce(): This looks good, but we need to test a few special cases: These are incorrect. In the first case, we get a missing value even though we’ve explicitly asked to ignore them. In the second case, we get NULL instead of a length one numeric vector (as we do for every other set of inputs). The two problems are related. If we give Reduce() a length one vector, it doesn’t have anything to reduce, so it just returns the input. If we give it an input of length zero, it always returns NULL. The easiest way to fix this problem is to use the init argument of Reduce(). This is added to the start of every input vector: r_add() is equivalent to sum(). It would be nice to have a vectorised version of add() so that we can perform the addition of two vectors of numbers in element-wise fashion. We could use Map() or vapply() to implement this, but neither is perfect. Map() returns a list, instead of a numeric vector, so we need to use simplify2array(). vapply() returns a vector but it requires us to loop over a set of indices. A few test cases help to ensure that it behaves as we expect. We’re a bit stricter than base R here because we don’t do recycling. (You could add that if you wanted, but I find that recycling is a frequent source of silent bugs.) Another variant of add() is the cumulative sum. We can implement it with Reduce() by setting the accumulate argument to TRUE: This is equivalent to cumsum(). Finally, we might want to define addition for more complicated data structures like matrices. We could create row and col variants that sum across rows and columns, respectively, or we could go the whole hog and define an array version that could sum across any arbitrary set of dimensions. These are easily implemented as combinations of add() and apply(). The first two are equivalent to rowSums() and colSums(). If every function we have created has an existing equivalent in base R, why did we bother? There are two main reasons: Since all variants were implemented by combining a simple binary operator (add()) and a well-tested functional (Reduce(), Map(), apply()), we know that our variants will behave consistently. We can apply the same infrastructure to other operators, especially those that might not have the full suite of variants in base R. The downside of this approach is that these implementations are not that efficient. (For example, colSums(x) is much faster than apply(x, 2, sum).) However, even if they aren’t that fast, simple implementations are still a good starting point because they’re less likely to have bugs. When you create faster versions, you can compare the results to make sure your fast versions are still correct. If you enjoyed this section, you might also enjoy “List out of lambda”, a blog article by Steve Losh that shows how you can produce high level language structures (like lists) out of more primitive language features (like closures, aka lambdas). Implement smaller and larger functions that, given two inputs, return either the smaller or the larger value. Implement na.rm = TRUE: what should the identity be? (Hint: smaller(x, smaller(NA, NA, na.rm = TRUE), na.rm = TRUE) must be x, so smaller(NA, NA, na.rm = TRUE) must be bigger than any other value of x.) Use smaller and larger to implement equivalents of min(), max(), pmin(), pmax(), and new functions row_min() and row_max(). Create a table that has and, or, add, multiply, smaller, and larger in the columns and binary operator, reducing variant, vectorised variant, and array variants in the rows. Fill in the cells with the names of base R functions that perform each of the roles. Compare the names and arguments of the existing R functions. How consistent are they? How could you improve them? Complete the matrix by implementing any missing functions. How does paste() fit into this structure? What is the scalar binary function that underlies paste()? What are the sep and collapse arguments to paste() equivalent to? Are there any paste variants that don’t have existing R implementations? © Hadley Wickham. Powered by jekyll,
          knitr, and
          pandoc. Source
          available on github.
         How to contribute Edit this page R is not a fast language. This is not an accident. R was purposely designed to make data analysis and statistics easier for you to do. It was not designed to make life easier for your computer. While R is slow compared to other programming languages, for most purposes, it’s fast enough.  The goal of this part of the book is to give you a deeper understanding of R’s performance characteristics. In this chapter, you’ll learn about some of the trade-offs that R has made, valuing flexibility over performance. The following four chapters will give you the skills to improve the speed of your code when you need to: In Profiling, you’ll learn how to systematically make your code faster. First you figure what’s slow, and then you apply some general techniques to make the slow parts faster. In Memory, you’ll learn about how R uses memory, and how garbage collection and copy-on-modify affect performance and memory usage. For really high-performance code, you can move outside of R and use another programming language. Rcpp will teach you the absolute minimum you need to know about C++ so you can write fast code using the Rcpp package. To really understand the performance of built-in base functions, you’ll need to learn a little bit about R’s C API. In R’s C interface, you’ll learn a little about R’s C internals. Let’s get started by learning more about why R is slow. To understand R’s performance, it helps to think about R as both a language and as an implementation of that language. The R-language is abstract: it defines what R code means and how it should work. The implementation is concrete: it reads R code and computes a result. The most popular implementation is the one from r-project.org. I’ll call that implementation GNU-R to distinguish it from R-language, and from the other implementations I’ll discuss later in the chapter.  The distinction between R-language and GNU-R is a bit murky because the R-language is not formally defined. While there is the R language definition, it is informal and incomplete. The R-language is mostly defined in terms of how GNU-R works. This is in contrast to other languages, like C++ and javascript, that make a clear distinction between language and implementation by laying out formal specifications that describe in minute detail how every aspect of the language should work. Nevertheless, the distinction between R-language and GNU-R is still useful: poor performance due to the language is hard to fix without breaking existing code; fixing poor performance due to the implementation is easier. In Language performance, I discuss some of the ways in which the design of the R-language imposes fundamental constraints on R’s speed. In Implementation performance, I discuss why GNU-R is currently far from the theoretical maximum, and why improvements in performance happen so slowly. While it’s hard to know exactly how much faster a better implementation could be, a >10x improvement in speed seems achievable. In alternative implementations, I discuss some of the promising new implementations of R, and describe one important technique they use to make R code run faster. Beyond performance limitations due to design and implementation, it has to be said that a lot of R code is slow simply because it’s poorly written. Few R users have any formal training in programming or software development. Fewer still write R code for a living. Most people use R to understand data: it’s more important to get an answer quickly than to develop a system that will work in a wide variety of situations. This means that it’s relatively easy to make most R code much faster, as we’ll see in the following chapters. Before we examine some of the slower parts of the R-language and GNU-R, we need to learn a little about benchmarking so that we can give our intuitions about performance a concrete foundation. A microbenchmark is a measurement of the performance of a very small piece of code, something that might take microseconds (µs) or nanoseconds (ns) to run. I’m going to use microbenchmarks to demonstrate the performance of very low-level pieces of R code, which help develop your intuition for how R works. This intuition, by-and-large, is not useful for increasing the speed of real code. The observed differences in microbenchmarks will typically be dominated by higher-order effects in real code; a deep understanding of subatomic physics is not very helpful when baking. Don’t change the way you code because of these microbenchmarks. Instead wait until you’ve read the practical advice in the following chapters.  The best tool for microbenchmarking in R is the microbenchmark package. It provides very precise timings, making it possible to compare operations that only take a tiny amount of time. For example, the following code compares the speed of two ways of computing a square root. By default, microbenchmark() runs each expression 100 times (controlled by the times parameter). In the process, it also randomises the order of the expressions. It summarises the results with a minimum (min), lower quartile (lq), median, upper quartile (uq), and maximum (max). Focus on the median, and use the upper and lower quartiles (lq and uq) to get a feel for the variability. In this example, you can see that using the special purpose sqrt() function is faster than the general exponentiation operator. As with all microbenchmarks, pay careful attention to the units: each computation takes about 800 ns, 800 billionths of a second. To help calibrate the impact of a microbenchmark on run time, it’s useful to think about how many times a function needs to run before it takes a second. If a microbenchmark takes: The sqrt() function takes about 800 ns, or 0.8 µs, to compute the square root of 100 numbers. That means if you repeated the operation a million times, it would take 0.8 s. So changing the way you compute the square root is unlikely to significantly affect real code. Instead of using microbenchmark(), you could use the built-in function system.time(). But system.time() is much less precise, so you’ll need to repeat each operation many times with a loop, and then divide to find the average time of each operation, as in the code below. How do the estimates from system.time() compare to those from microbenchmark()? Why are they different? Here are two other ways to compute the square root of a vector. Which do you think will be fastest? Which will be slowest? Use microbenchmarking to test your answers. Use microbenchmarking to rank the basic arithmetic operators (+, -, *, /, and ^) in terms of their speed. Visualise the results. Compare the speed of arithmetic on integers vs. doubles. You can change the units in which the microbenchmark results are expressed with the unit parameter. Use unit = "eps" to show the number of evaluations needed to take 1 second. Repeat the benchmarks above with the eps unit. How does this change your intuition for performance? In this section, I’ll explore three trade-offs that limit the performance of the R-language: extreme dynamism, name lookup with mutable environments, and lazy evaluation of function arguments. I’ll illustrate each trade-off with a microbenchmark, showing how it slows GNU-R down. I benchmark GNU-R because you can’t benchmark the R-language (it can’t run code). This means that the results are only suggestive of the cost of these design decisions, but are nevertheless useful. I’ve picked these three examples to illustrate some of the trade-offs that are key to language design: the designer must balance speed, flexibility, and ease of implementation. If you’d like to learn more about the performance characteristics of the R-language and how they affect real code, I highly recommend “Evaluating the Design of the R Language” by Floreal Morandat, Brandon Hill, Leo Osvald, and Jan Vitek. It uses a powerful methodology that combines a modified R interpreter and a wide set of code found in the wild. R is an extremely dynamic programming language. Almost anything can be modified after it is created. To give just a few examples, you can: Pretty much the only things you can’t change are objects in sealed namespaces, which are created when you load a package. The advantage of dynamism is that you need minimal upfront planning. You can change your mind at any time, iterating your way to a solution without having to start afresh. The disadvantage of dynamism is that it’s difficult to predict exactly what will happen with a given function call. This is a problem because the easier it is to predict what’s going to happen, the easier it is for an interpreter or compiler to make an optimisation. (If you’d like more details, Charles Nutter expands on this idea at On Languages, VMs, Optimization, and the Way of the World.) If an interpreter can’t predict what’s going to happen, it has to consider many options before it finds the right one. For example, the following loop is slow in R, because R doesn’t know that x is always an integer. That means R has to look for the right + method (i.e., is it adding doubles, or integers?) in every iteration of the loop. The cost of finding the right method is higher for non-primitive functions. The following microbenchmark illustrates the cost of method dispatch for S3, S4, and RC. I create a generic and a method for each OO system, then call the generic and see how long it takes to find and call the method. I also time how long it takes to call the bare function for comparison.  On my computer, the bare function takes about 200 ns. S3 method dispatch takes an additional 2,000 ns; S4 dispatch, 11,000 ns; and RC dispatch, 10,000 ns. S3 and S4 method dispatch are expensive because R must search for the right method every time the generic is called; it might have changed between this call and the last. R could do better by caching methods between calls, but caching is hard to do correctly and a notorious source of bugs. It’s surprisingly difficult to find the value associated with a name in the R-language. This is due to combination of lexical scoping and extreme dynamism. Take the following example. Each time we print a it comes from a different environment:  This means that you can’t do name lookup just once: you have to start from scratch each time. This problem is exacerbated by the fact that almost every operation is a lexically scoped function call. You might think the following simple function calls two functions: + and ^. In fact, it calls four because { and ( are regular functions in R. Since these functions are in the global environment, R has to look through every environment in the search path, which could easily be 10 or 20 environments. The following microbenchmark hints at the performance costs. We create four versions of f(), each with one more environment (containing 26 bindings) between the environment of f() and the base environment where +, ^, (, and { are defined. Each additional environment between f() and the base environment makes the function slower by about 30 ns. It might be possible to implement a caching system so that R only needs to look up the value of each name once. This is hard because there are so many ways to change the value associated with a name: <<-, assign(), eval(), and so on. Any caching system would have to know about these functions to make sure the cache was correctly invalidated and you didn’t get an out-of-date value.  Another simple fix would be to add more built-in constants that you can’t override. This, for example, would mean that R always knew exactly what +, -, {, and ( meant, and you wouldn’t have to repeatedly look up their definitions. That would make the interpreter more complicated (because there are more special cases) and hence harder to maintain, and the language less flexible. This would change the R-language, but it would be unlikely to affect much existing code because it’s such a bad idea to override functions like { and (. In R, function arguments are evaluated lazily (as discussed in lazy evaluation and capturing expressions). To implement lazy evaluation, R uses a promise object that contains the expression needed to compute the result and the environment in which to perform the computation. Creating these objects has some overhead, so each additional argument to a function decreases its speed a little.  The following microbenchmark compares the runtime of a very simple function. Each version of the function has one additional argument. This suggests that adding an additional argument slows the function down by ~20 ns. In most other programming languages there is little overhead for adding extra arguments. Many compiled languages will even warn you if arguments are never used (like in the above example), and automatically remove them from the function. scan() has the most arguments (21) of any base function. About how much time does it take to make 21 promises each time scan is called? Given a simple input (e.g., scan(text = "1 2 3", quiet = T)) what proportion of the total run time is due to creating those promises? Read “Evaluating the Design of the R Language”. What other aspects of the R-language slow it down? Construct microbenchmarks to illustrate. How does the performance of S3 method dispatch change with the length of the class vector? How does performance of S4 method dispatch change with number of superclasses? How about RC? What is the cost of multiple inheritance and multiple dispatch on S4 method dispatch? Why is the cost of name lookup less for functions in the base package? The design of the R language limits its maximum theoretical performance, but GNU-R is currently nowhere near that maximum. There are many things that can (and will) be done to improve performance. This section discusses some aspects of GNU-R that are slow not because of their definition, but because of their implementation. R is over 20 years old. It contains nearly 800,000 lines of code (about 45% C, 19% R, and 17% Fortran). Changes to base R can only be made by members of the R Core Team (or R-core for short). Currently R-core has twenty members, but only six are active in day-to-day development. No one on R-core works full time on R. Most are statistics professors who can only spend a relatively small amount of their time on R. Because of the care that must be taken to avoid breaking existing code, R-core tends to be very conservative about accepting new code. It can be frustrating to see R-core reject proposals that would improve performance. However, the overriding concern for R-core is not to make R fast, but to build a stable platform for data analysis and statistics.  Below, I’ll show two small, but illustrative, examples of parts of R that are currently slow but could, with some effort, be made faster. They are not critical parts of base R, but they have been sources of frustration for me in the past. As with all microbenchmarks, these won’t affect the performance of most code, but can be important for special cases. The following microbenchmark shows seven ways to access a single value (the number in the bottom-right corner) from the built-in mtcars dataset. The variation in performance is startling: the slowest method takes 30x longer than the fastest. There’s no reason that there has to be such a huge difference in performance. It’s simply that no one has had the time to fix it.   Some base functions are known to be slow. For example, take the following three implementations of squish(), a function that ensures that the smallest value in a vector is at least a and its largest value is at most b. The first implementation, squish_ife(), uses ifelse(). ifelse() is known to be slow because it is relatively general and must evaluate all arguments fully. The second implementation, squish_p(), uses pmin() and pmax(). Because these two functions are so specialised, one might expect that they would be fast. However, they’re actually rather slow. This is because they can take any number of arguments and they have to do some relatively complicated checks to determine which method to use. The final implementation uses basic subassignment.    Using pmin() and pmax() is about 3x faster than ifelse(), and using subsetting directly is about twice as fast again. We can often do even better by using C++. The following example compares the best R implementation to a relatively simple, if verbose, implementation in C++. Even if you’ve never used C++, you should still be able to follow the basic strategy: loop over every element in the vector and perform a different action depending on whether or not the value is less than a and/or greater than b. The C++ implementation is around 3x faster than the best pure R implementation. (You’ll learn how to access this C++ code from R in Rcpp.) The performance characteristics of squish_ife(), squish_p(), and squish_in_place() vary considerably with the size of x. Explore the differences. Which sizes lead to the biggest and smallest differences? Compare the performance costs of extracting an element from a list, a column from a matrix, and a column from a data frame. Do the same for rows. There are some exciting new implementations of R. While they all try to stick as closely as possible to the existing language definition, they improve speed by using ideas from modern interpreter design. The four most mature open-source projects are:  pqR (pretty quick R) by Radford Neal. Built on top of R 2.15.0, it fixes many obvious performance issues, and provides better memory management and some support for automatic multithreading.  Renjin by BeDataDriven. Renjin uses the Java virtual machine, and has an extensive test suite.  FastR by a team from Purdue. FastR is similar to Renjin, but it makes more ambitious optimisations and is somewhat less mature.  Riposte by Justin Talbot and Zachary DeVito. Riposte is experimental and ambitious. For the parts of R it implements, it is extremely fast. Riposte is described in more detail in Riposte: A Trace-Driven Compiler and Parallel VM for Vector Code in R.  These are roughly ordered from most practical to most ambitious. Another project, CXXR by Andrew Runnalls, does not provide any performance improvements. Instead, it aims to refactor R’s internal C code in order to build a stronger foundation for future development, to keep behaviour identical to GNU-R, and to create better, more extensible documentation of its internals.  R is a huge language and it’s not clear whether any of these approaches will ever become mainstream. It’s a hard task to make an alternative implementation run all R code in the same way as GNU-R. Can you imagine having to reimplement every function in base R to be not only faster, but also to have exactly the same documented bugs? However, even if these implementations never make a dent in the use of GNU-R, they still provide benefits: Simpler implementations make it easy to validate new approaches before porting to GNU-R. Knowing which aspects of the language can be changed with minimal impact on existing code and maximal impact on performance can help to guide us to where we should direct our attention. Alternative implementations put pressure on the R-core to incorporate performance improvements. One of the most important approaches that pqR, Renjin, FastR, and Riposte are exploring is the idea of deferred evaluation. As Justin Talbot, the author of Riposte, points out: “for long vectors, R’s execution is completely memory bound. It spends almost all of its time reading and writing vector intermediates to memory”. If we could eliminate these intermediate vectors, we could improve performance and reduce memory usage.  The following example shows a very simple example of how deferred evaluation can help. We have three vectors, x, y, z, each containing 1 million elements, and we want to find the sum of x + y where z is TRUE. (This represents a simplification of a pretty common sort of data analysis question.) In R, this creates two big temporary vectors: x + y, 1 million elements long, and (x + y)[z], about 500,000 elements long. This means you need to have extra memory available for the intermediate calculation, and you have to shuttle the data back and forth between the CPU and memory. This slows computation down because the CPU can’t work at maximum efficiency if it’s always waiting for more data to come in. However, if we rewrote the function using a loop in a language like C++, we only need one intermediate value: the sum of all the values we’ve seen: On my computer, this approach is about eight times faster than the vectorised R equivalent, which is already pretty fast. The goal of deferred evaluation is to perform this transformation automatically, so you can write concise R code and have it automatically translated into efficient machine code. Sophisticated translators can also figure out how to make the most of multiple cores. In the above example, if you have four cores, you could split x, y, and z into four pieces performing the conditional sum on each core, then adding together the four individual results. Deferred evaluation can also work with for loops, automatically discovering operations that can be vectorised. This chapter has discussed some of the fundamental reasons that R is slow. The following chapters will give you the tools to do something about it when it impacts your code. © Hadley Wickham. Powered by jekyll,
          knitr, and
          pandoc. Source
          available on github.
         How to contribute Edit this page A solid understanding of R’s memory management will help you predict how much memory you’ll need for a given task and help you to make the most of the memory you have. It can even help you write faster code because accidental copies are a major cause of slow code. The goal of this chapter is to help you understand the basics of memory management in R, moving from individual objects to functions to larger blocks of code. Along the way, you’ll learn about some common myths, such as that you need to call gc() to free up memory, or that for loops are always slow.  Object size shows you how to use object_size() to see how much memory an object occupies, and uses that as a launching point to improve your understanding of how R objects are stored in memory. Memory usage and garbage collection introduces you to the mem_used() and mem_change() functions that will help you understand how R allocates and frees memory. Memory profiling with lineprof shows you how to use the lineprof package to understand how memory is allocated and released in larger code blocks. Modification in place introduces you to the address() and refs() functions so that you can understand when R modifies in place and when R modifies a copy. Understanding when objects are copied is very important for writing efficient R code. In this chapter, we’ll use tools from the pryr and lineprof packages to understand memory usage, and a sample dataset from ggplot2. If you don’t already have them, run this code to get the packages you need: The details of R’s memory management are not documented in a single place. Most of the information in this chapter was gleaned from a close reading of the documentation (particularly ?Memory and ?gc), the memory profiling section of R-exts, and the SEXPs section of R-ints. The rest I figured out by reading the C source code, performing small experiments, and asking questions on R-devel. Any mistakes are entirely mine. To understand memory usage in R, we will start with pryr::object_size(). This function tells you how many bytes of memory an object occupies:  (This function is better than the built-in object.size() because it accounts for shared elements within an object and includes the size of environments.) Something interesting occurs if we use object_size() to systematically explore the size of an integer vector. The code below computes and plots the memory usage of integer vectors ranging in length from 0 to 50 elements. You might expect that the size of an empty vector would be zero and that memory usage would grow proportionately with length. Neither of those things are true!   This isn’t just an artefact of integer vectors. Every length 0 vector occupies 40 bytes of memory: Those 40 bytes are used to store four components possessed by every object in R: Object metadata (4 bytes). These metadata store the base type (e.g. integer) and information used for debugging and memory management. Two pointers: one to the next object in memory and one to the previous object (2 * 8 bytes). This doubly-linked list makes it easy for internal R code to loop through every object in memory. A pointer to the attributes (8 bytes). All vectors have three additional components:  The length of the vector (4 bytes). By using only 4 bytes, you might expect that R could only support vectors up to 24 × 8 − 1 (231, about two billion) elements. But in R 3.0.0 and later, you can actually have vectors up to 252 elements. Read R-internals to see how support for long vectors was added without having to change the size of this field.   The “true” length of the vector (4 bytes). This is basically never used, except when the object is the hash table used for an environment. In that case, the true length represents the allocated space, and the length represents the space currently used. The data (?? bytes). An empty vector has 0 bytes of data, but it’s obviously very important otherwise! Numeric vectors occupy 8 bytes for every element, integer vectors 4, and complex vectors 16. If you’re keeping count you’ll notice that this only adds up to 36 bytes. The remaining 4 bytes are used for padding so that each component starts on an 8 byte (= 64-bit) boundary. Most cpu architectures require pointers to be aligned in this way, and even if they don’t require it, accessing non-aligned pointers tends to be rather slow. (If you’re interested, you can read more about it in C structure packing.) This explains the intercept on the graph. But why does the memory size grow irregularly? To understand why, you need to know a little bit about how R requests memory from the operating system. Requesting memory (with malloc()) is a relatively expensive operation. Having to request memory every time a small vector is created would slow R down considerably. Instead, R asks for a big block of memory and then manages that block itself. This block is called the small vector pool and is used for vectors less than 128 bytes long. For efficiency and simplicity, it only allocates vectors that are 8, 16, 32, 48, 64, or 128 bytes long. If we adjust our previous plot to remove the 40 bytes of overhead, we can see that those values correspond to the jumps in memory use.  Beyond 128 bytes, it no longer makes sense for R to manage vectors. After all, allocating big chunks of memory is something that operating systems are very good at. Beyond 128 bytes, R will ask for memory in multiples of 8 bytes. This ensures good alignment. A subtlety of the size of an object is that components can be shared across multiple objects. For example, look at the following code: y isn’t three times as big as x because R is smart enough to not copy x three times; instead it just points to the existing x. It’s misleading to look at the sizes of x and y individually. If you want to know how much space they take up together, you have to supply them to the same object_size() call: In this case, x and y together take up the same amount of space as y alone. This is not always the case. If there are no shared components, as in the following example, then you can add up the sizes of individual components to find out the total size: The same issue also comes up with strings, because R has a global string pool. This means that each unique string is only stored in one place, and therefore character vectors take up less memory than you might expect:  Repeat the analysis above for numeric, logical, and complex vectors. If a data frame has one million rows, and three variables (two numeric, and one integer), how much space will it take up? Work it out from theory, then verify your work by creating a data frame and measuring its size. Compare the sizes of the elements in the following two lists. Each contains basically the same data, but one contains vectors of small strings while the other contains a single long string. Which takes up more memory: a factor (x) or the equivalent character vector (as.character(x))? Why? Explain the difference in size between 1:5 and list(1:5). While object_size() tells you the size of a single object, pryr::mem_used() tells you the total size of all objects in memory:  This number won’t agree with the amount of memory reported by your operating system for a number of reasons: It only includes objects created by R, not the R interpreter itself. Both R and the operating system are lazy: they won’t reclaim memory until it’s actually needed. R might be holding on to memory because the OS hasn’t yet asked for it back. R counts the memory occupied by objects but there may be gaps due to deleted objects. This problem is known as memory fragmentation. mem_change() builds on top of mem_used() to tell you how memory changes during code execution. Positive numbers represent an increase in the memory used by R, and negative numbers represent a decrease.  Even operations that don’t do anything use up a little memory. This is because R is tracking the history of everything you do. You can ignore anything on the order of around 2 kB. In some languages, you have to explicitly delete unused objects for their memory to be returned. R uses an alternative approach: garbage collection (or GC for short). GC automatically releases memory when an object is no longer used. It does this by tracking how many names point to each object, and when there are no names pointing to an object, it deletes that object.  Despite what you might have read elsewhere, there’s never any need to call gc() yourself. R will automatically run garbage collection whenever it needs more space; if you want to see when that is, call gcinfo(TRUE). The only reason you might want to call gc() is to ask R to return memory to the operating system. However, even that might not have any effect: older versions of Windows had no way for a program to return memory to the OS.  GC takes care of releasing objects that are no longer used. However, you do need to be aware of possible memory leaks. A memory leak occurs when you keep pointing to an object without realising it. In R, the two main causes of memory leaks are formulas and closures because they both capture the enclosing environment. The following code illustrates the problem. In f1(), 1:1e6 is only referenced inside the function, so when the function completes the memory is returned and the net memory change is 0. f2() and f3() both return objects that capture environments, so that x is not freed when the function completes.  mem_change() captures the net change in memory when running a block of code. Sometimes, however, we may want to measure incremental change. One way to do this is to use memory profiling to capture usage every few milliseconds. This functionality is provided by utils::Rprof() but it doesn’t provide a very useful display of the results. Instead we’ll use the lineprof package. It is powered by Rprof(), but displays the results in a more informative manner.  To demonstrate lineprof, we’re going to explore a bare-bones implementation of read.delim() with only three arguments:  We’ll also create a sample csv file: Using lineprof is straightforward. source() the code, apply lineprof() to an expression, then use shine() to view the results. Note that you must use source() to load the code. This is because lineprof uses srcrefs to match up the code and run times. The needed srcrefs are only created when you load code from disk.  shine() will also open a new web page (or if you’re using RStudio, a new pane) that shows your source code annotated with information about memory usage. shine() starts a shiny app which will “block” your R session. To exit, press escape or ctrl + break. Next to the source code, four columns provide details about the performance of the code: t, the time (in seconds) spent on that line of code (explained in measuring performance). a, the memory (in megabytes) allocated by that line of code. r, the memory (in megabytes) released by that line of code. While memory allocation is deterministic, memory release is stochastic: it depends on when the GC was run. This means that memory release only tells you that the memory released was no longer needed before this line. d, the number of vector duplications that occurred. A vector duplication occurs when R copies a vector as a result of its copy on modify semantics. You can hover over any of the bars to get the exact numbers. In this example, looking at the allocations tells us most of the story: scan() allocates about 2.5 MB of memory, which is very close to the 2.8 MB of space that the file occupies on disk. You wouldn’t expect the two numbers to be identical because R doesn’t need to store the commas and because the global string pool will save some memory. Converting the columns allocates another 0.6 MB of memory. You’d also expect this step to free some memory because we’ve converted string columns into integer and numeric columns (which occupy less space), but we can’t see those releases because GC hasn’t been triggered yet. Finally, calling as.data.frame() on a list allocates about 1.6 megabytes of memory and performs over 600 duplications. This is because as.data.frame() isn’t terribly efficient and ends up copying the input multiple times. We’ll discuss duplication more in the next section. There are two downsides to profiling: read_delim() only takes around half a second, but profiling can, at best, capture memory usage every 1 ms. This means we’ll only get about 500 samples. Since GC is lazy, we can never tell exactly when memory is no longer needed. You can work around both problems by using torture = TRUE, which forces R to run GC after every allocation (see gctorture() for more details). This helps with both problems because memory is freed as soon as possible, and R runs 10–100x slower. This effectively makes the resolution of the timer greater, so that you can see smaller allocations and exactly when memory is no longer needed. When the input is a list, we can make a more efficient as.data.frame() by using special knowledge. A data frame is a list with class data.frame and row.names attribute. row.names is either a character vector or vector of sequential integers, stored in a special format created by .set_row_names(). This leads to an alternative as.data.frame(): What impact does this function have on read_delim()? What are the downsides of this function? Line profile the following function with torture = TRUE. What is surprising? Read the source code of rm() to figure out what’s going on. What happens to x in the following code?   There are two possibilities: R modifies x in place. R makes a copy of x to a new location, modifies the copy, and then uses the name x to point to the new location. It turns out that R can do either depending on the circumstances. In the example above, it will modify in place. But if another variable also points to x, then R will copy it to a new location. To explore what’s going on in greater detail, we use two tools from the pryr package. Given the name of a variable, address() will tell us the variable’s location in memory and refs() will tell us how many names point to that location.   (Note that if you’re using RStudio, refs() will always return 2: the environment browser makes a reference to every object you create on the command line.) refs() is only an estimate. It can only distinguish between one and more than one reference (future versions of R might do better). This means that refs() returns 2 in both of the following cases:  When refs(x) is 1, modification will occur in place. When refs(x) is 2, R will make a copy (this ensures that other pointers to the object remain unaffected). Note that in the following example, y keeps pointing to the same location while x changes. Another useful function is tracemem(). It prints a message every time the traced object is copied:  For interactive use, tracemem() is slightly more useful than refs(), but because it just prints a message, it’s harder to program with. I don’t use it in this book because it interacts poorly with knitr, the tool I use to interleave text and code. Non-primitive functions that touch the object always increment the ref count. Primitive functions usually don’t. (The reasons are a little complicated, but see the R-devel thread confused about NAMED.)  Generally, provided that the object is not referred to elsewhere, any primitive replacement function will modify in place. This includes [[<-, [<-, @<-, $<-, attr<-, attributes<-, class<-, dim<-, dimnames<-, names<-, and levels<-. To be precise, all non-primitive functions increment refs, but a primitive function may be written in such a way that it doesn’t. The rules are sufficiently complicated that there’s little point in trying to memorise them. Instead, you should approach the problem practically by using refs() and address() to figure out when objects are being copied.  While determining that copies are being made is not hard, preventing such behaviour is. If you find yourself resorting to exotic tricks to avoid copies, it may be time to rewrite your function in C++, as described in Rcpp. For loops in R have a reputation for being slow. Often that slowness is because you’re modifying a copy instead of modifying in place. Consider the following code. It subtracts the median from each column of a large data frame:  You may be surprised to realise that every iteration of the loop copies the data frame. We can see that more clearly by using address() and refs() for a small sample of the loop: For each iteration, x is moved to a new location so refs(x) is always 2. This occurs because [<-.data.frame is not a primitive function, so it always increments the refs. We can make the function substantially more efficient by using a list instead of a data frame. Modifying a list uses primitive functions, so the refs are not incremented and all modifications occur in place: This behaviour was substantially more problematic prior to R 3.1.0, because every copy of the data frame was a deep copy. This made the motivating example take around 5 s, compared to 0.01 s today. The code below makes one duplication. Where does it occur and why? (Hint: look at refs(y).) The implementation of as.data.frame() in the previous section has one big downside. What is it and how could you avoid it? © Hadley Wickham. Powered by jekyll,
          knitr, and
          pandoc. Source
          available on github.
         How to contribute Edit this page Sometimes R code just isn’t fast enough. You’ve used profiling to figure out where your bottlenecks are, and you’ve done everything you can in R, but your code still isn’t fast enough. In this chapter you’ll learn how to improve performance by rewriting key functions in C++. This magic comes by way of the Rcpp package, a fantastic tool written by Dirk Eddelbuettel and Romain Francois (with key contributions by Doug Bates, John Chambers, and JJ Allaire). Rcpp makes it very simple to connect C++ to R. While it is possible to write C or Fortran code for use in R, it will be painful by comparison. Rcpp provides a clean, approachable API that lets you write high-performance code, insulated from R’s arcane C API.   Typical bottlenecks that C++ can address include: Loops that can’t be easily vectorised because subsequent iterations depend on previous ones. Recursive functions, or problems which involve calling functions millions of times. The overhead of calling a function in C++ is much lower than that in R. Problems that require advanced data structures and algorithms that R doesn’t provide. Through the standard template library (STL), C++ has efficient implementations of many important data structures, from ordered maps to double-ended queues. The aim of this chapter is to discuss only those aspects of C++ and Rcpp that are absolutely necessary to help you eliminate bottlenecks in your code. We won’t spend much time on advanced features like object oriented programming or templates because the focus is on writing small, self-contained functions, not big programs. A working knowledge of C++ is helpful, but not essential. Many good tutorials and references are freely available, including http://www.learncpp.com/ and http://www.cplusplus.com/. For more advanced topics, the Effective C++ series by Scott Meyers is popular choice. You may also enjoy Dirk Eddelbuettel’s Seamless R and C++ integration with Rcpp, which goes into much greater detail into all aspects of Rcpp. Getting started with C++ teaches you how to write C++ by converting simple R functions to their C++ equivalents. You’ll learn how C++ differs from R, and what the key scalar, vector, and matrix classes are called. Using sourceCpp shows you how to use sourceCpp() to load a C++ file from disk in the same way you use source() to load a file of R code. Attributes & other classes discusses how to modify attributes from Rcpp, and mentions some of the other important classes. Missing values teaches you how to work with R’s missing values in C++. Rcpp sugar discusses Rcpp “sugar”, which allows you to avoid loops in C++ and write code that looks very similar to vectorised R code. The STL shows you how to use some of the most important data structures and algorithms from the standard template library, or STL, built-in to C++. Case studies shows two real case studies where Rcpp was used to get considerable performance improvements. Putting Rcpp in a package teaches you how to add C++ code to a package. Learning more concludes the chapter with pointers to more resources to help you learn Rcpp and C++. All examples in this chapter need version 0.10.1 or above of the Rcpp package. This version includes cppFunction() and sourceCpp(), which makes it very easy to connect C++ to R. Install the latest version of Rcpp from CRAN with install.packages("Rcpp"). You’ll also need a working C++ compiler. To get it: cppFunction() allows you to write C++ functions in R:  When you run this code, Rcpp will compile the C++ code and construct an R function that connects to the compiled C++ function. We’re going to use this simple interface to learn how to write C++. C++ is a large language, and there’s no way to cover it all in just one chapter. Instead, you’ll get the basics so that you can start writing useful functions to address bottlenecks in your R code. The following sections will teach you the basics by translating simple R functions to their C++ equivalents. We’ll start simple with a function that has no inputs and a scalar output, and then get progressively more complicated: Let’s start with a very simple function. It has no arguments and always returns the integer 1: The equivalent C++ function is: We can compile and use this from R with cppFunction This small function illustrates a number of important differences between R and C++: The syntax to create a function looks like the syntax to call a function; you don’t use assignment to create functions as you do in R. You must declare the type of output the function returns. This function returns an int (a scalar integer). The classes for the most common types of R vectors are: NumericVector, IntegerVector, CharacterVector, and LogicalVector. Scalars and vectors are different. The scalar equivalents of numeric, integer, character, and logical vectors are: double, int, String, and bool. You must use an explicit return statement to return a value from a function. Every statement is terminated by a ;. The next example function implements a scalar version of the sign() function which returns 1 if the input is positive, and -1 if it’s negative: In the C++ version: We declare the type of each input in the same way we declare the type of the output. While this makes the code a little more verbose, it also makes it very obvious what type of input the function needs. The if syntax is identical — while there are some big differences between R and C++, there are also lots of similarities! C++ also has a while statement that works the same way as R’s. As in R you can use break to exit the loop, but to skip one iteration you need to use continue instead of next. One big difference between R and C++ is that the cost of loops is much lower in C++. For example, we could implement the sum function in R using a loop. If you’ve been programming in R a while, you’ll probably have a visceral reaction to this function! In C++, loops have very little overhead, so it’s fine to use them. In STL, you’ll see alternatives to for loops that more clearly express your intent; they’re not faster, but they can make your code easier to understand. The C++ version is similar, but: To find the length of the vector, we use the .size() method, which returns an integer. C++ methods are called with . (i.e., a full stop). The for statement has a different syntax: for(init; check; increment). This loop is initialised by creating a new variable called i with value 0. Before each iteration we check that i < n, and terminate the loop if it’s not. After each iteration, we increment the value of i by one, using the special prefix operator ++ which increases the value of i by 1. In C++, vector indices start at 0. I’ll say this again because it’s so important: IN C++, VECTOR INDICES START AT 0! This is a very common source of bugs when converting R functions to C++. Use = for assignment, not <-. C++ provides operators that modify in-place: total += x[i] is equivalent to total = total + x[i]. Similar in-place operators are -=, *=, and /=. This is a good example of where C++ is much more efficient than R. As shown by the following microbenchmark, sumC() is competitive with the built-in (and highly optimised) sum(), while sumR() is several orders of magnitude slower. Next we’ll create a function that computes the Euclidean distance between a value and a vector of values: It’s not obvious that we want x to be a scalar from the function definition. We’d need to make that clear in the documentation. That’s not a problem in the C++ version because we have to be explicit about types: This function introduces only a few new concepts: We create a new numeric vector of length n with a constructor: NumericVector out(n). Another useful way of making a vector is to copy an existing one: NumericVector zs = clone(ys). C++ uses pow(), not ^, for exponentiation. Note that because the R version is fully vectorised, it’s already going to be fast. On my computer, it takes around 8 ms with a 1 million element y vector. The C++ function is twice as fast, ~4 ms, but assuming it took you 10 minutes to write the C++ function, you’d need to run it ~150,000 times to make rewriting worthwhile. The reason why the C++ function is faster is subtle, and relates to memory management. The R version needs to create an intermediate vector the same length as y (x - ys), and allocating memory is an expensive operation. The C++ function avoids this overhead because it uses an intermediate scalar. In the sugar section, you’ll see how to rewrite this function to take advantage of Rcpp’s vectorised operations so that the C++ code is almost as concise as R code. Each vector type has a matrix equivalent: NumericMatrix, IntegerMatrix, CharacterMatrix, and LogicalMatrix. Using them is straightforward. For example, we could create a function that reproduces rowSums(): The main differences: In C++, you subset a matrix with (), not []. Use .nrow() and .ncol() methods to get the dimensions of a matrix. So far, we’ve used inline C++ with cppFunction(). This makes presentation simpler, but for real problems, it’s usually easier to use stand-alone C++ files and then source them into R using sourceCpp(). This lets you take advantage of text editor support for C++ files (e.g., syntax highlighting) as well as making it easier to identify the line numbers in compilation errors.  Your stand-alone C++ file should have extension .cpp, and needs to start with: And for each function that you want available within R, you need to prefix it with: Note that the space is mandatory. If you’re familiar with roxygen2, you might wonder how this relates to @export. Rcpp::export controls whether a function is exported from C++ to R; @export controls whether a function is exported from a package and made available to the user. You can embed R code in special C++ comment blocks. This is really convenient if you want to run some test code: The R code is run with source(echo = TRUE) so you don’t need to explicitly print output. To compile the C++ code, use sourceCpp("path/to/file.cpp"). This will create the matching R functions and add them to your current session. Note that these functions can not be saved in a .Rdata file and reloaded in a later session; they must be recreated each time you restart R. For example, running sourceCpp() on the following file implements mean in C++ and then compares it to the built-in mean(): NB: if you run this code yourself, you’ll notice that meanC() is much faster than the built-in mean(). This is because it trades numerical accuracy for speed. For the remainder of this chapter C++ code will be presented stand-alone rather than wrapped in a call to cppFunction. If you want to try compiling and/or modifying the examples you should paste them into a C++ source file that includes the elements described above. With the basics of C++ in hand, it’s now a great time to practice by reading and writing some simple C++ functions. For each of the following functions, read the code and figure out what the corresponding base R function is. You might not understand every part of the code yet, but you should be able to figure out the basics of what the function does. To practice your function writing skills, convert the following functions into C++. For now, assume the inputs have no missing values. all() cumprod(), cummin(), cummax(). diff(). Start by assuming lag 1, and then generalise for lag n. range. var. Read about the approaches you can take on wikipedia. Whenever implementing a numerical algorithm, it’s always good to check what is already known about the problem. You’ve already seen the basic vector classes (IntegerVector, NumericVector, LogicalVector, CharacterVector) and their scalar (int, double, bool, String) and matrix (IntegerMatrix, NumericMatrix, LogicalMatrix, CharacterMatrix) equivalents. All R objects have attributes, which can be queried and modified with .attr(). Rcpp also provides .names() as an alias for the name attribute. The following code snippet illustrates these methods. Note the use of ::create(), a class method. This allows you to create an R vector from C++ scalar values:   For S4 objects, .slot() plays a similar role to .attr(). Rcpp also provides classes List and DataFrame, but they are more useful for output than input. This is because lists and data frames can contain arbitrary classes but C++ needs to know their classes in advance. If the list has known structure (e.g., it’s an S3 object), you can extract the components and manually convert them to their C++ equivalents with as(). For example, the object created by lm(), the function that fits a linear model, is a list whose components are always of the same type. The following code illustrates how you might extract the mean percentage error (mpe()) of a linear model. This isn’t a good example of when to use C++, because it’s so easily implemented in R, but it shows how to work with an important S3 class. Note the use of .inherits() and the stop() to check that the object really is a linear model.   You can put R functions in an object of type Function. This makes calling an R function from C++ straightforward. We first define our C++ function:  Then call it from R: What type of object does an R function return? We don’t know, so we use the catchall type RObject. An alternative is to return a List. For example, the following code is a basic implementation of lapply in C++: Calling R functions with positional arguments is obvious: But to use named arguments, you need a special syntax: There are also classes for many more specialised language objects: Environment, ComplexVector, RawVector, DottedPair, Language, Promise, Symbol, WeakReference, and so on. These are beyond the scope of this chapter and won’t be discussed further. If you’re working with missing values, you need to know two things:  The following code explores what happens when you take one of R’s missing values, coerce it into a scalar, and then coerce back to an R vector. Note that this kind of experimentation is a useful way to figure out what any operation does. With the exception of bool, things look pretty good here: all of the missing values have been preserved. However, as we’ll see in the following sections, things are not quite as straightforward as they seem. With integers, missing values are stored as the smallest integer. If you don’t do anything to them, they’ll be preserved. But, since C++ doesn’t know that the smallest integer has this special behaviour, if you do anything to it you’re likely to get an incorrect value: for example, evalCpp('NA_INTEGER + 1') gives -2147483647. So if you want to work with missing values in integers, either use a length one IntegerVector or be very careful with your code. With doubles, you may be able to get away with ignoring missing values and working with NaNs (not a number). This is because R’s NA is a special type of IEEE 754 floating point number NaN. So any logical expression that involves a NaN (or in C++, NAN) always evaluates as FALSE: But be careful when combining then with boolean values: However, in numeric contexts NaNs will propagate NAs: String is a scalar string class introduced by Rcpp, so it knows how to deal with missing values. While C++’s bool has two possible values (true or false), a logical vector in R has three (TRUE, FALSE, and NA). If you coerce a length 1 logical vector, make sure it doesn’t contain any missing values otherwise they will be converted to TRUE. With vectors, you need to use a missing value specific to the type of vector, NA_REAL, NA_INTEGER, NA_LOGICAL, NA_STRING: To check if a value in a vector is missing, use the class method ::is_na(): Another alternative is the sugar function is_na(), which takes a vector and returns a logical vector. Rewrite any of the functions from the first exercise to deal with missing values. If na.rm is true, ignore the missing values. If na.rm is false, return a missing value if the input contains any missing values. Some good functions to practice with are min(), max(), range(), mean(), and var(). Rewrite cumsum() and diff() so they can handle missing values. Note that these functions have slightly more complicated behaviour. Rcpp provides a lot of syntactic “sugar” to ensure that C++ functions work very similarly to their R equivalents. In fact, Rcpp sugar makes it possible to write efficient C++ code that looks almost identical to its R equivalent. If there’s a sugar version of the function you’re interested in, you should use it: it’ll be both expressive and well tested. Sugar functions aren’t always faster than a handwritten equivalent, but they will get faster in the future as more time is spent on optimising Rcpp. Sugar functions can be roughly broken down into All the basic arithmetic and logical operators are vectorised: + *, -, /, pow, <, <=, >, >=, ==, !=, !. For example, we could use sugar to considerably simplify the implementation of pdistC(). The sugar function any() and all() are fully lazy so that any(x == 0), for example, might only need to evaluate one element of a vector, and return a special type that can be converted into a bool using .is_true(), .is_false(), or .is_na(). We could also use this sugar to write an efficient function to determine whether or not a numeric vector contains any missing values. To do this in R, we could use any(is.na(x)): However, this will do the same amount of work regardless of the location of the missing value. Here’s the C++ implementation: A number of helpful functions provide a “view” of a vector: head(), tail(), rep_each(), rep_len(), rev(), seq_along(), and seq_len(). In R these would all produce copies of the vector, but in Rcpp they simply point to the existing vector and override the subsetting operator ([) to implement special behaviour. This makes them very efficient: for instance, rep_len(x, 1e6) does not have to make a million copies of x. Finally, there’s a grab bag of sugar functions that mimic frequently used R functions: Math functions: abs(), acos(), asin(), atan(), beta(), ceil(), ceiling(), choose(), cos(), cosh(), digamma(), exp(), expm1(), factorial(), floor(), gamma(), lbeta(), lchoose(), lfactorial(), lgamma(), log(), log10(), log1p(), pentagamma(), psigamma(), round(), signif(), sin(), sinh(), sqrt(), tan(), tanh(), tetragamma(), trigamma(), trunc(). Scalar summaries: mean(), min(), max(), sum(), sd(), and (for vectors) var(). Vector summaries: cumsum(), diff(), pmin(), and pmax(). Finding values: match(), self_match(), which_max(), which_min(). Dealing with duplicates: duplicated(), unique(). d/q/p/r for all standard distributions. Finally, noNA(x) asserts that the vector x does not contain any missing values, and allows optimisation of some mathematical operations. The real strength of C++ shows itself when you need to implement more complex algorithms. The standard template library (STL) provides a set of extremely useful data structures and algorithms. This section will explain some of the most important algorithms and data structures and point you in the right direction to learn more. I can’t teach you everything you need to know about the STL, but hopefully the examples will show you the power of the STL, and persuade you that it’s useful to learn more.  If you need an algorithm or data structure that isn’t implemented in STL, a good place to look is boost. Installing boost on your computer is beyond the scope of this chapter, but once you have it installed, you can use boost data structures and algorithms by including the appropriate header file with (e.g.) #include <boost/array.hpp>. Iterators are used extensively in the STL: many functions either accept or return iterators. They are the next step up from basic loops, abstracting away the details of the underlying data structure. Iterators have three main operators:  For example we could re-write our sum function using iterators: The main changes are in the for loop: We start at x.begin() and loop until we get to x.end(). A small optimization is to store the value of the end iterator so we don’t need to look it up each time. This only saves about 2 ns per iteration, so it’s only important when the calculations in the loop are very simple. Instead of indexing into x, we use the dereference operator to get its current value: *it. Notice the type of the iterator: NumericVector::iterator. Each vector type has its own iterator type: LogicalVector::iterator, CharacterVector::iterator, etc. Iterators also allow us to use the C++ equivalents of the apply family of functions. For example, we could again rewrite sum() to use the accumulate() function, which takes a starting and an ending iterator, and adds up all the values in the vector. The third argument to accumulate gives the initial value: it’s particularly important because this also determines the data type that accumulate uses (so we use 0.0 and not 0 so that accumulate uses a double, not an int.). To use accumulate() we need to include the <numeric> header. accumulate() (along with the other functions in <numeric>, like adjacent_difference(), inner_product(), and partial_sum()) is not that important in Rcpp because Rcpp sugar provides equivalents. The <algorithm> header provides a large number of algorithms that work with iterators. A good reference is available at http://www.cplusplus.com/reference/algorithm/. For example, we could write a basic Rcpp version of findInterval() that takes two arguments a vector of values and a vector of breaks, and locates the bin that each x falls into. This shows off a few more advanced iterator features. Read the code below and see if you can figure out how it works.  The key points are: We step through two iterators (input and output) simultaneously. We can assign into an dereferenced iterator (out_it) to change the values in out. upper_bound() returns an iterator. If we wanted the value of the upper_bound() we could dereference it; to figure out its location, we use the distance() function. Small note: if we want this function to be as fast as findInterval() in R (which uses handwritten C code), we need to compute the calls to .begin() and .end() once and save the results. This is easy, but it distracts from this example so it has been omitted. Making this change yields a function that’s slightly faster than R’s findInterval() function, but is about 1/10 of the code. It’s generally better to use algorithms from the STL than hand rolled loops. In Effective STL, Scott Meyers gives three reasons: efficiency, correctness, and maintainability. Algorithms from the STL are written by C++ experts to be extremely efficient, and they have been around for a long time so they are well tested. Using standard algorithms also makes the intent of your code more clear, helping to make it more readable and more maintainable. The STL provides a large set of data structures: array, bitset, list, forward_list, map, multimap, multiset, priority_queue, queue, dequeue, set, stack, unordered_map, unordered_set, unordered_multimap, unordered_multiset, and vector. The most important of these data structures are the vector, the unordered_set, and the unordered_map. We’ll focus on these three in this section, but using the others is similar: they just have different performance trade-offs. For example, the deque (pronounced “deck”) has a very similar interface to vectors but a different underlying implementation that has different performance trade-offs. You may want to try them for your problem. A good reference for STL data structures is http://www.cplusplus.com/reference/stl/ — I recommend you keep it open while working with the STL. Rcpp knows how to convert from many STL data structures to their R equivalents, so you can return them from your functions without explicitly converting to R data structures. An STL vector is very similar to an R vector, except that it grows efficiently. This makes vectors appropriate to use when you don’t know in advance how big the output will be. Vectors are templated, which means that you need to specify the type of object the vector will contain when you create it: vector<int>, vector<bool>, vector<double>, vector<String>. You can access individual elements of a vector using the standard [] notation, and you can add a new element to the end of the vector using .push_back(). If you have some idea in advance how big the vector will be, you can use .reserve() to allocate sufficient storage.  The following code implements run length encoding (rle()). It produces two vectors of output: a vector of values, and a vector lengths giving how many times each element is repeated. It works by looping through the input vector x comparing each value to the previous: if it’s the same, then it increments the last value in lengths; if it’s different, it adds the value to the end of values, and sets the corresponding length to 1. (An alternative implementation would be to replace i with the iterator lengths.rbegin() which always points to the last element of the vector. You might want to try implementing that yourself.) Other methods of a vector are described at http://www.cplusplus.com/reference/vector/vector/. Sets maintain a unique set of values, and can efficiently tell if you’ve seen a value before. They are useful for problems that involve duplicates or unique values (like unique, duplicated, or in). C++ provides both ordered (std::set) and unordered sets (std::unordered_set), depending on whether or not order matters for you. Unordered sets tend to be much faster (because they use a hash table internally rather than a tree), so even if you need an ordered set, you should consider using an unordered set and then sorting the output. Like vectors, sets are templated, so you need to request the appropriate type of set for your purpose: unordered_set<int>, unordered_set<bool>, etc. More details are available at http://www.cplusplus.com/reference/set/set/ and http://www.cplusplus.com/reference/unordered_set/unordered_set/.  The following function uses an unordered set to implement an equivalent to duplicated() for integer vectors. Note the use of seen.insert(x[i]).second. insert() returns a pair, the .first value is an iterator that points to element and the .second value is a boolean that’s true if the value was a new addition to the set. Note that unordered sets are only available in C++ 11, which means we need to use the cpp11 plugin, [[Rcpp::plugins(cpp11)]]. A map is similar to a set, but instead of storing presence or absence, it can store additional data. It’s useful for functions like table() or match() that need to look up a value. As with sets, there are ordered (std::map) and unordered (std::unordered_map) versions. Since maps have a value and a key, you need to specify both types when initialising a map: map<double, int>, unordered_map<int, double>, and so on. The following example shows how you could use a map to implement table() for numeric vectors:  Note that unordered maps are only available in C++ 11, so to use them, you’ll again need [[Rcpp::plugins(cpp11)]]. To practice using the STL algorithms and data structures, implement the following using R functions in C++, using the hints provided: median.default() using partial_sort. %in% using unordered_set and the find() or count() methods. unique() using an unordered_set (challenge: do it in one line!). min() using std::min(), or max() using std::max(). which.min() using min_element, or which.max() using max_element. setdiff(), union(), and intersect() for integers using sorted ranges and set_union, set_intersection and set_difference. The following case studies illustrate some real life uses of C++ to replace slow R code. The following case study updates an example blogged about by Dirk Eddelbuettel, illustrating the conversion of a Gibbs sampler in R to C++. The R and C++ code shown below is very similar (it only took a few minutes to convert the R version to the C++ version), but runs about 20 times faster on my computer. Dirk’s blog post also shows another way to make it even faster: using the faster random number generator functions in GSL (easily accessible from R through the RcppGSL package) can make it another 2–3x faster.  The R code is as follows: This is straightforward to convert to C++. We: add type declarations to all variables use ( instead of [ to index into the matrix subscript the results of rgamma and rnorm to convert from a vector into a scalar Benchmarking the two implementations yields: This example is adapted from “Rcpp is smoking fast for agent-based models in data frames”. The challenge is to predict a model response from three inputs. The basic R version of the predictor looks like: We want to be able to apply this function to many inputs, so we might write a vector-input version using a for loop. If you’re familiar with R, you’ll have a gut feeling that this will be slow, and indeed it is. There are two ways we could attack this problem. If you have a good R vocabulary, you might immediately see how to vectorise the function (using ifelse(), pmin(), and pmax()). Alternatively, we could rewrite vacc1a() and vacc1() in C++, using our knowledge that loops and function calls have much lower overhead in C++. Either approach is fairly straightforward. In R: (If you’ve worked R a lot you might recognise some potential bottlenecks in this code: ifelse, pmin, and pmax are known to be slow, and could be replaced with p + 0.75 + 0.5 * female, p[p < 0] <- 0, p[p > 1] <- 1. You might want to try timing those variations yourself.) Or in C++: We next generate some sample data, and check that all three versions return the same values: The original blog post forgot to do this, and introduced a bug in the C++ version: it used 0.004 instead of 0.04. Finally, we can benchmark our three approaches: Not surprisingly, our original approach with loops is very slow. Vectorising in R gives a huge speedup, and we can eke out even more performance (~10x) with the C++ loop. I was a little surprised that the C++ was so much faster, but it is because the R version has to create 11 vectors to store intermediate results, where the C++ code only needs to create 1. The same C++ code that is used with sourceCpp() can also be bundled into a package. There are several benefits of moving code from a stand-alone C++ source file to a package:  Your code can be made available to users without C++ development tools. Multiple source files and their dependencies are handled automatically by the R package build system. Packages provide additional infrastructure for testing, documentation, and consistency. To add Rcpp to an existing package, you put your C++ files in the src/ directory and modify/create the following configuration files: In DESCRIPTION add Make sure your NAMESPACE includes: We need to import something (anything) from Rcpp so that internal Rcpp code is properly loaded. This is a bug in R and hopefully will be fixed in the future. To generate a new Rcpp package that includes a simple “hello world” function you can use Rcpp.package.skeleton(): To generate a package based on C++ files that you’ve been using with sourceCpp(), use the cpp_files parameter: Before building the packge, you’ll need to run Rcpp::compileAttributes(). This function scans the C++ files for Rcpp::export attributes and generates the code required to make the functions available in R. Re-run compileAttributes() whenever functions are added, removed, or have their signatures changed. This is done automatically by the devtools package and by Rstudio. For more details see the Rcpp package vignette, vignette("Rcpp-package"). This chapter has only touched on a small part of Rcpp, giving you the basic tools to rewrite poorly performing R code in C++. The Rcpp book is the best reference to learn more about Rcpp. As noted, Rcpp has many other capabilities that make it easy to interface R to existing C++ code, including: Additional features of attributes including specifying default arguments, linking in external C++ dependencies, and exporting C++ interfaces from packages. These features and more are covered in the Rcpp attributes vignette, vignette("Rcpp-attributes"). Automatically creating wrappers between C++ data structures and R data structures, including mapping C++ classes to reference classes. A good introduction to this topic is Rcpp modules vignette, vignette("Rcpp-modules") The Rcpp quick reference guide, vignette("Rcpp-quickref"), contains a useful summary of Rcpp classes and common programming idioms. I strongly recommend keeping an eye on the Rcpp homepage and Dirk’s Rcpp page as well as signing up for the Rcpp mailing list. Rcpp is still under active development, and is getting better with every release. Other resources I’ve found helpful in learning C++ are: Effective C++ and Effective STL by Scott Meyers. C++ Annotations, aimed at “knowledgeable users of C (or any other language using a C-like grammar, like Perl or Java) who would like to know more about, or make the transition to, C++”. Algorithm Libraries, which provides a more technical, but still concise, description of important STL concepts. (Follow the links under notes). Writing performance code may also require you to rethink your basic approach: a solid understanding of basic data structures and algorithms is very helpful here. That’s beyond the scope of this book, but I’d suggest the Algorithm Design Manual, MIT’s Introduction to Algorithms, Algorithms by Robert Sedgewick and Kevin Wayne which has a free online textbook and a matching coursera course. I’d like to thank the Rcpp-mailing list for many helpful conversations, particularly Romain Francois and Dirk Eddelbuettel who have not only provided detailed answers to many of my questions, but have been incredibly responsive at improving Rcpp. This chapter would not have been possible without JJ Allaire; he encouraged me to learn C++ and then answered many of my dumb questions along the way. © Hadley Wickham. Powered by jekyll,
          knitr, and
          pandoc. Source
          available on github.
         How to contribute Edit this page The first principle of using a package is that all R code goes in R/. In this chapter, you’ll learn about the R/ directory, my recommendations for organising your functions into files, and some general tips on good style. You’ll also learn about some important differences between functions in scripts and functions in packages. The first practical advantage to using a package is that it’s easy to re-load your code. You can either run devtools::load_all(), or in RStudio press Ctrl/Cmd + Shift + L, which also saves all open files, saving you a keystroke. This keyboard shortcut leads to a fluid development workflow: Edit an R file. Press Ctrl/Cmd + Shift + L. Explore the code in the console. Rinse and repeat. Congratulations! You’ve learned your first package development workflow. Even if you learn nothing else from this book, you’ll have gained a useful workflow for editing and reloading R code. While you’re free to arrange functions into files as you wish, the two extremes are bad: don’t put all functions into one file and don’t put each function into its own separate file. (It’s OK if some files only contain one function, particularly if the function is large or has a lot of documentation.). File names should be meaningful and end in .R. Pay attention to capitalization, since you, or some of your collaborators, might be using an operating system with a case-insensitive file system (e.g., Microsoft Windows or OS X). Avoid problems by never using filenames that differ only in capitalisation. My rule of thumb is that if I can’t remember the name of the file where a function lives, I need to either separate the functions into more files or give the file a better name. (Unfortunately you can’t use subdirectories inside R/. The next best thing is to use a common prefix, e.g., abc-*.R.). The arrangement of functions within files is less important if you master two important RStudio keyboard shortcuts that let you jump to the definition of a function: Click a function name in code and press F2. Press Ctrl + . then start typing the name:  After navigating to a function using one of these tools, you can go back to where you were by clicking the back arrow at the top-left of the editor (), or by pressing Ctrl/Cmd-F9. Good coding style is like using correct punctuation. You can manage without it, but it sure makes things easier to read. As with styles of punctuation, there are many possible variations. The following guide describes the style that I use (in this book and elsewhere). It is based on Google’s R style guide, with a few tweaks. You don’t have to use my style, but I strongly recommend that you use a consistent style and you document it. If you’re working on someone else’s code, don’t impose your own style. Instead, read their style documentation and follow it as closely as possible. Good style is important because while your code only has one author, it will usually have multiple readers. This is especially true when you’re writing code with others. In that case, it’s a good idea to agree on a common style up-front. Since no style is strictly better than another, working with others may mean that you’ll need to sacrifice some preferred aspects of your style. The formatR package, by Yihui Xie, makes it easier to clean up poorly formatted code. It can’t do everything, but it can quickly get your code from terrible to pretty good. Make sure to read the notes on the website before using it. It’s as easy as: A complementary approach is to use a code linter. Rather than automatically fixing problems, a linter just warns you about them. The lintr package by Jim Hester checks for compliance with this style guide and lets you know where you’ve missed something. Compared to formatR, it picks up more potential problems (because it doesn’t need to also fix them), but you will still see false positives. Variable and function names should be lowercase. Use an underscore (_) to separate words within a name (reserve . for S3 methods). Camel case is a legitimate alternative, but be consistent! Generally, variable names should be nouns and function names should be verbs. Strive for names that are concise and meaningful (this is not easy!). Where possible, avoid using names of existing functions and variables. This will cause confusion for the readers of your code. Place spaces around all infix operators (=, +, -, <-, etc.). The same rule applies when using = in function calls. Always put a space after a comma, and never before (just like in regular English). There’s a small exception to this rule: :, :: and ::: don’t need spaces around them. (If you haven’t seen :: or ::: before, don’t worry - you’ll learn all about them in namespaces.) Place a space before left parentheses, except in a function call. Extra spacing (i.e., more than one space in a row) is ok if it improves alignment of equal signs or assignments (<-). Do not place spaces around code in parentheses or square brackets (unless there’s a comma, in which case see above). An opening curly brace should never go on its own line and should always be followed by a new line. A closing curly brace should always go on its own line, unless it’s followed by else. Always indent the code inside curly braces. It’s ok to leave very short statements on the same line: Strive to limit your code to 80 characters per line. This fits comfortably on a printed page with a reasonably sized font. If you find yourself running out of room, this is a good indication that you should encapsulate some of the work in a separate function. When indenting your code, use two spaces. Never use tabs or mix tabs and spaces. Change these options in the code preferences pane:  The only exception is if a function definition runs over multiple lines. In that case, indent the second line to where the definition starts: Use <-, not =, for assignment. Comment your code. Each line of a comment should begin with the comment symbol and a single space: #. Comments should explain the why, not the what.  Use commented lines of - and = to break up your file into easily readable chunks. Up until now, you’ve probably been writing scripts, R code saved in a file that you load with source(). There are two main differences between code in scripts and packages: In a script, code is run when it is loaded. In a package, code is run when it is built. This means your package code should only create objects, the vast majority of which will be functions. Functions in your package will be used in situations that you didn’t imagine. This means your functions need to be thoughtful in the way that they interact with the outside world. The next two sections expand on these important differences. When you load a script with source(), every line of code is executed and the results are immediately made available. Things are different in a package, because it is loaded in two steps. When the package is built (e.g. by CRAN) all the code in R/ is executed and the results are saved. When you load a package, with library() or require(), the cached results are made available to you. If you loaded scripts in the same way as packages, your code would look like this: For example, take x <- Sys.time(). If you put this in a script, x would tell you when the script was source()d. But if you put that same code in a package, x would tell you when the package was built. This means that you should never run code at the top-level of a package: package code should only create objects, mostly functions. For example, imagine your foo package contains this code: If someone tries to use it: The code won’t work because ggplot2’s qplot() function won’t be available: library(foo) doesn’t re-execute library(ggplot2). The top-level R code in a package is only executed when the package is built, not when it’s loaded. To get around this problem you might be tempted to do: That’s also problematic, as you’ll see below. Instead, describe the packages your code needs in the DESCRIPTION file, as you’ll learn in package dependencies. Another big difference between a script and a package is that other people are going to use your package, and they’re going to use it in situations that you never imagined. This means you need to pay attention to the R landscape, which includes not just the available functions and objects, but all the global settings. You have changed the R landscape if you’ve loaded a package with library(), or changed a global option with options(), or modified the working directory with setwd(). If the behaviour of other functions differs before and after running your function, you’ve modified the landscape. Changing the landscape is bad because it makes code much harder to understand. There are some functions that modify global settings that you should never use because there are better alternatives: Don’t use library() or require(). These modify the search path, affecting what functions are available from the global environment. It’s better to use the DESCRIPTION to specify your package’s requirements, as described in the next chapter. This also makes sure those packages are installed when your package is installed. Never use source() to load code from a file. source() modifies the current environment, inserting the results of executing the code. Instead, rely on devtools::load_all() which automatically sources all files in R/. If you’re using source() to create a dataset, instead switch to data/ as described in datasets. Other functions need to be used with caution. If you use them, make sure to clean up after yourself with on.exit(): If you modify global options() or graphics par(), save the old values and reset when you’re done: Avoid modifying the working directory. If you do have to change it, make sure to change it back when you’re done: Creating plots and printing output to the console are two other ways of affecting the global R environment. Often you can’t avoid these (because they’re important!) but it’s good practice to isolate them in functions that only produce output. This also makes it easier for other people to repurpose your work for new uses. For example, if you separate data preparation and plotting into two functions, others can use your data prep work (which is often the hardest part!) to create new visualisations. The flip side of the coin is that you should avoid relying on the user’s landscape, which might be different to yours. For example, functions like read.csv() are dangerous because the value of stringsAsFactors argument comes from the global option stringsAsFactors. If you expect it to be TRUE (the default), and the user has set it to be FALSE, your code might fail. Occasionally, packages do need side-effects. This is most common if your package talks to an external system — you might need to do some initial setup when the package loads. To do that, you can use two special functions: .onLoad() and .onAttach(). These are called when the package is loaded and attached. You’ll learn about the distinction between the two in Namespaces. For now, you should always use .onLoad() unless explicitly directed otherwise. Some common uses of .onLoad() and .onAttach() are: To display an informative message when the package loads. This might make usage conditions clear, or display useful tips. Startup messages is one place where you should use .onAttach() instead of .onLoad(). To display startup messages, always use packageStartupMessage(), and not message(). (This allows suppressPackageStartupMessages() to selectively suppress package startup messages). To set custom options for your package with options(). To avoid conflicts with other packages, ensure that you prefix option names with the name of your package. Also be careful not to override options that the user has already set. I use the following code in devtools to set up useful options: Then devtools functions can use e.g. getOption("devtools.name") to get the name of the package author, and know that a sensible default value has already been set. To connect R to another programming language. For example, if you use rJava to talk to a .jar file, you need to call rJava::.jpackage(). To make C++ classes available as reference classes in R with Rcpp modules, you call Rcpp::loadRcppModules(). To register vignette engines with tools::vignetteEngine(). As you can see in the examples, .onLoad() and .onAttach() are called with two arguments: libname and pkgname. They’re rarely used (they’re a holdover from the days when you needed to use library.dynam() to load compiled code). They give the path where the package is installed (the “library”), and the name of the package. If you use .onLoad(), consider using .onUnload() to clean up any side effects. By convention, .onLoad() and friends are usually saved in a file called zzz.R. (Note that .First.lib() and .Last.lib() are old versions of .onLoad() and .onUnload() and should no longer be used.) Another type of side-effect is defining S4 classes, methods and generics. R packages capture these side-effects so they can be replayed when the package is loaded, but they need to be called in the right order. For example, before you can define a method, you must have defined both the generic and the class. This requires that the R files be sourced in a specific order. This order is controlled by the Collate field in the DESCRIPTION. This is described in more detail in documenting S4. (Each chapter will finish with some hints for submitting your package to CRAN. If you don’t plan on submitting your package to CRAN, feel free to ignore them!) If you’re planning on submitting your package to CRAN, you must use only ASCII characters in your .R files. You can still include unicode characters in strings, but you need to use the special unicode escape "\u1234" format. The easiest way to do that is to use stringi::stri_escape_unicode(): © Hadley Wickham. Powered by jekyll,
          knitr, and
          pandoc. Source
          available on github.
